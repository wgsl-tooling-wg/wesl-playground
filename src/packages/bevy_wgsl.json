{"bevy::core_pipeline::fullscreen_vertex_shader":"struct FullscreenVertexOutput {\n    @builtin(position)\n    position: vec4<f32>,\n    @location(0)\n    uv: vec2<f32>,\n};\n\n// This vertex shader produces the following, when drawn using indices 0..3:\n//\n//  1 |  0-----x.....2\n//  0 |  |  s  |  . ´\n// -1 |  x_____x´\n// -2 |  :  .´\n// -3 |  1´\n//    +---------------\n//      -1  0  1  2  3\n//\n// The axes are clip-space x and y. The region marked s is the visible region.\n// The digits in the corners of the right-angled triangle are the vertex\n// indices.\n//\n// The top-left has UV 0,0, the bottom-left has 0,2, and the top-right has 2,0.\n// This means that the UV gets interpolated to 1,1 at the bottom-right corner\n// of the clip-space rectangle that is at 1,-1 in clip space.\n@vertex\nfn fullscreen_vertex_shader(@builtin(vertex_index) vertex_index: u32) -> FullscreenVertexOutput {\n    // See the explanation above for how this works\n    let uv = vec2<f32>(f32(vertex_index >> 1u), f32(vertex_index & 1u)) * 2.0;\n    let clip_position = vec4<f32>(uv * vec2<f32>(2.0, -2.0) + vec2<f32>(-1.0, 1.0), 0.0, 1.0);\n\n    return FullscreenVertexOutput(clip_position, uv);\n}\n","bevy::core_pipeline::oit":"import package::pbr::mesh_view_bindings::{view, oit_layers, oit_layer_ids, oit_settings};\n\n@if(!OIT_ENABLED)\nconst module_requires_flag_OIT_ENABLED = false;\n@if(!OIT_ENABLED)\nconst_assert module_requires_flag_OIT_ENABLED; // module requires feature flag OIT_ENABLED\n\n// Add the fragment to the oit buffer\nfn oit_draw(position: vec4f, color: vec4f) {\n    // Don't add fully transparent fragments to the list\n    // because we don't want to have to sort them in the resolve pass\n    if color.a < oit_settings.alpha_threshold {\n        return;\n    }\n    // get the index of the current fragment relative to the screen size\n    let screen_index = i32(floor(position.x) + floor(position.y) * view.viewport.z);\n    // get the size of the buffer.\n    // It's always the size of the screen\n    let buffer_size = i32(view.viewport.z * view.viewport.w);\n\n    // gets the layer index of the current fragment\n    var layer_id = atomicAdd(&oit_layer_ids[screen_index], 1);\n    // exit early if we've reached the maximum amount of fragments per layer\n    if layer_id >= oit_settings.layers_count {\n        // force to store the oit_layers_count to make sure we don't\n        // accidentally increase the index above the maximum value\n        atomicStore(&oit_layer_ids[screen_index], oit_settings.layers_count);\n        // TODO for tail blending we should return the color here\n        return;\n    }\n\n    // get the layer_index from the screen\n    let layer_index = screen_index + layer_id * buffer_size;\n    let rgb9e5_color = package::pbr::rgb9e5::vec3_to_rgb9e5_(color.rgb);\n    let depth_alpha = pack_24bit_depth_8bit_alpha(position.z, color.a);\n    oit_layers[layer_index] = vec2(rgb9e5_color, depth_alpha);\n}\n\nfn pack_24bit_depth_8bit_alpha(depth: f32, alpha: f32) -> u32 {\n    let depth_bits = u32(saturate(depth) * f32(0xFFFFFFu) + 0.5);\n    let alpha_bits = u32(saturate(alpha) * f32(0xFFu) + 0.5);\n    return (depth_bits & 0xFFFFFFu) | ((alpha_bits & 0xFFu) << 24u);\n}\n\nfn unpack_24bit_depth_8bit_alpha(packed: u32) -> vec2<f32> {\n    let depth_bits = packed & 0xFFFFFFu;\n    let alpha_bits = (packed >> 24u) & 0xFFu;\n    return vec2(f32(depth_bits) / f32(0xFFFFFFu), f32(alpha_bits) / f32(0xFFu));\n}\n","bevy::core_pipeline::post_processing::chromatic_aberration":"// The chromatic aberration postprocessing effect.\n//\n// This makes edges of objects turn into multicolored streaks.\n\n\n// See `package::core_pipeline::post_process::ChromaticAberration` for more\n// information on these fields.\nstruct ChromaticAberrationSettings {\n    intensity: f32,\n    max_samples: u32,\n    unused_a: u32,\n    unused_b: u32,\n}\n\n// The source framebuffer texture.\n@group(0) @binding(0) var chromatic_aberration_source_texture: texture_2d<f32>;\n// The sampler used to sample the source framebuffer texture.\n@group(0) @binding(1) var chromatic_aberration_source_sampler: sampler;\n// The 1D lookup table for chromatic aberration.\n@group(0) @binding(2) var chromatic_aberration_lut_texture: texture_2d<f32>;\n// The sampler used to sample that lookup table.\n@group(0) @binding(3) var chromatic_aberration_lut_sampler: sampler;\n// The settings supplied by the developer.\n@group(0) @binding(4) var<uniform> chromatic_aberration_settings: ChromaticAberrationSettings;\n\nfn chromatic_aberration(start_pos: vec2<f32>) -> vec3<f32> {\n    // Radial chromatic aberration implemented using the *Inside* technique:\n    //\n    // <https://github.com/playdeadgames/publications/blob/master/INSIDE/rendering_inside_gdc2016.pdf>\n\n    let end_pos = mix(start_pos, vec2(0.5), chromatic_aberration_settings.intensity);\n\n    // Determine the number of samples. We aim for one sample per texel, unless\n    // that's higher than the developer-specified maximum number of samples, in\n    // which case we choose the maximum number of samples.\n    let texel_length = length((end_pos - start_pos) *\n        vec2<f32>(textureDimensions(chromatic_aberration_source_texture)));\n    let sample_count = min(u32(ceil(texel_length)), chromatic_aberration_settings.max_samples);\n\n    var color: vec3<f32>;\n    if (sample_count > 1u) {\n        // The LUT texture is in clamp-to-edge mode, so we start at 0.5 texels\n        // from the sides so that we have a nice gradient over the entire LUT\n        // range.\n        let lut_u_offset = 0.5 / f32(textureDimensions(chromatic_aberration_lut_texture).x);\n\n        var sample_sum = vec3(0.0);\n        var modulate_sum = vec3(0.0);\n\n        // Start accumulating samples.\n        for (var sample_index = 0u; sample_index < sample_count; sample_index += 1u) {\n            let t = (f32(sample_index) + 0.5) / f32(sample_count);\n\n            // Sample the framebuffer.\n            let sample_uv = mix(start_pos, end_pos, t);\n            let sample = textureSampleLevel(\n                chromatic_aberration_source_texture,\n                chromatic_aberration_source_sampler,\n                sample_uv,\n                0.0,\n            ).rgb;\n\n            // Sample the LUT.\n            let lut_u = mix(lut_u_offset, 1.0 - lut_u_offset, t);\n            let modulate = textureSampleLevel(\n                chromatic_aberration_lut_texture,\n                chromatic_aberration_lut_sampler,\n                vec2(lut_u, 0.5),\n                0.0,\n            ).rgb;\n\n            // Modulate the sample by the LUT value.\n            sample_sum += sample * modulate;\n            modulate_sum += modulate;\n        }\n\n        color = sample_sum / modulate_sum;\n    } else {\n        // If there's only one sample, don't do anything. If we don't do this,\n        // then this shader will apply whatever tint is in the center of the LUT\n        // texture to such pixels, which is wrong.\n        color = textureSampleLevel(\n            chromatic_aberration_source_texture,\n            chromatic_aberration_source_sampler,\n            start_pos,\n            0.0,\n        ).rgb;\n    }\n\n    return color;\n}\n","bevy::core_pipeline::tonemapping":"import package::render::{\n    view::ColorGrading,\n    color_operations::{hsv_to_rgb, rgb_to_hsv},\n    maths::{PI_2, powsafe},\n};\n\nimport package::core_pipeline::tonemapping_lut_bindings::{\n    dt_lut_texture,\n    dt_lut_sampler,\n};\n\n// Half the size of the crossfade region between shadows and midtones and\n// between midtones and highlights. This value, 0.1, corresponds to 10% of the\n// gamut on either side of the cutoff point.\nconst LEVEL_MARGIN: f32 = 0.1;\n\n// The inverse reciprocal of twice the above, used when scaling the midtone\n// region.\nconst LEVEL_MARGIN_DIV: f32 = 0.5 / LEVEL_MARGIN;\n\nfn sample_current_lut(p: vec3<f32>) -> vec3<f32> {\n    // Don't include code that will try to sample from LUTs if tonemap method doesn't require it\n    // Allows this file to be imported without necessarily needing the lut texture bindings\n    @if(TONEMAP_METHOD_AGX)\n    return textureSampleLevel(dt_lut_texture, dt_lut_sampler, p, 0.0).rgb;\n    @elif(TONEMAP_METHOD_TONY_MC_MAPFACE)\n    return textureSampleLevel(dt_lut_texture, dt_lut_sampler, p, 0.0).rgb;\n    @elif(TONEMAP_METHOD_BLENDER_FILMIC)\n    return textureSampleLevel(dt_lut_texture, dt_lut_sampler, p, 0.0).rgb;\n    @else\n    return vec3(1.0, 0.0, 1.0);\n}\n\n// --------------------------------------\n// --- SomewhatBoringDisplayTransform ---\n// --------------------------------------\n// By Tomasz Stachowiak\n\nfn rgb_to_ycbcr(col: vec3<f32>) -> vec3<f32> {\n    let m = mat3x3<f32>(\n        0.2126, 0.7152, 0.0722,\n        -0.1146, -0.3854, 0.5,\n        0.5, -0.4542, -0.0458\n    );\n    return col * m;\n}\n\nfn ycbcr_to_rgb(col: vec3<f32>) -> vec3<f32> {\n    let m = mat3x3<f32>(\n        1.0, 0.0, 1.5748,\n        1.0, -0.1873, -0.4681,\n        1.0, 1.8556, 0.0\n    );\n    return max(vec3(0.0), col * m);\n}\n\nfn tonemap_curve(v: f32) -> f32 {\n    @if(false) {\n        // Large linear part in the lows, but compresses highs.\n        let c = v + v * v + 0.5 * v * v * v;\n        return c / (1.0 + c);\n    }\n    @else {\n        return 1.0 - exp(-v);\n    }\n}\n\nfn tonemap_curve3_(v: vec3<f32>) -> vec3<f32> {\n    return vec3(tonemap_curve(v.r), tonemap_curve(v.g), tonemap_curve(v.b));\n}\n\nfn somewhat_boring_display_transform(col: vec3<f32>) -> vec3<f32> {\n    var boring_color = col;\n    let ycbcr = rgb_to_ycbcr(boring_color);\n\n    let bt = tonemap_curve(length(ycbcr.yz) * 2.4);\n    var desat = max((bt - 0.7) * 0.8, 0.0);\n    desat *= desat;\n\n    let desat_col = mix(boring_color.rgb, ycbcr.xxx, desat);\n\n    let tm_luma = tonemap_curve(ycbcr.x);\n    let tm0 = boring_color.rgb * max(0.0, tm_luma / max(1e-5, tonemapping_luminance(boring_color.rgb)));\n    let final_mult = 0.97;\n    let tm1 = tonemap_curve3_(desat_col);\n\n    boring_color = mix(tm0, tm1, bt * bt);\n\n    return boring_color * final_mult;\n}\n\n// ------------------------------------------\n// ------------- Tony McMapface -------------\n// ------------------------------------------\n// By Tomasz Stachowiak\n// https://github.com/h3r2tic/tony-mc-mapface\n\nconst TONY_MC_MAPFACE_LUT_DIMS: f32 = 48.0;\n\nfn sample_tony_mc_mapface_lut(stimulus: vec3<f32>) -> vec3<f32> {\n    var uv = (stimulus / (stimulus + 1.0)) * (f32(TONY_MC_MAPFACE_LUT_DIMS - 1.0) / f32(TONY_MC_MAPFACE_LUT_DIMS)) + 0.5 / f32(TONY_MC_MAPFACE_LUT_DIMS);\n    return sample_current_lut(saturate(uv)).rgb;\n}\n\n// ---------------------------------\n// ---------- ACES Fitted ----------\n// ---------------------------------\n\n// Same base implementation that Godot 4.0 uses for Tonemap ACES.\n\n// https://github.com/TheRealMJP/BakingLab/blob/master/BakingLab/ACES.hlsl\n\n// The code in this file was originally written by Stephen Hill (@self_shadow), who deserves all\n// credit for coming up with this fit and implementing it. Buy him a beer next time you see him. :)\n\nfn RRTAndODTFit(v: vec3<f32>) -> vec3<f32> {\n    let a = v * (v + 0.0245786) - 0.000090537;\n    let b = v * (0.983729 * v + 0.4329510) + 0.238081;\n    return a / b;\n}\n\nfn ACESFitted(color: vec3<f32>) -> vec3<f32> {\n    var fitted_color = color;\n\n    // sRGB => XYZ => D65_2_D60 => AP1 => RRT_SAT\n    let rgb_to_rrt = mat3x3<f32>(\n        vec3(0.59719, 0.35458, 0.04823),\n        vec3(0.07600, 0.90834, 0.01566),\n        vec3(0.02840, 0.13383, 0.83777)\n    );\n\n    // ODT_SAT => XYZ => D60_2_D65 => sRGB\n    let odt_to_rgb = mat3x3<f32>(\n        vec3(1.60475, -0.53108, -0.07367),\n        vec3(-0.10208, 1.10813, -0.00605),\n        vec3(-0.00327, -0.07276, 1.07602)\n    );\n\n    fitted_color *= rgb_to_rrt;\n\n    // Apply RRT and ODT\n    fitted_color = RRTAndODTFit(fitted_color);\n\n    fitted_color *= odt_to_rgb;\n\n    // Clamp to [0, 1]\n    fitted_color = saturate(fitted_color);\n\n    return fitted_color;\n}\n\n// -------------------------------\n// ------------- AgX -------------\n// -------------------------------\n// By Troy Sobotka\n// https://github.com/MrLixm/AgXc\n// https://github.com/sobotka/AgX\n\n/*\n    Increase color saturation of the given color data.\n    :param color: expected sRGB primaries input\n    :param saturationAmount: expected 0-1 range with 1=neutral, 0=no saturation.\n    -- ref[2] [4]\n*/\nfn saturation(color: vec3<f32>, saturationAmount: f32) -> vec3<f32> {\n    let luma = tonemapping_luminance(color);\n    return mix(vec3(luma), color, vec3(saturationAmount));\n}\n\n/*\n    Output log domain encoded data.\n    Similar to OCIO lg2 AllocationTransform.\n    ref[0]\n*/\nfn convertOpenDomainToNormalizedLog2_(color: vec3<f32>, minimum_ev: f32, maximum_ev: f32) -> vec3<f32> {\n    let in_midgray = 0.18;\n\n    // remove negative before log transform\n    var normalized_color = max(vec3(0.0), color);\n    // avoid infinite issue with log -- ref[1]\n    normalized_color = select(normalized_color, 0.00001525878 + normalized_color, normalized_color  < vec3<f32>(0.00003051757));\n    normalized_color = clamp(\n        log2(normalized_color / in_midgray),\n        vec3(minimum_ev),\n        vec3(maximum_ev)\n    );\n    let total_exposure = maximum_ev - minimum_ev;\n\n    return (normalized_color - minimum_ev) / total_exposure;\n}\n\n// Inverse of above\nfn convertNormalizedLog2ToOpenDomain(color: vec3<f32>, minimum_ev: f32, maximum_ev: f32) -> vec3<f32> {\n    var open_color = color;\n    let in_midgray = 0.18;\n    let total_exposure = maximum_ev - minimum_ev;\n\n    open_color = (open_color * total_exposure) + minimum_ev;\n    open_color = pow(vec3(2.0), open_color);\n    open_color = open_color * in_midgray;\n\n    return open_color;\n}\n\n\n/*=================\n    Main processes\n=================*/\n\n// Prepare the data for display encoding. Converted to log domain.\nfn applyAgXLog(Image: vec3<f32>) -> vec3<f32> {\n    var prepared_image = max(vec3(0.0), Image); // clamp negatives\n    let r = dot(prepared_image, vec3(0.84247906, 0.0784336, 0.07922375));\n    let g = dot(prepared_image, vec3(0.04232824, 0.87846864, 0.07916613));\n    let b = dot(prepared_image, vec3(0.04237565, 0.0784336, 0.87914297));\n    prepared_image = vec3(r, g, b);\n\n    prepared_image = convertOpenDomainToNormalizedLog2_(prepared_image, -10.0, 6.5);\n\n    prepared_image = clamp(prepared_image, vec3(0.0), vec3(1.0));\n    return prepared_image;\n}\n\nfn applyLUT3D(Image: vec3<f32>, block_size: f32) -> vec3<f32> {\n    return sample_current_lut(Image * ((block_size - 1.0) / block_size) + 0.5 / block_size).rgb;\n}\n\n// -------------------------\n// -------------------------\n// -------------------------\n\nfn sample_blender_filmic_lut(stimulus: vec3<f32>) -> vec3<f32> {\n    let block_size = 64.0;\n    let normalized = saturate(convertOpenDomainToNormalizedLog2_(stimulus, -11.0, 12.0));\n    return applyLUT3D(normalized, block_size);\n}\n\n// from https://64.github.io/tonemapping/\n// reinhard on RGB oversaturates colors\nfn tonemapping_reinhard(color: vec3<f32>) -> vec3<f32> {\n    return color / (1.0 + color);\n}\n\nfn tonemapping_reinhard_extended(color: vec3<f32>, max_white: f32) -> vec3<f32> {\n    let numerator = color * (1.0 + (color / vec3<f32>(max_white * max_white)));\n    return numerator / (1.0 + color);\n}\n\n// luminance coefficients from Rec. 709.\n// https://en.wikipedia.org/wiki/Rec._709\nfn tonemapping_luminance(v: vec3<f32>) -> f32 {\n    return dot(v, vec3<f32>(0.2126, 0.7152, 0.0722));\n}\n\nfn tonemapping_change_luminance(c_in: vec3<f32>, l_out: f32) -> vec3<f32> {\n    let l_in = tonemapping_luminance(c_in);\n    return c_in * (l_out / l_in);\n}\n\nfn tonemapping_reinhard_luminance(color: vec3<f32>) -> vec3<f32> {\n    let l_old = tonemapping_luminance(color);\n    let l_new = l_old / (1.0 + l_old);\n    return tonemapping_change_luminance(color, l_new);\n}\n\nfn rgb_to_srgb_simple(color: vec3<f32>) -> vec3<f32> {\n    return pow(color, vec3<f32>(1.0 / 2.2));\n}\n\n// Source: Advanced VR Rendering, GDC 2015, Alex Vlachos, Valve, Slide 49\n// https://media.steampowered.com/apps/valve/2015/Alex_Vlachos_Advanced_VR_Rendering_GDC2015.pdf\nfn screen_space_dither(frag_coord: vec2<f32>) -> vec3<f32> {\n    var dither = vec3<f32>(dot(vec2<f32>(171.0, 231.0), frag_coord)).xxx;\n    dither = fract(dither.rgb / vec3<f32>(103.0, 71.0, 97.0));\n    return (dither - 0.5) / 255.0;\n}\n\n// Performs the \"sectional\" color grading: i.e. the color grading that applies\n// individually to shadows, midtones, and highlights.\nfn sectional_color_grading(\n    in: vec3<f32>,\n    color_grading: ptr<function, ColorGrading>,\n) -> vec3<f32> {\n    var color = in;\n\n    // Determine whether the color is a shadow, midtone, or highlight. Colors\n    // close to the edges are considered a mix of both, to avoid sharp\n    // discontinuities. The formulas are taken from Blender's compositor.\n\n    let level = (color.r + color.g + color.b) / 3.0;\n\n    // Determine whether this color is a shadow, midtone, or highlight. If close\n    // to the cutoff points, blend between the two to avoid sharp color\n    // discontinuities.\n    var levels = vec3(0.0);\n    let midtone_range = (*color_grading).midtone_range;\n    if (level < midtone_range.x - LEVEL_MARGIN) {\n        levels.x = 1.0;\n    } else if (level < midtone_range.x + LEVEL_MARGIN) {\n        levels.y = ((level - midtone_range.x) * LEVEL_MARGIN_DIV) + 0.5;\n        levels.z = 1.0 - levels.y;\n    } else if (level < midtone_range.y - LEVEL_MARGIN) {\n        levels.y = 1.0;\n    } else if (level < midtone_range.y + LEVEL_MARGIN) {\n        levels.z = ((level - midtone_range.y) * LEVEL_MARGIN_DIV) + 0.5;\n        levels.y = 1.0 - levels.z;\n    } else {\n        levels.z = 1.0;\n    }\n\n    // Calculate contrast/saturation/gamma/gain/lift.\n    let contrast = dot(levels, (*color_grading).contrast);\n    let saturation = dot(levels, (*color_grading).saturation);\n    let gamma = dot(levels, (*color_grading).gamma);\n    let gain = dot(levels, (*color_grading).gain);\n    let lift = dot(levels, (*color_grading).lift);\n\n    // Adjust saturation and contrast.\n    let luma = tonemapping_luminance(color);\n    color = luma + saturation * (color - luma);\n    color = 0.5 + (color - 0.5) * contrast;\n\n    // The [ASC CDL] formula for color correction. Given *i*, an input color, we\n    // have:\n    //\n    //     out = (i × s + o)ⁿ\n    //\n    // Following the normal photographic naming convention, *gain* is the *s*\n    // factor, *lift* is the *o* term, and the inverse of *gamma* is the *n*\n    // exponent.\n    //\n    // [ASC CDL]: https://en.wikipedia.org/wiki/ASC_CDL#Combined_Function\n    color = powsafe(color * gain + lift, 1.0 / gamma);\n\n    // Account for exposure.\n    color = color * powsafe(vec3(2.0), (*color_grading).exposure);\n    return max(color, vec3(0.0));\n}\n\nfn tone_mapping(in: vec4<f32>, in_color_grading: ColorGrading) -> vec4<f32> {\n    var color = max(in.rgb, vec3(0.0));\n    var color_grading = in_color_grading;   // So we can take pointers to it.\n\n    // Rotate hue if needed, by converting to and from HSV. Remember that hue is\n    // an angle, so it needs to be modulo 2π.\n    @if(HUE_ROTATE) {\n        var hsv = rgb_to_hsv(color);\n        hsv.r = (hsv.r + color_grading.hue) % PI_2;\n        color = hsv_to_rgb(hsv);\n    }\n\n    // Perform white balance correction. Conveniently, this is a linear\n    // transform. The matrix was pre-calculated from the temperature and tint\n    // values on the CPU.\n    @if(WHITE_BALANCE) {\n        color = max(color_grading.balance * color, vec3(0.0));\n    }\n\n    // Perform the \"sectional\" color grading: i.e. the color grading that\n    // applies individually to shadows, midtones, and highlights.\n    @if(SECTIONAL_COLOR_GRADING) {\n        color = sectional_color_grading(color, &color_grading);\n    }\n    @else {\n        // If we're not doing sectional color grading, the exposure might still need\n        // to be applied, for example when using auto exposure.\n        color = color * powsafe(vec3(2.0), color_grading.exposure);\n    }\n\n    // tone_mapping\n    @if(TONEMAP_METHOD_NONE) {\n        color = color;\n    }\n    @elif(TONEMAP_METHOD_REINHARD) {\n        color = tonemapping_reinhard(color.rgb);\n    }\n    @elif(TONEMAP_METHOD_REINHARD_LUMINANCE) {\n        color = tonemapping_reinhard_luminance(color.rgb);\n    }\n    @elif(TONEMAP_METHOD_ACES_FITTED) {\n        color = ACESFitted(color.rgb);\n    }\n    @elif(TONEMAP_METHOD_AGX) {\n        color = applyAgXLog(color);\n        color = applyLUT3D(color, 32.0);\n    }\n    @elif(TONEMAP_METHOD_SOMEWHAT_BORING_DISPLAY_TRANSFORM) {\n        color = somewhat_boring_display_transform(color.rgb);\n    }\n    @elif(TONEMAP_METHOD_TONY_MC_MAPFACE) {\n        color = sample_tony_mc_mapface_lut(color);\n    }\n    @elif(TONEMAP_METHOD_BLENDER_FILMIC) {\n        color = sample_blender_filmic_lut(color.rgb);\n    }\n\n    // Perceptual post tonemapping grading\n    color = saturation(color, color_grading.post_saturation);\n\n    return vec4(color, in.a);\n}\n\n// This is an **incredibly crude** approximation of the inverse of the tone mapping function.\n// We assume here that there's a simple linear relationship between the input and output\n// which is not true at all, but useful to at least preserve the overall luminance of colors\n// when sampling from an already tonemapped image. (e.g. for transmissive materials when HDR is off)\nfn approximate_inverse_tone_mapping(in: vec4<f32>, color_grading: ColorGrading) -> vec4<f32> {\n    let out = tone_mapping(in, color_grading);\n    let approximate_ratio = length(in.rgb) / length(out.rgb);\n    return vec4(in.rgb * approximate_ratio, in.a);\n}\n","bevy::core_pipeline::tonemapping_lut_bindings":"@group(0) @binding(constants::TONEMAPPING_LUT_TEXTURE_BINDING_INDEX) var dt_lut_texture: texture_3d<f32>;\n@group(0) @binding(constants::TONEMAPPING_LUT_SAMPLER_BINDING_INDEX) var dt_lut_sampler: sampler;\n\n","bevy::pbr::ambient":"import package::pbr::{\n    lighting::{EnvBRDFApprox, F_AB},\n    mesh_view_bindings::lights,\n};\n\n// A precomputed `NdotV` is provided because it is computed regardless,\n// but `world_normal` and the view vector `V` are provided separately for more advanced uses.\nfn ambient_light(\n    world_position: vec4<f32>,\n    world_normal: vec3<f32>,\n    V: vec3<f32>,\n    NdotV: f32,\n    diffuse_color: vec3<f32>,\n    specular_color: vec3<f32>,\n    perceptual_roughness: f32,\n    occlusion: vec3<f32>,\n) -> vec3<f32> {\n    let diffuse_ambient = EnvBRDFApprox(diffuse_color, F_AB(1.0, NdotV));\n    let specular_ambient = EnvBRDFApprox(specular_color, F_AB(perceptual_roughness, NdotV));\n\n    // No real world material has specular values under 0.02, so we use this range as a\n    // \"pre-baked specular occlusion\" that extinguishes the fresnel term, for artistic control.\n    // See: https://google.github.io/filament/Filament.html#specularocclusion\n    let specular_occlusion = saturate(dot(specular_color, vec3(50.0 * 0.33)));\n\n    return (diffuse_ambient + specular_ambient * specular_occlusion) * lights.ambient_color.rgb * occlusion;\n}\n","bevy::pbr::atmosphere::bindings":"import package::render::view::View;\n\nimport package::pbr::{\n    mesh_view_types::Lights,\n    atmosphere::types::{Atmosphere, AtmosphereSettings, AtmosphereTransforms}\n};\n\n@group(0) @binding(0) var<uniform> atmosphere: Atmosphere;\n@group(0) @binding(1) var<uniform> settings: AtmosphereSettings;\n@group(0) @binding(2) var<uniform> atmosphere_transforms: AtmosphereTransforms;\n@group(0) @binding(3) var<uniform> view: View;\n@group(0) @binding(4) var<uniform> lights: Lights;\n@group(0) @binding(5) var transmittance_lut: texture_2d<f32>;\n@group(0) @binding(6) var transmittance_lut_sampler: sampler;\n@group(0) @binding(7) var multiscattering_lut: texture_2d<f32>;\n@group(0) @binding(8) var multiscattering_lut_sampler: sampler;\n@group(0) @binding(9) var sky_view_lut: texture_2d<f32>;\n@group(0) @binding(10) var sky_view_lut_sampler: sampler;\n@group(0) @binding(11) var aerial_view_lut: texture_3d<f32>;\n@group(0) @binding(12) var aerial_view_lut_sampler: sampler;\n","bevy::pbr::atmosphere::bruneton_functions":"// Copyright (c) 2017 Eric Bruneton\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions\n// are met:\n// 1. Redistributions of source code must retain the above copyright\n//    notice, this list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright\n//    notice, this list of conditions and the following disclaimer in the\n//    documentation and/or other materials provided with the distribution.\n// 3. Neither the name of the copyright holders nor the names of its\n//    contributors may be used to endorse or promote products derived from\n//    this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF\n// THE POSSIBILITY OF SUCH DAMAGE.\n//\n// Precomputed Atmospheric Scattering\n// Copyright (c) 2008 INRIA\n// All rights reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions\n// are met:\n// 1. Redistributions of source code must retain the above copyright\n//    notice, this list of conditions and the following disclaimer.\n// 2. Redistributions in binary form must reproduce the above copyright\n//    notice, this list of conditions and the following disclaimer in the\n//    documentation and/or other materials provided with the distribution.\n// 3. Neither the name of the copyright holders nor the names of its\n//    contributors may be used to endorse or promote products derived from\n//    this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF\n// THE POSSIBILITY OF SUCH DAMAGE.\n\n\nimport package::pbr::atmosphere::{\n    types::Atmosphere,\n    bindings::atmosphere,\n};\n\n// Mapping from view height (r) and zenith cos angle (mu) to UV coordinates in the transmittance LUT\n// Assuming r between ground and top atmosphere boundary, and mu= cos(zenith_angle)\n// Chosen to increase precision near the ground and to work around a discontinuity at the horizon\n// See Bruneton and Neyret 2008, \"Precomputed Atmospheric Scattering\" section 4\nfn transmittance_lut_r_mu_to_uv(r: f32, mu: f32) -> vec2<f32> {\n  // Distance along a horizontal ray from the ground to the top atmosphere boundary\n    let H = sqrt(atmosphere.top_radius * atmosphere.top_radius - atmosphere.bottom_radius * atmosphere.bottom_radius);\n\n  // Distance from a point at height r to the horizon\n  // ignore the case where r <= atmosphere.bottom_radius\n    let rho = sqrt(max(r * r - atmosphere.bottom_radius * atmosphere.bottom_radius, 0.0));\n\n  // Distance from a point at height r to the top atmosphere boundary at zenith angle mu\n    let d = distance_to_top_atmosphere_boundary(r, mu);\n\n  // Minimum and maximum distance to the top atmosphere boundary from a point at height r\n    let d_min = atmosphere.top_radius - r; // length of the ray straight up to the top atmosphere boundary\n    let d_max = rho + H; // length of the ray to the top atmosphere boundary and grazing the horizon\n\n    let u = (d - d_min) / (d_max - d_min);\n    let v = rho / H;\n    return vec2<f32>(u, v);\n}\n\n// Inverse of the mapping above, mapping from UV coordinates in the transmittance LUT to view height (r) and zenith cos angle (mu)\nfn transmittance_lut_uv_to_r_mu(uv: vec2<f32>) -> vec2<f32> {\n  // Distance to top atmosphere boundary for a horizontal ray at ground level\n    let H = sqrt(atmosphere.top_radius * atmosphere.top_radius - atmosphere.bottom_radius * atmosphere.bottom_radius);\n\n  // Distance to the horizon, from which we can compute r:\n    let rho = H * uv.y;\n    let r = sqrt(rho * rho + atmosphere.bottom_radius * atmosphere.bottom_radius);\n\n  // Distance to the top atmosphere boundary for the ray (r,mu), and its minimum\n  // and maximum values over all mu- obtained for (r,1) and (r,mu_horizon) -\n  // from which we can recover mu:\n    let d_min = atmosphere.top_radius - r;\n    let d_max = rho + H;\n    let d = d_min + uv.x * (d_max - d_min);\n\n    var mu: f32;\n    if d == 0.0 {\n        mu = 1.0;\n    } else {\n        mu = (H * H - rho * rho - d * d) / (2.0 * r * d);\n    }\n\n    mu = clamp(mu, -1.0, 1.0);\n\n    return vec2<f32>(r, mu);\n}\n\n/// Simplified ray-sphere intersection\n/// where:\n/// Ray origin, o = [0,0,r] with r <= atmosphere.top_radius\n/// mu is the cosine of spherical coordinate theta (-1.0 <= mu <= 1.0)\n/// so ray direction in spherical coordinates is [1,acos(mu),0] which needs to be converted to cartesian\n/// Direction of ray, u = [0,sqrt(1-mu*mu),mu]\n/// Center of sphere, c = [0,0,0]\n/// Radius of sphere, r = atmosphere.top_radius\n/// This function solves the quadratic equation for line-sphere intersection simplified under these assumptions\nfn distance_to_top_atmosphere_boundary(r: f32, mu: f32) -> f32 {\n  // ignore the case where r > atmosphere.top_radius\n    let positive_discriminant = max(r * r * (mu * mu - 1.0) + atmosphere.top_radius * atmosphere.top_radius, 0.0);\n    return max(-r * mu + sqrt(positive_discriminant), 0.0);\n}\n\n/// Simplified ray-sphere intersection\n/// as above for intersections with the ground\nfn distance_to_bottom_atmosphere_boundary(r: f32, mu: f32) -> f32 {\n    let positive_discriminant = max(r * r * (mu * mu - 1.0) + atmosphere.bottom_radius * atmosphere.bottom_radius, 0.0);\n    return max(-r * mu - sqrt(positive_discriminant), 0.0);\n}\n\nfn ray_intersects_ground(r: f32, mu: f32) -> bool {\n    return mu < 0.0 && r * r * (mu * mu - 1.0) + atmosphere.bottom_radius * atmosphere.bottom_radius >= 0.0;\n}\n","bevy::pbr::atmosphere::functions":"import package::render::maths::{PI, HALF_PI, PI_2, fast_acos, fast_acos_4, fast_atan2};\n\nimport package::pbr::atmosphere::{\n    types::Atmosphere,\n    bindings::{\n        atmosphere, settings, view, lights, transmittance_lut, transmittance_lut_sampler, \n        multiscattering_lut, multiscattering_lut_sampler, sky_view_lut, sky_view_lut_sampler,\n        aerial_view_lut, aerial_view_lut_sampler, atmosphere_transforms\n    },\n    bruneton_functions::{\n        transmittance_lut_r_mu_to_uv, transmittance_lut_uv_to_r_mu, \n        ray_intersects_ground, distance_to_top_atmosphere_boundary, \n        distance_to_bottom_atmosphere_boundary\n    },\n};\n\n// NOTE FOR CONVENTIONS: \n// r:\n//   radius, or distance from planet center \n//\n// altitude:\n//   distance from planet **surface**\n//\n// mu:\n//   cosine of the zenith angle of a ray with\n//   respect to the planet normal\n//\n// atmosphere space:\n//   abbreviated as \"as\" (contrast with vs, cs, ws), this space is similar\n//   to view space, but with the camera positioned horizontally on the planet\n//   surface, so the horizon is a horizontal line centered vertically in the\n//   frame. This enables the non-linear latitude parametrization the paper uses \n//   to concentrate detail near the horizon \n\n\n// CONSTANTS\n\nconst FRAC_PI: f32 = 0.3183098862; // 1 / π\nconst FRAC_2_PI: f32 = 0.15915494309;  // 1 / (2π)\nconst FRAC_3_16_PI: f32 = 0.0596831036594607509; // 3 / (16π)\nconst FRAC_4_PI: f32 = 0.07957747154594767; // 1 / (4π)\nconst ROOT_2: f32 = 1.41421356; // √2\n\n// During raymarching, each segment is sampled at a single point. This constant determines\n// where in the segment that sample is taken (0.0 = start, 0.5 = middle, 1.0 = end).\n// We use 0.3 to sample closer to the start of each segment, which better approximates\n// the exponential falloff of atmospheric density.\nconst MIDPOINT_RATIO: f32 = 0.3;\n\n// LUT UV PARAMETERIZATIONS\n\nfn unit_to_sub_uvs(val: vec2<f32>, resolution: vec2<f32>) -> vec2<f32> {\n    return (val + 0.5f / resolution) * (resolution / (resolution + 1.0f));\n}\n\nfn sub_uvs_to_unit(val: vec2<f32>, resolution: vec2<f32>) -> vec2<f32> {\n    return (val - 0.5f / resolution) * (resolution / (resolution - 1.0f));\n}\n\nfn multiscattering_lut_r_mu_to_uv(r: f32, mu: f32) -> vec2<f32> {\n    let u = 0.5 + 0.5 * mu;\n    let v = saturate((r - atmosphere.bottom_radius) / (atmosphere.top_radius - atmosphere.bottom_radius)); //TODO\n    return unit_to_sub_uvs(vec2(u, v), vec2<f32>(settings.multiscattering_lut_size));\n}\n\nfn multiscattering_lut_uv_to_r_mu(uv: vec2<f32>) -> vec2<f32> {\n    let adj_uv = sub_uvs_to_unit(uv, vec2<f32>(settings.multiscattering_lut_size));\n    let r = mix(atmosphere.bottom_radius, atmosphere.top_radius, adj_uv.y);\n    let mu = adj_uv.x * 2 - 1;\n    return vec2(r, mu);\n}\n\nfn sky_view_lut_r_mu_azimuth_to_uv(r: f32, mu: f32, azimuth: f32) -> vec2<f32> {\n    let u = (azimuth * FRAC_2_PI) + 0.5;\n\n    let v_horizon = sqrt(r * r - atmosphere.bottom_radius * atmosphere.bottom_radius);\n    let cos_beta = v_horizon / r;\n    // Using fast_acos_4 for better precision at small angles\n    // to avoid artifacts at the horizon\n    let beta = fast_acos_4(cos_beta);\n    let horizon_zenith = PI - beta;\n    let view_zenith = fast_acos_4(mu);\n\n    // Apply non-linear transformation to compress more texels \n    // near the horizon where high-frequency details matter most\n    // l is latitude in [-π/2, π/2] and v is texture coordinate in [0,1]\n    let l = view_zenith - horizon_zenith;\n    let abs_l = abs(l);\n\n    let v = 0.5 + 0.5 * sign(l) * sqrt(abs_l / HALF_PI);\n\n    return unit_to_sub_uvs(vec2(u, v), vec2<f32>(settings.sky_view_lut_size));\n}\n\nfn sky_view_lut_uv_to_zenith_azimuth(r: f32, uv: vec2<f32>) -> vec2<f32> {\n    let adj_uv = sub_uvs_to_unit(vec2(uv.x, 1.0 - uv.y), vec2<f32>(settings.sky_view_lut_size));\n    let azimuth = (adj_uv.x - 0.5) * PI_2;\n\n    // Horizon parameters\n    let v_horizon = sqrt(r * r - atmosphere.bottom_radius * atmosphere.bottom_radius);\n    let cos_beta = v_horizon / r;\n    let beta = fast_acos_4(cos_beta);\n    let horizon_zenith = PI - beta;\n\n    // Inverse of horizon-detail mapping to recover original latitude from texture coordinate\n    let t = abs(2.0 * (adj_uv.y - 0.5));\n    let l = sign(adj_uv.y - 0.5) * HALF_PI * t * t;\n\n    return vec2(horizon_zenith - l, azimuth);\n}\n\n// LUT SAMPLING\n\nfn sample_transmittance_lut(r: f32, mu: f32) -> vec3<f32> {\n    let uv = transmittance_lut_r_mu_to_uv(r, mu);\n    return textureSampleLevel(transmittance_lut, transmittance_lut_sampler, uv, 0.0).rgb;\n}\n\n// NOTICE: This function is copyrighted by Eric Bruneton and INRIA, and falls\n// under the license reproduced in bruneton_functions.wgsl (variant of MIT license)\n//\n// FIXME: this function should be in bruneton_functions.wgsl, but because naga_oil doesn't \n// support cyclic imports it's stuck here\nfn sample_transmittance_lut_segment(r: f32, mu: f32, t: f32) -> vec3<f32> {\n    let r_t = get_local_r(r, mu, t);\n    let mu_t = clamp((r * mu + t) / r_t, -1.0, 1.0);\n\n    if ray_intersects_ground(r, mu) {\n        return min(\n            sample_transmittance_lut(r_t, -mu_t) / sample_transmittance_lut(r, -mu),\n            vec3(1.0)\n        );\n    } else {\n        return min(\n            sample_transmittance_lut(r, mu) / sample_transmittance_lut(r_t, mu_t), vec3(1.0)\n        );\n    }\n}\n\nfn sample_multiscattering_lut(r: f32, mu: f32) -> vec3<f32> {\n    let uv = multiscattering_lut_r_mu_to_uv(r, mu);\n    return textureSampleLevel(multiscattering_lut, multiscattering_lut_sampler, uv, 0.0).rgb;\n}\n\nfn sample_sky_view_lut(r: f32, ray_dir_as: vec3<f32>) -> vec3<f32> {\n    let mu = ray_dir_as.y;\n    let azimuth = fast_atan2(ray_dir_as.x, -ray_dir_as.z);\n    let uv = sky_view_lut_r_mu_azimuth_to_uv(r, mu, azimuth);\n    return textureSampleLevel(sky_view_lut, sky_view_lut_sampler, uv, 0.0).rgb;\n}\n\nfn ndc_to_camera_dist(ndc: vec3<f32>) -> f32 {\n    let view_pos = view.view_from_clip * vec4(ndc, 1.0);\n    let t = length(view_pos.xyz / view_pos.w) * settings.scene_units_to_m;\n    return t;\n}\n\n// RGB channels: total inscattered light along the camera ray to the current sample.\n// A channel: average transmittance across all wavelengths to the current sample.\nfn sample_aerial_view_lut(uv: vec2<f32>, t: f32) -> vec3<f32> {\n    let t_max = settings.aerial_view_lut_max_distance;\n    let num_slices = f32(settings.aerial_view_lut_size.z);\n    // Each texel stores the value of the scattering integral over the whole slice,\n    // which requires us to offset the w coordinate by half a slice. For\n    // example, if we wanted the value of the integral at the boundary between slices,\n    // we'd need to sample at the center of the previous slice, and vice-versa for\n    // sampling in the center of a slice.\n    let uvw = vec3(uv, saturate(t / t_max - 0.5 / num_slices));\n    let sample = textureSampleLevel(aerial_view_lut, aerial_view_lut_sampler, uvw, 0.0);\n    // Since sampling anywhere between w=0 and w=t_slice will clamp to the first slice,\n    // we need to do a linear step over the first slice towards zero at the camera's\n    // position to recover the correct integral value.\n    let t_slice = t_max / num_slices;\n    let fade = saturate(t / t_slice);\n    // Recover the values from log space\n    return exp(sample.rgb) * fade;\n}\n\n// PHASE FUNCTIONS\n\n// -(L . V) == (L . -V). -V here is our ray direction, which points away from the view \n// instead of towards it (which would be the *view direction*, V)\n\n// evaluates the rayleigh phase function, which describes the likelihood\n// of a rayleigh scattering event scattering light from the light direction towards the view\nfn rayleigh(neg_LdotV: f32) -> f32 {\n    return FRAC_3_16_PI * (1 + (neg_LdotV * neg_LdotV));\n}\n\n// evaluates the henyey-greenstein phase function, which describes the likelihood\n// of a mie scattering event scattering light from the light direction towards the view\nfn henyey_greenstein(neg_LdotV: f32) -> f32 {\n    let g = atmosphere.mie_asymmetry;\n    let denom = 1.0 + g * g - 2.0 * g * neg_LdotV;\n    return FRAC_4_PI * (1.0 - g * g) / (denom * sqrt(denom));\n}\n\n// ATMOSPHERE SAMPLING\n\nstruct AtmosphereSample {\n    /// units: m^-1\n    rayleigh_scattering: vec3<f32>,\n\n    /// units: m^-1\n    mie_scattering: f32,\n\n    /// the sum of scattering and absorption. Since the phase function doesn't\n    /// matter for this, we combine rayleigh and mie extinction to a single \n    //  value.\n    //\n    /// units: m^-1\n    extinction: vec3<f32>\n}\n\n/// Samples atmosphere optical densities at a given radius\nfn sample_atmosphere(r: f32) -> AtmosphereSample {\n    let altitude = clamp(r, atmosphere.bottom_radius, atmosphere.top_radius) - atmosphere.bottom_radius;\n\n    // atmosphere values at altitude\n    let mie_density = exp(-atmosphere.mie_density_exp_scale * altitude);\n    let rayleigh_density = exp(-atmosphere.rayleigh_density_exp_scale * altitude);\n    var ozone_density: f32 = max(0.0, 1.0 - (abs(altitude - atmosphere.ozone_layer_altitude) / (atmosphere.ozone_layer_width * 0.5)));\n\n    let mie_scattering = mie_density * atmosphere.mie_scattering;\n    let mie_absorption = mie_density * atmosphere.mie_absorption;\n    let mie_extinction = mie_scattering + mie_absorption;\n\n    let rayleigh_scattering = rayleigh_density * atmosphere.rayleigh_scattering;\n    // no rayleigh absorption\n    // rayleigh extinction is the sum of scattering and absorption\n\n    // ozone doesn't contribute to scattering\n    let ozone_absorption = ozone_density * atmosphere.ozone_absorption;\n\n    var sample: AtmosphereSample;\n    sample.rayleigh_scattering = rayleigh_scattering;\n    sample.mie_scattering = mie_scattering;\n    sample.extinction = rayleigh_scattering + mie_extinction + ozone_absorption;\n\n    return sample;\n}\n\n/// evaluates L_scat, equation 3 in the paper, which gives the total single-order scattering towards the view at a single point\nfn sample_local_inscattering(local_atmosphere: AtmosphereSample, ray_dir: vec3<f32>, local_r: f32, local_up: vec3<f32>) -> vec3<f32> {\n    var inscattering = vec3(0.0);\n    for (var light_i: u32 = 0u; light_i < lights.n_directional_lights; light_i++) {\n        let light = &lights.directional_lights[light_i];\n\n        let mu_light = dot((*light).direction_to_light, local_up);\n\n        // -(L . V) == (L . -V). -V here is our ray direction, which points away from the view\n        // instead of towards it (as is the convention for V)\n        let neg_LdotV = dot((*light).direction_to_light, ray_dir);\n\n        // Phase functions give the proportion of light\n        // scattered towards the camera for each scattering type\n        let rayleigh_phase = rayleigh(neg_LdotV);\n        let mie_phase = henyey_greenstein(neg_LdotV);\n        let scattering_coeff = local_atmosphere.rayleigh_scattering * rayleigh_phase + local_atmosphere.mie_scattering * mie_phase;\n\n        let transmittance_to_light = sample_transmittance_lut(local_r, mu_light);\n        let shadow_factor = transmittance_to_light * f32(!ray_intersects_ground(local_r, mu_light));\n\n        // Transmittance from scattering event to light source\n        let scattering_factor = shadow_factor * scattering_coeff;\n\n        // Additive factor from the multiscattering LUT\n        let psi_ms = sample_multiscattering_lut(local_r, mu_light);\n        let multiscattering_factor = psi_ms * (local_atmosphere.rayleigh_scattering + local_atmosphere.mie_scattering);\n\n        inscattering += (*light).color.rgb * (scattering_factor + multiscattering_factor);\n    }\n    return inscattering * view.exposure;\n}\n\nconst SUN_ANGULAR_SIZE: f32 = 0.0174533; // angular diameter of sun in radians\n\nfn sample_sun_radiance(ray_dir_ws: vec3<f32>) -> vec3<f32> {\n    let r = view_radius();\n    let mu_view = ray_dir_ws.y;\n    let shadow_factor = f32(!ray_intersects_ground(r, mu_view));\n    var sun_radiance = vec3(0.0);\n    for (var light_i: u32 = 0u; light_i < lights.n_directional_lights; light_i++) {\n        let light = &lights.directional_lights[light_i];\n        let neg_LdotV = dot((*light).direction_to_light, ray_dir_ws);\n        let angle_to_sun = fast_acos(neg_LdotV);\n        let pixel_size = fwidth(angle_to_sun);\n        let factor = smoothstep(0.0, -pixel_size * ROOT_2, angle_to_sun - SUN_ANGULAR_SIZE * 0.5);\n        let sun_solid_angle = (SUN_ANGULAR_SIZE * SUN_ANGULAR_SIZE) * 4.0 * FRAC_PI;\n        sun_radiance += ((*light).color.rgb / sun_solid_angle) * factor * shadow_factor;\n    }\n    return sun_radiance;\n}\n\n// TRANSFORM UTILITIES\n\nfn max_atmosphere_distance(r: f32, mu: f32) -> f32 {\n    let t_top = distance_to_top_atmosphere_boundary(r, mu);\n    let t_bottom = distance_to_bottom_atmosphere_boundary(r, mu);\n    let hits = ray_intersects_ground(r, mu);\n    return mix(t_top, t_bottom, f32(hits));\n}\n\n/// Assuming y=0 is the planet ground, returns the view radius in meters\nfn view_radius() -> f32 {\n    return view.world_position.y * settings.scene_units_to_m + atmosphere.bottom_radius;\n}\n\n// We assume the `up` vector at the view position is the y axis, since the world is locally flat/level.\n// t = distance along view ray in atmosphere space\n// NOTE: this means that if your world is actually spherical, this will be wrong.\nfn get_local_up(r: f32, t: f32, ray_dir: vec3<f32>) -> vec3<f32> {\n    return normalize(vec3(0.0, r, 0.0) + t * ray_dir);\n}\n\n// Given a ray starting at radius r, with mu = cos(zenith angle),\n// and a t = distance along the ray, gives the new radius at point t\nfn get_local_r(r: f32, mu: f32, t: f32) -> f32 {\n    return sqrt(t * t + 2.0 * r * mu * t + r * r);\n}\n\n// Convert uv [0.0 .. 1.0] coordinate to ndc space xy [-1.0 .. 1.0]\nfn uv_to_ndc(uv: vec2<f32>) -> vec2<f32> {\n    return uv * vec2(2.0, -2.0) + vec2(-1.0, 1.0);\n}\n\n/// Convert ndc space xy coordinate [-1.0 .. 1.0] to uv [0.0 .. 1.0]\nfn ndc_to_uv(ndc: vec2<f32>) -> vec2<f32> {\n    return ndc * vec2(0.5, -0.5) + vec2(0.5);\n}\n\n/// Converts a direction in world space to atmosphere space\nfn direction_world_to_atmosphere(dir_ws: vec3<f32>) -> vec3<f32> {\n    let dir_as = atmosphere_transforms.atmosphere_from_world * vec4(dir_ws, 0.0);\n    return dir_as.xyz;\n}\n\n/// Converts a direction in atmosphere space to world space\nfn direction_atmosphere_to_world(dir_as: vec3<f32>) -> vec3<f32> {\n    let dir_ws = atmosphere_transforms.world_from_atmosphere * vec4(dir_as, 0.0);\n    return dir_ws.xyz;\n}\n\n// Modified from skybox.wgsl. For this pass we don't need to apply a separate sky transform or consider camera viewport.\n// w component is the cosine of the view direction with the view forward vector, to correct step distance at the edges of the viewport\nfn uv_to_ray_direction(uv: vec2<f32>) -> vec4<f32> {\n    // Using world positions of the fragment and camera to calculate a ray direction\n    // breaks down at large translations. This code only needs to know the ray direction.\n    // The ray direction is along the direction from the camera to the fragment position.\n    // In view space, the camera is at the origin, so the view space ray direction is\n    // along the direction of the fragment position - (0,0,0) which is just the\n    // fragment position.\n    // Use the position on the near clipping plane to avoid -inf world position\n    // because the far plane of an infinite reverse projection is at infinity.\n    let view_position_homogeneous = view.view_from_clip * vec4(\n        uv_to_ndc(uv),\n        1.0,\n        1.0,\n    );\n\n    let view_ray_direction = view_position_homogeneous.xyz / view_position_homogeneous.w;\n    // Transforming the view space ray direction by the inverse view matrix, transforms the\n    // direction to world space. Note that the w element is set to 0.0, as this is a\n    // vector direction, not a position, That causes the matrix multiplication to ignore\n    // the translations from the view matrix.\n    let ray_direction = (view.world_from_view * vec4(view_ray_direction, 0.0)).xyz;\n\n    return vec4(normalize(ray_direction), -view_ray_direction.z);\n}\n\nfn zenith_azimuth_to_ray_dir(zenith: f32, azimuth: f32) -> vec3<f32> {\n    let sin_zenith = sin(zenith);\n    let mu = cos(zenith);\n    let sin_azimuth = sin(azimuth);\n    let cos_azimuth = cos(azimuth);\n    return vec3(sin_azimuth * sin_zenith, mu, -cos_azimuth * sin_zenith);\n}\n","bevy::pbr::atmosphere::types":"struct Atmosphere {\n    // Radius of the planet\n    bottom_radius: f32, // units: m\n\n    // Radius at which we consider the atmosphere to 'end' for out calculations (from center of planet)\n    top_radius: f32, // units: m\n\n    ground_albedo: vec3<f32>,\n\n    rayleigh_density_exp_scale: f32,\n    rayleigh_scattering: vec3<f32>,\n\n    mie_density_exp_scale: f32,\n    mie_scattering: f32, // units: m^-1\n    mie_absorption: f32, // units: m^-1\n    mie_asymmetry: f32, // the \"asymmetry\" value of the phase function, unitless. Domain: (-1, 1)\n\n    ozone_layer_altitude: f32, // units: m\n    ozone_layer_width: f32, // units: m\n    ozone_absorption: vec3<f32>, // ozone absorption. units: m^-1\n}\n\nstruct AtmosphereSettings {\n    transmittance_lut_size: vec2<u32>,\n    multiscattering_lut_size: vec2<u32>,\n    sky_view_lut_size: vec2<u32>,\n    aerial_view_lut_size: vec3<u32>,\n    transmittance_lut_samples: u32,\n    multiscattering_lut_dirs: u32,\n    multiscattering_lut_samples: u32,\n    sky_view_lut_samples: u32,\n    aerial_view_lut_samples: u32,\n    aerial_view_lut_max_distance: f32,\n    scene_units_to_m: f32,\n}\n\n\n// \"Atmosphere space\" is just the view position with y=0 and oriented horizontally,\n// so the horizon stays a horizontal line in our luts\nstruct AtmosphereTransforms {\n    world_from_atmosphere: mat4x4<f32>,\n    atmosphere_from_world: mat4x4<f32>,\n}\n","bevy::pbr::clustered_forward":"import package::pbr::{\n    mesh_view_bindings as bindings,\n    utils::rand_f,\n};\n\nimport package::render::{\n   color_operations::hsv_to_rgb,\n   maths::PI_2,\n};\n\n// Offsets within the `cluster_offsets_and_counts` buffer for a single cluster.\n//\n// These offsets must be monotonically nondecreasing. That is, indices are\n// always sorted into the following order: point lights, spot lights, reflection\n// probes, irradiance volumes.\nstruct ClusterableObjectIndexRanges {\n    // The offset of the index of the first point light.\n    first_point_light_index_offset: u32,\n    // The offset of the index of the first spot light, which also terminates\n    // the list of point lights.\n    first_spot_light_index_offset: u32,\n    // The offset of the index of the first reflection probe, which also\n    // terminates the list of spot lights.\n    first_reflection_probe_index_offset: u32,\n    // The offset of the index of the first irradiance volumes, which also\n    // terminates the list of reflection probes.\n    first_irradiance_volume_index_offset: u32,\n    first_decal_offset: u32,\n    // One past the offset of the index of the final clusterable object for this\n    // cluster.\n    last_clusterable_object_index_offset: u32,\n}\n\n// NOTE: Keep in sync with bevy/pbr/src/light.rs\nfn view_z_to_z_slice(view_z: f32, is_orthographic: bool) -> u32 {\n    var z_slice: u32 = 0u;\n    if is_orthographic {\n        // NOTE: view_z is correct in the orthographic case\n        z_slice = u32(floor((view_z - bindings::lights.cluster_factors.z) * bindings::lights.cluster_factors.w));\n    } else {\n        // NOTE: had to use -view_z to make it positive else log(negative) is nan\n        z_slice = u32(log(-view_z) * bindings::lights.cluster_factors.z - bindings::lights.cluster_factors.w + 1.0);\n    }\n    // NOTE: We use min as we may limit the far z plane used for clustering to be closer than\n    // the furthest thing being drawn. This means that we need to limit to the maximum cluster.\n    return min(z_slice, bindings::lights.cluster_dimensions.z - 1u);\n}\n\nfn fragment_cluster_index(frag_coord: vec2<f32>, view_z: f32, is_orthographic: bool) -> u32 {\n    let xy = vec2<u32>(floor((frag_coord - bindings::view.viewport.xy) * bindings::lights.cluster_factors.xy));\n    let z_slice = view_z_to_z_slice(view_z, is_orthographic);\n    // NOTE: Restricting cluster index to avoid undefined behavior when accessing uniform buffer\n    // arrays based on the cluster index.\n    return min(\n        (xy.y * bindings::lights.cluster_dimensions.x + xy.x) * bindings::lights.cluster_dimensions.z + z_slice,\n        bindings::lights.cluster_dimensions.w - 1u\n    );\n}\n\n// this must match CLUSTER_COUNT_SIZE in light.rs\nconst CLUSTER_COUNT_SIZE = 9u;\n\n// Returns the indices of clusterable objects belonging to the given cluster.\n//\n// Note that if fewer than 3 SSBO bindings are available (in WebGL 2,\n// primarily), light probes aren't clustered, and therefore both light probe\n// index ranges will be empty.\nfn unpack_clusterable_object_index_ranges(cluster_index: u32) -> ClusterableObjectIndexRanges {\n    @if(AVAILABLE_STORAGE_BUFFER_BINDINGS__GE_3) {\n        let offset_and_counts_a = bindings::cluster_offsets_and_counts.data[cluster_index][0];\n        let offset_and_counts_b = bindings::cluster_offsets_and_counts.data[cluster_index][1];\n\n        // Sum up the counts to produce the range brackets.\n        //\n        // We could have stored the range brackets in `cluster_offsets_and_counts`\n        // directly, but doing it this way makes the logic in this path more\n        // consistent with the WebGL 2 path below.\n        let point_light_offset = offset_and_counts_a.x;\n        let spot_light_offset = point_light_offset + offset_and_counts_a.y;\n        let reflection_probe_offset = spot_light_offset + offset_and_counts_a.z;\n        let irradiance_volume_offset = reflection_probe_offset + offset_and_counts_a.w;\n        let decal_offset = irradiance_volume_offset + offset_and_counts_b.x;\n        let last_clusterable_offset = decal_offset + offset_and_counts_b.y;\n        return ClusterableObjectIndexRanges(\n            point_light_offset,\n            spot_light_offset,\n            reflection_probe_offset,\n            irradiance_volume_offset,\n            decal_offset,\n            last_clusterable_offset\n        );\n    }\n    @else {\n        let raw_offset_and_counts = bindings::cluster_offsets_and_counts.data[cluster_index >> 2u][cluster_index & ((1u << 2u) - 1u)];\n        //  [ 31     ..     18 | 17      ..      9 | 8       ..     0 ]\n        //  [      offset      | point light count | spot light count ]\n        let offset_and_counts = vec3<u32>(\n            (raw_offset_and_counts >> (CLUSTER_COUNT_SIZE * 2u)) & ((1u << (32u - (CLUSTER_COUNT_SIZE * 2u))) - 1u),\n            (raw_offset_and_counts >> CLUSTER_COUNT_SIZE)        & ((1u << CLUSTER_COUNT_SIZE) - 1u),\n            raw_offset_and_counts                                & ((1u << CLUSTER_COUNT_SIZE) - 1u),\n        );\n\n        // We don't cluster reflection probes or irradiance volumes on this\n        // platform, as there's no room in the UBO. Thus, those offset ranges\n        // (corresponding to `offset_d` and `offset_e` above) are empty and are\n        // simply copies of `offset_c`.\n\n        let offset_a = offset_and_counts.x;\n        let offset_b = offset_a + offset_and_counts.y;\n        let offset_c = offset_b + offset_and_counts.z;\n\n        return ClusterableObjectIndexRanges(offset_a, offset_b, offset_c, offset_c, offset_c, offset_c);\n    }\n}\n\n// Returns the index of the clusterable object at the given offset.\n//\n// Note that, in the case of a light probe, the index refers to an element in\n// one of the two `light_probes` sublists, not the `clusterable_objects` list.\nfn get_clusterable_object_id(index: u32) -> u32 {\n    @if(AVAILABLE_STORAGE_BUFFER_BINDINGS__GE_3) {\n        return bindings::clusterable_object_index_lists.data[index];\n    }\n    @else {\n        // The index is correct but in clusterable_object_index_lists we pack 4 u8s into a u32\n        // This means the index into clusterable_object_index_lists is index / 4\n        let indices = bindings::clusterable_object_index_lists.data[index >> 4u][(index >> 2u) &\n            ((1u << 2u) - 1u)];\n        // And index % 4 gives the sub-index of the u8 within the u32 so we shift by 8 * sub-index\n        return (indices >> (8u * (index & ((1u << 2u) - 1u)))) & ((1u << 8u) - 1u);\n    }\n}\n\nfn cluster_debug_visualization(\n    input_color: vec4<f32>,\n    view_z: f32,\n    is_orthographic: bool,\n    clusterable_object_index_ranges: ClusterableObjectIndexRanges,\n    cluster_index: u32,\n) -> vec4<f32> {\n    var output_color = input_color;\n\n    // Cluster allocation debug (using 'over' alpha blending)\n    @if(CLUSTERED_FORWARD_DEBUG_Z_SLICES) {\n        // NOTE: This debug mode visualizes the z-slices\n        let cluster_overlay_alpha = 0.1;\n        var z_slice: u32 = view_z_to_z_slice(view_z, is_orthographic);\n        // A hack to make the colors alternate a bit more\n        if (z_slice & 1u) == 1u {\n            z_slice = z_slice + bindings::lights.cluster_dimensions.z / 2u;\n        }\n        let slice_color_hsv = vec3(\n            f32(z_slice) / f32(bindings::lights.cluster_dimensions.z + 1u) * PI_2,\n            1.0,\n            0.5\n        );\n        let slice_color = hsv_to_rgb(slice_color_hsv);\n        output_color = vec4<f32>(\n            (1.0 - cluster_overlay_alpha) * output_color.rgb + cluster_overlay_alpha * slice_color,\n            output_color.a\n        );\n    }\n    @if(CLUSTERED_FORWARD_DEBUG_CLUSTER_COMPLEXITY) {\n        // NOTE: This debug mode visualizes the number of clusterable objects within\n        // the cluster that contains the fragment. It shows a sort of cluster\n        // complexity measure.\n        let cluster_overlay_alpha = 0.1;\n        let max_complexity_per_cluster = 64.0;\n        let object_count = clusterable_object_index_ranges.first_reflection_probe_index_offset -\n            clusterable_object_index_ranges.first_point_light_index_offset;\n        output_color.r = (1.0 - cluster_overlay_alpha) * output_color.r + cluster_overlay_alpha *\n            smoothstep(0.0, max_complexity_per_cluster, f32(object_count));\n        output_color.g = (1.0 - cluster_overlay_alpha) * output_color.g + cluster_overlay_alpha *\n            (1.0 - smoothstep(0.0, max_complexity_per_cluster, f32(object_count)));\n    }\n    @if(CLUSTERED_FORWARD_DEBUG_CLUSTER_COHERENCY) {\n        // NOTE: Visualizes the cluster to which the fragment belongs\n        let cluster_overlay_alpha = 0.1;\n        var rng = cluster_index;\n        let cluster_color_hsv = vec3(rand_f(&rng) * PI_2, 1.0, 0.5);\n        let cluster_color = hsv_to_rgb(cluster_color_hsv);\n        output_color = vec4<f32>(\n            (1.0 - cluster_overlay_alpha) * output_color.rgb + cluster_overlay_alpha * cluster_color,\n            output_color.a\n        );\n    }\n\n    return output_color;\n}\n","bevy::pbr::decal::clustered":"// Support code for clustered decals.\n//\n// This module provides an iterator API, which you may wish to use in your own\n// shaders if you want clustered decals to provide textures other than the base\n// color. The iterator API allows you to iterate over all decals affecting the\n// current fragment. Use `clustered_decal_iterator_new()` and\n// `clustered_decal_iterator_next()` as follows:\n//\n//      let view_z = get_view_z(vec4(world_position, 1.0));\n//      let is_orthographic = view_is_orthographic();\n//\n//      let cluster_index =\n//          clustered_forward::fragment_cluster_index(frag_coord, view_z, is_orthographic);\n//      var clusterable_object_index_ranges =\n//          clustered_forward::unpack_clusterable_object_index_ranges(cluster_index);\n//\n//      var iterator = clustered_decal_iterator_new(world_position, &clusterable_object_index_ranges);\n//      while (clustered_decal_iterator_next(&iterator)) {\n//          ... sample from the texture at iterator.texture_index at iterator.uv ...\n//      }\n//\n// In this way, in conjunction with a custom material, you can provide your own\n// texture arrays that mirror `mesh_view_bindings::clustered_decal_textures` in\n// order to support decals with normal maps, etc.\n//\n// Note that the order in which decals are returned is currently unpredictable,\n// though generally stable from frame to frame.\n\n\n\nimport package::pbr::clustered_forward;\nimport package::pbr::clustered_forward::ClusterableObjectIndexRanges;\nimport package::pbr::mesh_view_bindings;\nimport package::render::maths;\n\n@if(!CLUSTERED_DECALS_ARE_USABLE)\nconst module_requires_flag_CLUSTERED_DECALS_ARE_USABLE = false;\n@if(!CLUSTERED_DECALS_ARE_USABLE)\nconst_assert module_requires_flag_CLUSTERED_DECALS_ARE_USABLE; // module requires feature flag CLUSTERED_DECALS_ARE_USABLE\n\n// An object that allows stepping through all clustered decals that affect a\n// single fragment.\nstruct ClusteredDecalIterator {\n    // Public fields follow:\n    // The index of the decal texture in the binding array.\n    texture_index: i32,\n    // The UV coordinates at which to sample that decal texture.\n    uv: vec2<f32>,\n    // A custom tag you can use for your own purposes.\n    tag: u32,\n\n    // Private fields follow:\n    // The current offset of the index in the `ClusterableObjectIndexRanges` list.\n    decal_index_offset: i32,\n    // The end offset of the index in the `ClusterableObjectIndexRanges` list.\n    end_offset: i32,\n    // The world-space position of the fragment.\n    world_position: vec3<f32>,\n}\n\n\n// Creates a new iterator over the decals at the current fragment.\n//\n// You can retrieve `clusterable_object_index_ranges` as follows:\n//\n//      let view_z = get_view_z(world_position);\n//      let is_orthographic = view_is_orthographic();\n//\n//      let cluster_index =\n//          clustered_forward::fragment_cluster_index(frag_coord, view_z, is_orthographic);\n//      var clusterable_object_index_ranges =\n//          clustered_forward::unpack_clusterable_object_index_ranges(cluster_index);\nfn clustered_decal_iterator_new(\n    world_position: vec3<f32>,\n    clusterable_object_index_ranges: ptr<function, ClusterableObjectIndexRanges>\n) -> ClusteredDecalIterator {\n    return ClusteredDecalIterator(\n        -1,\n        vec2(0.0),\n        0u,\n        // We subtract 1 because the first thing `decal_iterator_next` does is\n        // add 1.\n        i32((*clusterable_object_index_ranges).first_decal_offset) - 1,\n        i32((*clusterable_object_index_ranges).last_clusterable_object_index_offset),\n        world_position,\n    );\n}\n\n// Populates the `iterator.texture_index` and `iterator.uv` fields for the next\n// decal overlapping the current world position.\n//\n// Returns true if another decal was found or false if no more decals were found\n// for this position.\nfn clustered_decal_iterator_next(iterator: ptr<function, ClusteredDecalIterator>) -> bool {\n    if ((*iterator).decal_index_offset == (*iterator).end_offset) {\n        return false;\n    }\n\n    (*iterator).decal_index_offset += 1;\n\n    while ((*iterator).decal_index_offset < (*iterator).end_offset) {\n        let decal_index = i32(clustered_forward::get_clusterable_object_id(\n            u32((*iterator).decal_index_offset)\n        ));\n        let decal_space_vector =\n            (mesh_view_bindings::clustered_decals.decals[decal_index].local_from_world *\n            vec4((*iterator).world_position, 1.0)).xyz;\n\n        if (all(decal_space_vector >= vec3(-0.5)) && all(decal_space_vector <= vec3(0.5))) {\n            (*iterator).texture_index =\n                i32(mesh_view_bindings::clustered_decals.decals[decal_index].image_index);\n            (*iterator).uv = decal_space_vector.xy * vec2(1.0, -1.0) + vec2(0.5);\n            (*iterator).tag =\n                mesh_view_bindings::clustered_decals.decals[decal_index].tag;\n            return true;\n        }\n\n        (*iterator).decal_index_offset += 1;\n    }\n\n    return false;\n}\n\n\n// Returns the view-space Z coordinate for the given world position.\nfn get_view_z(world_position: vec3<f32>) -> f32 {\n    return dot(vec4<f32>(\n        mesh_view_bindings::view.view_from_world[0].z,\n        mesh_view_bindings::view.view_from_world[1].z,\n        mesh_view_bindings::view.view_from_world[2].z,\n        mesh_view_bindings::view.view_from_world[3].z\n    ), vec4(world_position, 1.0));\n}\n\n// Returns true if the current view describes an orthographic projection or\n// false otherwise.\nfn view_is_orthographic() -> bool {\n    return mesh_view_bindings::view.clip_from_view[3].w == 1.0;\n}\n\n// Modifies the base color at the given position to account for decals.\n//\n// Returns the new base color with decals taken into account. If no decals\n// overlap the current world position, returns the supplied base color\n// unmodified.\n// \n// TODO(mbr): I feature-gated this.\n// It used to work without the CLUSTERED_DECALS_ARE_USABLE flag.\nfn apply_decal_base_color(\n    world_position: vec3<f32>,\n    frag_coord: vec2<f32>,\n    initial_base_color: vec4<f32>,\n) -> vec4<f32> {\n    var base_color = initial_base_color;\n\n    // Fetch the clusterable object index ranges for this world position.\n\n    let view_z = get_view_z(world_position);\n    let is_orthographic = view_is_orthographic();\n\n    let cluster_index =\n        clustered_forward::fragment_cluster_index(frag_coord, view_z, is_orthographic);\n    var clusterable_object_index_ranges =\n        clustered_forward::unpack_clusterable_object_index_ranges(cluster_index);\n\n    // Iterate over decals.\n\n    var iterator = clustered_decal_iterator_new(world_position, &clusterable_object_index_ranges);\n    while (clustered_decal_iterator_next(&iterator)) {\n        // Sample the current decal.\n        let decal_base_color = textureSampleLevel(\n            mesh_view_bindings::clustered_decal_textures[iterator.texture_index],\n            mesh_view_bindings::clustered_decal_sampler,\n            iterator.uv,\n            0.0\n        );\n\n        // Blend with the accumulated fragment.\n        base_color = vec4(\n            mix(base_color.rgb, decal_base_color.rgb, decal_base_color.a),\n            base_color.a + decal_base_color.a\n        );\n    }\n\n    return base_color;\n}\n\n","bevy::pbr::decal::forward":"import package::pbr::{\n    forward_io::VertexOutput,\n    mesh_functions::get_world_from_local,\n    mesh_view_bindings::view,\n    pbr_functions::calculate_tbn_mikktspace,\n    prepass_utils::prepass_depth,\n    view_transformations::depth_ndc_to_view_z,\n};\nimport package::render::maths::project_onto;\n\n@group(2) @binding(200)\nvar<uniform> inv_depth_fade_factor: f32;\n\nstruct ForwardDecalInformation {\n    world_position: vec4<f32>,\n    uv: vec2<f32>,\n    alpha: f32,\n}\n\nfn get_forward_decal_info(in: VertexOutput) -> ForwardDecalInformation {\n    let world_from_local = get_world_from_local(in.instance_index);\n    let scale = (world_from_local * vec4(1.0, 1.0, 1.0, 0.0)).xyz;\n    let scaled_tangent = vec4(in.world_tangent.xyz / scale, in.world_tangent.w);\n\n    let V = normalize(view.world_position - in.world_position.xyz);\n\n    // Transform V from fragment to camera in world space to tangent space.\n    let TBN = calculate_tbn_mikktspace(in.world_normal, scaled_tangent);\n    let T = TBN[0];\n    let B = TBN[1];\n    let N = TBN[2];\n    let Vt = vec3(dot(V, T), dot(V, B), dot(V, N));\n\n    let frag_depth = depth_ndc_to_view_z(in.position.z);\n    let depth_pass_depth = depth_ndc_to_view_z(prepass_depth(in.position, 0u));\n    let diff_depth = frag_depth - depth_pass_depth;\n    let diff_depth_abs = abs(diff_depth);\n\n    // Apply UV parallax\n    let contact_on_decal = project_onto(V * diff_depth, in.world_normal);\n    let normal_depth = length(contact_on_decal);\n    let view_steepness = abs(Vt.z);\n    let delta_uv = normal_depth * Vt.xy * vec2(1.0, -1.0) / view_steepness;\n    let uv = in.uv + delta_uv;\n\n    let world_position = vec4(in.world_position.xyz + V * diff_depth_abs, in.world_position.w);\n    let alpha = saturate(1.0 - (normal_depth * inv_depth_fade_factor));\n\n    return ForwardDecalInformation(world_position, uv, alpha);\n}\n","bevy::pbr::environment_map":"import package::pbr::light_probe::query_light_probe;\nimport package::pbr::mesh_view_bindings as bindings;\nimport package::pbr::mesh_view_bindings::light_probes;\nimport package::pbr::mesh_view_bindings::environment_map_uniform;\nimport package::pbr::lighting::{F_Schlick_vec, LightingInput, LayerLightingInput, LAYER_BASE, LAYER_CLEARCOAT};\nimport package::pbr::clustered_forward::ClusterableObjectIndexRanges;\n\nstruct EnvironmentMapLight {\n    diffuse: vec3<f32>,\n    specular: vec3<f32>,\n};\n\nstruct EnvironmentMapRadiances {\n    irradiance: vec3<f32>,\n    radiance: vec3<f32>,\n}\n\n// Define two versions of this function, one for the case in which there are\n// multiple light probes and one for the case in which only the view light probe\n// is present.\n\n\n@if(MULTIPLE_LIGHT_PROBES_IN_ARRAY)\nfn compute_radiances(\n    input: LayerLightingInput,\n    clusterable_object_index_ranges: ptr<function, ClusterableObjectIndexRanges>,\n    world_position: vec3<f32>,\n    found_diffuse_indirect: bool,\n) -> EnvironmentMapRadiances {\n    // Unpack.\n    let N = input.N;\n    let R = input.R;\n    let NdotV = input.NdotV;\n    let perceptual_roughness = input.perceptual_roughness;\n    let roughness = input.roughness;\n\n    var radiances: EnvironmentMapRadiances;\n\n    // Search for a reflection probe that contains the fragment.\n    var query_result = query_light_probe(\n        world_position,\n        /*is_irradiance_volume=*/ false,\n        clusterable_object_index_ranges,\n    );\n\n    // If we didn't find a reflection probe, use the view environment map if applicable.\n    if (query_result.texture_index < 0) {\n        query_result.texture_index = light_probes.view_cubemap_index;\n        query_result.intensity = light_probes.intensity_for_view;\n        query_result.affects_lightmapped_mesh_diffuse =\n            light_probes.view_environment_map_affects_lightmapped_mesh_diffuse != 0u;\n    }\n\n    // If there's no cubemap, bail out.\n    if (query_result.texture_index < 0) {\n        radiances.irradiance = vec3(0.0);\n        radiances.radiance = vec3(0.0);\n        return radiances;\n    }\n\n    // Split-sum approximation for image based lighting: https://cdn2.unrealengine.com/Resources/files/2013SiggraphPresentationsNotes-26915738.pdf\n    let radiance_level = perceptual_roughness * f32(textureNumLevels(\n        bindings::specular_environment_maps[query_result.texture_index]) - 1u);\n\n    // If we're lightmapped, and we shouldn't accumulate diffuse light from the\n    // environment map, note that.\n    var enable_diffuse = !found_diffuse_indirect;\n    @if(LIGHTMAP) {\n        enable_diffuse = enable_diffuse && query_result.affects_lightmapped_mesh_diffuse;\n    }\n\n    if (enable_diffuse) {\n        var irradiance_sample_dir = N;\n        // Rotating the world space ray direction by the environment light map transform matrix, it is\n        // equivalent to rotating the diffuse environment cubemap itself.\n        irradiance_sample_dir = (environment_map_uniform.transform * vec4(irradiance_sample_dir, 1.0)).xyz;\n        // Cube maps are left-handed so we negate the z coordinate.\n        irradiance_sample_dir.z = -irradiance_sample_dir.z;\n        radiances.irradiance = textureSampleLevel(\n            bindings::diffuse_environment_maps[query_result.texture_index],\n            bindings::environment_map_sampler,\n            irradiance_sample_dir,\n            0.0).rgb * query_result.intensity;\n    }\n\n    var radiance_sample_dir = radiance_sample_direction(N, R, roughness);\n    // Rotating the world space ray direction by the environment light map transform matrix, it is\n    // equivalent to rotating the specular environment cubemap itself.\n    radiance_sample_dir = (environment_map_uniform.transform * vec4(radiance_sample_dir, 1.0)).xyz;\n    // Cube maps are left-handed so we negate the z coordinate.\n    radiance_sample_dir.z = -radiance_sample_dir.z;\n    radiances.radiance = textureSampleLevel(\n        bindings::specular_environment_maps[query_result.texture_index],\n        bindings::environment_map_sampler,\n        radiance_sample_dir,\n        radiance_level).rgb * query_result.intensity;\n\n    return radiances;\n}\n\n\n@else\nfn compute_radiances(\n    input: LayerLightingInput,\n    clusterable_object_index_ranges: ptr<function, ClusterableObjectIndexRanges>,\n    world_position: vec3<f32>,\n    found_diffuse_indirect: bool,\n) -> EnvironmentMapRadiances {\n    // Unpack.\n    let N = input.N;\n    let R = input.R;\n    let NdotV = input.NdotV;\n    let perceptual_roughness = input.perceptual_roughness;\n    let roughness = input.roughness;\n\n    var radiances: EnvironmentMapRadiances;\n\n    if (light_probes.view_cubemap_index < 0) {\n        radiances.irradiance = vec3(0.0);\n        radiances.radiance = vec3(0.0);\n        return radiances;\n    }\n\n    // Split-sum approximation for image based lighting: https://cdn2.unrealengine.com/Resources/files/2013SiggraphPresentationsNotes-26915738.pdf\n    // Technically we could use textureNumLevels(specular_environment_map) - 1 here, but we use a uniform\n    // because textureNumLevels() does not work on WebGL2\n    let radiance_level = perceptual_roughness * f32(light_probes.smallest_specular_mip_level_for_view);\n\n    let intensity = light_probes.intensity_for_view;\n\n    // If we're lightmapped, and we shouldn't accumulate diffuse light from the\n    // environment map, note that.\n    var enable_diffuse = !found_diffuse_indirect;\n    @if(LIGHTMAP) {\n        enable_diffuse = enable_diffuse &&\n            light_probes.view_environment_map_affects_lightmapped_mesh_diffuse;\n    }\n\n    if (enable_diffuse) {\n        var irradiance_sample_dir = N;\n        // Rotating the world space ray direction by the environment light map transform matrix, it is\n        // equivalent to rotating the diffuse environment cubemap itself.\n        irradiance_sample_dir = (environment_map_uniform.transform * vec4(irradiance_sample_dir, 1.0)).xyz;\n        // Cube maps are left-handed so we negate the z coordinate.\n        irradiance_sample_dir.z = -irradiance_sample_dir.z;\n        radiances.irradiance = textureSampleLevel(\n            bindings::diffuse_environment_map,\n            bindings::environment_map_sampler,\n            irradiance_sample_dir,\n            0.0).rgb * intensity;\n    }\n\n    var radiance_sample_dir = radiance_sample_direction(N, R, roughness);\n    // Rotating the world space ray direction by the environment light map transform matrix, it is\n    // equivalent to rotating the specular environment cubemap itself.\n    radiance_sample_dir = (environment_map_uniform.transform * vec4(radiance_sample_dir, 1.0)).xyz;\n    // Cube maps are left-handed so we negate the z coordinate.\n    radiance_sample_dir.z = -radiance_sample_dir.z;\n    radiances.radiance = textureSampleLevel(\n        bindings::specular_environment_map,\n        bindings::environment_map_sampler,\n        radiance_sample_dir,\n        radiance_level).rgb * intensity;\n\n    return radiances;\n}\n\n\n// Adds the environment map light from the clearcoat layer to that of the base\n// layer.\n@if(STANDARD_MATERIAL_CLEARCOAT)\nfn environment_map_light_clearcoat(\n    out: ptr<function, EnvironmentMapLight>,\n    input: ptr<function, LightingInput>,\n    clusterable_object_index_ranges: ptr<function, ClusterableObjectIndexRanges>,\n    found_diffuse_indirect: bool,\n) {\n    // Unpack.\n    let world_position = (*input).P;\n    let clearcoat_NdotV = (*input).layers[LAYER_CLEARCOAT].NdotV;\n    let clearcoat_strength = (*input).clearcoat_strength;\n\n    // Calculate the Fresnel term `Fc` for the clearcoat layer.\n    // 0.04 is a hardcoded value for F0 from the Filament spec.\n    let clearcoat_F0 = vec3<f32>(0.04);\n    let Fc = F_Schlick_vec(clearcoat_F0, 1.0, clearcoat_NdotV) * clearcoat_strength;\n    let inv_Fc = 1.0 - Fc;\n\n    let clearcoat_radiances = compute_radiances(\n        (*input).layers[LAYER_CLEARCOAT],\n        clusterable_object_index_ranges,\n        world_position,\n        found_diffuse_indirect,\n    );\n\n    // Composite the clearcoat layer on top of the existing one.\n    // These formulas are from Filament:\n    // <https://google.github.io/filament/Filament.md.html#lighting/imagebasedlights/clearcoat>\n    (*out).diffuse *= inv_Fc;\n    (*out).specular = (*out).specular * inv_Fc * inv_Fc + clearcoat_radiances.radiance * Fc;\n}\n\nfn environment_map_light(\n    input: ptr<function, LightingInput>,\n    clusterable_object_index_ranges: ptr<function, ClusterableObjectIndexRanges>,\n    found_diffuse_indirect: bool,\n) -> EnvironmentMapLight {\n    // Unpack.\n    let roughness = (*input).layers[LAYER_BASE].roughness;\n    let diffuse_color = (*input).diffuse_color;\n    let NdotV = (*input).layers[LAYER_BASE].NdotV;\n    let F_ab = (*input).F_ab;\n    let F0 = (*input).F0_;\n    let world_position = (*input).P;\n\n    var out: EnvironmentMapLight;\n\n    let radiances = compute_radiances(\n        (*input).layers[LAYER_BASE],\n        clusterable_object_index_ranges,\n        world_position,\n        found_diffuse_indirect,\n    );\n\n    if (all(radiances.irradiance == vec3(0.0)) && all(radiances.radiance == vec3(0.0))) {\n        out.diffuse = vec3(0.0);\n        out.specular = vec3(0.0);\n        return out;\n    }\n\n    // No real world material has specular values under 0.02, so we use this range as a\n    // \"pre-baked specular occlusion\" that extinguishes the fresnel term, for artistic control.\n    // See: https://google.github.io/filament/Filament.html#specularocclusion\n    let specular_occlusion = saturate(dot(F0, vec3(50.0 * 0.33)));\n\n    // Multiscattering approximation: https://www.jcgt.org/published/0008/01/03/paper.pdf\n    // Useful reference: https://bruop.github.io/ibl\n    let Fr = max(vec3(1.0 - roughness), F0) - F0;\n    let kS = F0 + Fr * pow(1.0 - NdotV, 5.0);\n    let Ess = F_ab.x + F_ab.y;\n    let FssEss = kS * Ess * specular_occlusion;\n    let Ems = 1.0 - Ess;\n    let Favg = F0 + (1.0 - F0) / 21.0;\n    let Fms = FssEss * Favg / (1.0 - Ems * Favg);\n    let FmsEms = Fms * Ems;\n    let Edss = 1.0 - (FssEss + FmsEms);\n    let kD = diffuse_color * Edss;\n\n    if (!found_diffuse_indirect) {\n        out.diffuse = (FmsEms + kD) * radiances.irradiance;\n    } else {\n        out.diffuse = vec3(0.0);\n    }\n\n    out.specular = FssEss * radiances.radiance;\n\n    @if(STANDARD_MATERIAL_CLEARCOAT)\n    environment_map_light_clearcoat(\n        &out,\n        input,\n        clusterable_object_index_ranges,\n        found_diffuse_indirect,\n    );\n\n    return out;\n}\n\n// \"Moving Frostbite to Physically Based Rendering 3.0\", listing 22\n// https://seblagarde.wordpress.com/wp-content/uploads/2015/07/course_notes_moving_frostbite_to_pbr_v32.pdf#page=70\nfn radiance_sample_direction(N: vec3<f32>, R: vec3<f32>, roughness: f32) -> vec3<f32> {\n    let smoothness = saturate(1.0 - roughness);\n    let lerp_factor = smoothness * (sqrt(smoothness) + roughness);\n    return mix(N, R, lerp_factor);\n}\n","bevy::pbr::fog":"import package::pbr::{\n    mesh_view_bindings::fog,\n    mesh_view_types::Fog,\n};\n\n// Fog formulas adapted from:\n// https://learn.microsoft.com/en-us/windows/win32/direct3d9/fog-formulas\n// https://catlikecoding.com/unity/tutorials/rendering/part-14/\n// https://iquilezles.org/articles/fog/ (Atmospheric Fog and Scattering)\n\nfn scattering_adjusted_fog_color(\n    fog_params: Fog,\n    scattering: vec3<f32>,\n) -> vec4<f32> {\n    if (fog_params.directional_light_color.a > 0.0) {\n        return vec4<f32>(\n            fog_params.base_color.rgb\n                + scattering * fog_params.directional_light_color.rgb * fog_params.directional_light_color.a,\n            fog_params.base_color.a,\n        );\n    } else {\n        return fog_params.base_color;\n    }\n}\n\nfn linear_fog(\n    fog_params: Fog,\n    input_color: vec4<f32>,\n    distance: f32,\n    scattering: vec3<f32>,\n) -> vec4<f32> {\n    var fog_color = scattering_adjusted_fog_color(fog_params, scattering);\n    let start = fog_params.be.x;\n    let end = fog_params.be.y;\n    fog_color.a *= 1.0 - clamp((end - distance) / (end - start), 0.0, 1.0);\n    return vec4<f32>(mix(input_color.rgb, fog_color.rgb, fog_color.a), input_color.a);\n}\n\nfn exponential_fog(\n    fog_params: Fog,\n    input_color: vec4<f32>,\n    distance: f32,\n    scattering: vec3<f32>,\n) -> vec4<f32> {\n    var fog_color = scattering_adjusted_fog_color(fog_params, scattering);\n    let density = fog_params.be.x;\n    fog_color.a *= 1.0 - 1.0 / exp(distance * density);\n    return vec4<f32>(mix(input_color.rgb, fog_color.rgb, fog_color.a), input_color.a);\n}\n\nfn exponential_squared_fog(\n    fog_params: Fog,\n    input_color: vec4<f32>,\n    distance: f32,\n    scattering: vec3<f32>,\n) -> vec4<f32> {\n    var fog_color = scattering_adjusted_fog_color(fog_params, scattering);\n    let distance_times_density = distance * fog_params.be.x;\n    fog_color.a *= 1.0 - 1.0 / exp(distance_times_density * distance_times_density);\n    return vec4<f32>(mix(input_color.rgb, fog_color.rgb, fog_color.a), input_color.a);\n}\n\nfn atmospheric_fog(\n    fog_params: Fog,\n    input_color: vec4<f32>,\n    distance: f32,\n    scattering: vec3<f32>,\n) -> vec4<f32> {\n    var fog_color = scattering_adjusted_fog_color(fog_params, scattering);\n    let extinction_factor = 1.0 - 1.0 / exp(distance * fog_params.be);\n    let inscattering_factor = 1.0 - 1.0 / exp(distance * fog_params.bi);\n    return vec4<f32>(\n        input_color.rgb * (1.0 - extinction_factor * fog_color.a)\n            + fog_color.rgb * inscattering_factor * fog_color.a,\n        input_color.a\n    );\n}\n","bevy::pbr::forward_io":"struct Vertex {\n    @builtin(instance_index) instance_index: u32,\n    @if(VERTEX_POSITIONS)\n    @location(0) position: vec3<f32>,\n    @if(VERTEX_NORMALS)\n    @location(1) normal: vec3<f32>,\n    @if(VERTEX_UVS_A)\n    @location(2) uv: vec2<f32>,\n    @if(VERTEX_UVS_B)\n    @location(3) uv_b: vec2<f32>,\n    @if(VERTEX_TANGENTS)\n    @location(4) tangent: vec4<f32>,\n    @if(VERTEX_COLORS)\n    @location(5) color: vec4<f32>,\n    @if(SKINNED)\n    @location(6) joint_indices: vec4<u32>,\n    @if(SKINNED)\n    @location(7) joint_weights: vec4<f32>,\n    @if(MORPH_TARGETS)\n    @builtin(vertex_index) index: u32,\n};\n\nstruct VertexOutput {\n    // This is `clip position` when the struct is used as a vertex stage output\n    // and `frag coord` when used as a fragment stage input\n    @builtin(position) position: vec4<f32>,\n    @location(0) world_position: vec4<f32>,\n    @location(1) world_normal: vec3<f32>,\n    @if(VERTEX_UVS_A)\n    @location(2) uv: vec2<f32>,\n    @if(VERTEX_UVS_B)\n    @location(3) uv_b: vec2<f32>,\n    @if(VERTEX_TANGENTS)\n    @location(4) world_tangent: vec4<f32>,\n    @if(VERTEX_COLORS)\n    @location(5) color: vec4<f32>,\n    @if(VERTEX_OUTPUT_INSTANCE_INDEX)\n    @location(6) @interpolate(flat) instance_index: u32,\n    @if(VISIBILITY_RANGE_DITHER)\n    @location(7) @interpolate(flat) visibility_range_dither: i32,\n}\n\nstruct FragmentOutput {\n    @location(0) color: vec4<f32>,\n}\n","bevy::pbr::irradiance_volume":"import package::pbr::light_probe::query_light_probe;\nimport package::pbr::mesh_view_bindings::{\n    irradiance_volumes,\n    irradiance_volume,\n    irradiance_volume_sampler,\n    light_probes,\n};\nimport package::pbr::clustered_forward::ClusterableObjectIndexRanges;\n\n\n// See:\n// https://advances.realtimerendering.com/s2006/Mitchell-ShadingInValvesSourceEngine.pdf\n// Slide 28, \"Ambient Cube Basis\"\n@if(IRRADIANCE_VOLUMES_ARE_USABLE)\nfn irradiance_volume_light(\n    world_position: vec3<f32>,\n    N: vec3<f32>,\n    clusterable_object_index_ranges: ptr<function, ClusterableObjectIndexRanges>,\n) -> vec3<f32> {\n    // Search for an irradiance volume that contains the fragment.\n    let query_result = query_light_probe(\n        world_position,\n        /*is_irradiance_volume=*/ true,\n        clusterable_object_index_ranges,\n    );\n\n    // If there was no irradiance volume found, bail out.\n    if (query_result.texture_index < 0) {\n        return vec3(0.0f);\n    }\n\n    // If we're lightmapped, and the irradiance volume contributes no diffuse\n    // light, then bail out.\n    @if(LIGHTMAP)\n    if (!query_result.affects_lightmapped_mesh_diffuse) {\n        return vec3(0.0f);\n    }\n\n    @if(MULTIPLE_LIGHT_PROBES_IN_ARRAY)\n    let irradiance_volume_texture = irradiance_volumes[query_result.texture_index];\n    @else\n    let irradiance_volume_texture = irradiance_volume;\n\n    let atlas_resolution = vec3<f32>(textureDimensions(irradiance_volume_texture));\n    let resolution = vec3<f32>(textureDimensions(irradiance_volume_texture) / vec3(1u, 2u, 3u));\n\n    // Make sure to clamp to the edges to avoid texture bleed.\n    var unit_pos = (query_result.light_from_world * vec4(world_position, 1.0f)).xyz;\n    let stp = clamp((unit_pos + 0.5) * resolution, vec3(0.5f), resolution - vec3(0.5f));\n    let uvw = stp / atlas_resolution;\n\n    // The bottom half of each cube slice is the negative part, so choose it if applicable on each\n    // slice.\n    let neg_offset = select(vec3(0.0f), vec3(0.5f), N < vec3(0.0f));\n\n    let uvw_x = uvw + vec3(0.0f, neg_offset.x, 0.0f);\n    let uvw_y = uvw + vec3(0.0f, neg_offset.y, 1.0f / 3.0f);\n    let uvw_z = uvw + vec3(0.0f, neg_offset.z, 2.0f / 3.0f);\n\n    let rgb_x = textureSampleLevel(irradiance_volume_texture, irradiance_volume_sampler, uvw_x, 0.0).rgb;\n    let rgb_y = textureSampleLevel(irradiance_volume_texture, irradiance_volume_sampler, uvw_y, 0.0).rgb;\n    let rgb_z = textureSampleLevel(irradiance_volume_texture, irradiance_volume_sampler, uvw_z, 0.0).rgb;\n\n    // Use Valve's formula to sample.\n    let NN = N * N;\n    return (rgb_x * NN.x + rgb_y * NN.y + rgb_z * NN.z) * query_result.intensity;\n};\n","bevy::pbr::light_probe":"import package::pbr::clustered_forward;\nimport package::pbr::clustered_forward::ClusterableObjectIndexRanges;\nimport package::pbr::mesh_view_bindings::light_probes;\nimport package::pbr::mesh_view_types::LightProbe;\n\n// The result of searching for a light probe.\nstruct LightProbeQueryResult {\n    // The index of the light probe texture or textures in the binding array or\n    // arrays.\n    texture_index: i32,\n    // A scale factor that's applied to the diffuse and specular light from the\n    // light probe. This is in units of cd/m² (candela per square meter).\n    intensity: f32,\n    // Transform from world space to the light probe model space. In light probe\n    // model space, the light probe is a 1×1×1 cube centered on the origin.\n    light_from_world: mat4x4<f32>,\n    // Whether this light probe contributes diffuse light to lightmapped meshes.\n    affects_lightmapped_mesh_diffuse: bool,\n};\n\nfn transpose_affine_matrix(matrix: mat3x4<f32>) -> mat4x4<f32> {\n    let matrix4x4 = mat4x4<f32>(\n        matrix[0],\n        matrix[1],\n        matrix[2],\n        vec4<f32>(0.0, 0.0, 0.0, 1.0));\n    return transpose(matrix4x4);\n}\n\n\n// Searches for a light probe that contains the fragment.\n//\n// This is the version that's used when storage buffers are available and\n// light probes are clustered.\n//\n// TODO: Interpolate between multiple light probes.\n@if(AVAILABLE_STORAGE_BUFFER_BINDINGS__GE_3)\nfn query_light_probe(\n    world_position: vec3<f32>,\n    is_irradiance_volume: bool,\n    clusterable_object_index_ranges: ptr<function, ClusterableObjectIndexRanges>,\n) -> LightProbeQueryResult {\n    var result: LightProbeQueryResult;\n    result.texture_index = -1;\n\n    // Reflection probe indices are followed by irradiance volume indices in the\n    // cluster index list. Use this fact to create our bracketing range of\n    // indices.\n    var start_offset: u32;\n    var end_offset: u32;\n    if is_irradiance_volume {\n        start_offset = (*clusterable_object_index_ranges).first_irradiance_volume_index_offset;\n        end_offset = (*clusterable_object_index_ranges).first_decal_offset;\n    } else {\n        start_offset = (*clusterable_object_index_ranges).first_reflection_probe_index_offset;\n        end_offset = (*clusterable_object_index_ranges).first_irradiance_volume_index_offset;\n    }\n\n    for (var light_probe_index_offset: u32 = start_offset;\n            light_probe_index_offset < end_offset && result.texture_index < 0;\n            light_probe_index_offset += 1u) {\n        let light_probe_index = i32(clustered_forward::get_clusterable_object_id(\n            light_probe_index_offset));\n\n        var light_probe: LightProbe;\n        if is_irradiance_volume {\n            light_probe = light_probes.irradiance_volumes[light_probe_index];\n        } else {\n            light_probe = light_probes.reflection_probes[light_probe_index];\n        }\n\n        // Unpack the inverse transform.\n        let light_from_world =\n            transpose_affine_matrix(light_probe.light_from_world_transposed);\n\n        // Check to see if the transformed point is inside the unit cube\n        // centered at the origin.\n        let probe_space_pos = (light_from_world * vec4<f32>(world_position, 1.0f)).xyz;\n        if (all(abs(probe_space_pos) <= vec3(0.5f))) {\n            result.texture_index = light_probe.cubemap_index;\n            result.intensity = light_probe.intensity;\n            result.light_from_world = light_from_world;\n            result.affects_lightmapped_mesh_diffuse =\n                light_probe.affects_lightmapped_mesh_diffuse != 0u;\n            break;\n        }\n    }\n\n    return result;\n}\n\n// Searches for a light probe that contains the fragment.\n//\n// This is the version that's used when storage buffers aren't available and\n// light probes aren't clustered. It simply does a brute force search of all\n// light probes. Because platforms without sufficient SSBO bindings typically\n// lack bindless shaders, there will usually only be one of each type of light\n// probe present anyway.\n@else\nfn query_light_probe(\n    world_position: vec3<f32>,\n    is_irradiance_volume: bool,\n    clusterable_object_index_ranges: ptr<function, ClusterableObjectIndexRanges>,\n) -> LightProbeQueryResult {\n    var result: LightProbeQueryResult;\n    result.texture_index = -1;\n\n    var light_probe_count: i32;\n    if is_irradiance_volume {\n        light_probe_count = light_probes.irradiance_volume_count;\n    } else {\n        light_probe_count = light_probes.reflection_probe_count;\n    }\n\n    for (var light_probe_index: i32 = 0;\n            light_probe_index < light_probe_count && result.texture_index < 0;\n            light_probe_index += 1) {\n        var light_probe: LightProbe;\n        if is_irradiance_volume {\n            light_probe = light_probes.irradiance_volumes[light_probe_index];\n        } else {\n            light_probe = light_probes.reflection_probes[light_probe_index];\n        }\n\n        // Unpack the inverse transform.\n        let light_from_world =\n            transpose_affine_matrix(light_probe.light_from_world_transposed);\n\n        // Check to see if the transformed point is inside the unit cube\n        // centered at the origin.\n        let probe_space_pos = (light_from_world * vec4<f32>(world_position, 1.0f)).xyz;\n        if (all(abs(probe_space_pos) <= vec3(0.5f))) {\n            result.texture_index = light_probe.cubemap_index;\n            result.intensity = light_probe.intensity;\n            result.light_from_world = light_from_world;\n            result.affects_lightmapped_mesh_diffuse =\n                light_probe.affects_lightmapped_mesh_diffuse != 0u;\n\n            // TODO: Workaround for ICE in DXC https://github.com/microsoft/DirectXShaderCompiler/issues/6183\n            // We can't use `break` here because of the ICE.\n            // So instead we rely on the fact that we set `result.texture_index`\n            // above and check its value in the `for` loop header before\n            // looping.\n            // break;\n        }\n    }\n\n    return result;\n}\n","bevy::pbr::lighting":"import package::pbr::{\n    mesh_view_types::POINT_LIGHT_FLAGS_SPOT_LIGHT_Y_NEGATIVE,\n    mesh_view_bindings as view_bindings,\n};\nimport package::render::maths::PI;\n\nconst LAYER_BASE: u32 = 0;\nconst LAYER_CLEARCOAT: u32 = 1;\n\n// From the Filament design doc\n// https://google.github.io/filament/Filament.html#table_symbols\n// Symbol Definition\n// v    View unit vector\n// l    Incident light unit vector\n// n    Surface normal unit vector\n// h    Half unit vector between l and v\n// f    BRDF\n// f_d    Diffuse component of a BRDF\n// f_r    Specular component of a BRDF\n// α    Roughness, remapped from using input perceptualRoughness\n// σ    Diffuse reflectance\n// Ω    Spherical domain\n// f0    Reflectance at normal incidence\n// f90    Reflectance at grazing angle\n// χ+(a)    Heaviside function (1 if a>0 and 0 otherwise)\n// nior    Index of refraction (IOR) of an interface\n// ⟨n⋅l⟩    Dot product clamped to [0..1]\n// ⟨a⟩    Saturated value (clamped to [0..1])\n\n// The Bidirectional Reflectance Distribution Function (BRDF) describes the surface response of a standard material\n// and consists of two components, the diffuse component (f_d) and the specular component (f_r):\n// f(v,l) = f_d(v,l) + f_r(v,l)\n//\n// The form of the microfacet model is the same for diffuse and specular\n// f_r(v,l) = f_d(v,l) = 1 / { |n⋅v||n⋅l| } ∫_Ω D(m,α) G(v,l,m) f_m(v,l,m) (v⋅m) (l⋅m) dm\n//\n// In which:\n// D, also called the Normal Distribution Function (NDF) models the distribution of the microfacets\n// G models the visibility (or occlusion or shadow-masking) of the microfacets\n// f_m is the microfacet BRDF and differs between specular and diffuse components\n//\n// The above integration needs to be approximated.\n\n// Input to a lighting function for a single layer (either the base layer or the\n// clearcoat layer).\nstruct LayerLightingInput {\n    // The normal vector.\n    N: vec3<f32>,\n    // The reflected vector.\n    R: vec3<f32>,\n    // The normal vector ⋅ the view vector.\n    NdotV: f32,\n\n    // The perceptual roughness of the layer.\n    perceptual_roughness: f32,\n    // The roughness of the layer.\n    roughness: f32,\n}\n\n// Input to a lighting function (`point_light`, `spot_light`,\n// `directional_light`).\nstruct LightingInput {\n    @if(STANDARD_MATERIAL_CLEARCOAT)\n    layers: array<LayerLightingInput, 2>,\n    @else\n    layers: array<LayerLightingInput, 1>,\n\n    // The world-space position.\n    P: vec3<f32>,\n    // The vector to the view.\n    V: vec3<f32>,\n\n    // The diffuse color of the material.\n    diffuse_color: vec3<f32>,\n\n    // Specular reflectance at the normal incidence angle.\n    //\n    // This should be read F₀, but due to Naga limitations we can't name it that.\n    F0_: vec3<f32>,\n    // Constants for the BRDF approximation.\n    //\n    // See `EnvBRDFApprox` in\n    // <https://www.unrealengine.com/en-US/blog/physically-based-shading-on-mobile>.\n    // What we call `F_ab` they call `AB`.\n    F_ab: vec2<f32>,\n\n    // The strength of the clearcoat layer.\n    @if(STANDARD_MATERIAL_CLEARCOAT)\n    clearcoat_strength: f32,\n\n    // The anisotropy strength, reflecting the amount of increased roughness in\n    // the tangent direction.\n    @if(STANDARD_MATERIAL_ANISOTROPY)\n    anisotropy: f32,\n    // The tangent direction for anisotropy: i.e. the direction in which\n    // roughness increases.\n    @if(STANDARD_MATERIAL_ANISOTROPY)\n    Ta: vec3<f32>,\n    // The bitangent direction, which is the cross product of the normal with\n    // the tangent direction.\n    @if(STANDARD_MATERIAL_ANISOTROPY)\n    Ba: vec3<f32>,\n}\n\n// Values derived from the `LightingInput` for both diffuse and specular lights.\nstruct DerivedLightingInput {\n    // The half-vector between L, the incident light vector, and V, the view\n    // vector.\n    H: vec3<f32>,\n    // The normal vector ⋅ the incident light vector.\n    NdotL: f32,\n    // The normal vector ⋅ the half-vector.\n    NdotH: f32,\n    // The incident light vector ⋅ the half-vector.\n    LdotH: f32,\n}\n\n// distanceAttenuation is simply the square falloff of light intensity\n// combined with a smooth attenuation at the edge of the light radius\n//\n// light radius is a non-physical construct for efficiency purposes,\n// because otherwise every light affects every fragment in the scene\nfn getDistanceAttenuation(distanceSquare: f32, inverseRangeSquared: f32) -> f32 {\n    let factor = distanceSquare * inverseRangeSquared;\n    let smoothFactor = saturate(1.0 - factor * factor);\n    let attenuation = smoothFactor * smoothFactor;\n    return attenuation * 1.0 / max(distanceSquare, 0.0001);\n}\n\n// Normal distribution function (specular D)\n// Based on https://google.github.io/filament/Filament.html#citation-walter07\n\n// D_GGX(h,α) = α^2 / { π ((n⋅h)^2 (α2−1) + 1)^2 }\n\n// Simple implementation, has precision problems when using fp16 instead of fp32\n// see https://google.github.io/filament/Filament.html#listing_speculardfp16\nfn D_GGX(roughness: f32, NdotH: f32, h: vec3<f32>) -> f32 {\n    let oneMinusNdotHSquared = 1.0 - NdotH * NdotH;\n    let a = NdotH * roughness;\n    let k = roughness / (oneMinusNdotHSquared + a * a);\n    let d = k * k * (1.0 / PI);\n    return d;\n}\n\n// An approximation of the anisotropic GGX distribution function.\n//\n//                                     1\n//     D(𝐡) = ───────────────────────────────────────────────────\n//            παₜα_b((𝐡 ⋅ 𝐭)² / αₜ²) + (𝐡 ⋅ 𝐛)² / α_b² + (𝐡 ⋅ 𝐧)²)²\n//\n// * `T` = 𝐭 = the tangent direction = the direction of increased roughness.\n//\n// * `B` = 𝐛 = the bitangent direction = the direction of decreased roughness.\n//\n// * `at` = αₜ = the alpha-roughness in the tangent direction.\n//\n// * `ab` = α_b = the alpha-roughness in the bitangent direction.\n//\n// This is from the `KHR_materials_anisotropy` spec:\n// <https://github.com/KhronosGroup/glTF/blob/main/extensions/2.0/Khronos/KHR_materials_anisotropy/README.md#individual-lights>\nfn D_GGX_anisotropic(at: f32, ab: f32, NdotH: f32, TdotH: f32, BdotH: f32) -> f32 {\n    let a2 = at * ab;\n    let f = vec3(ab * TdotH, at * BdotH, a2 * NdotH);\n    let w2 = a2 / dot(f, f);\n    let d = a2 * w2 * w2 * (1.0 / PI);\n    return d;\n}\n\n// Visibility function (Specular G)\n// V(v,l,a) = G(v,l,α) / { 4 (n⋅v) (n⋅l) }\n// such that f_r becomes\n// f_r(v,l) = D(h,α) V(v,l,α) F(v,h,f0)\n// where\n// V(v,l,α) = 0.5 / { n⋅l sqrt((n⋅v)^2 (1−α2) + α2) + n⋅v sqrt((n⋅l)^2 (1−α2) + α2) }\n// Note the two sqrt's, that may be slow on mobile, see https://google.github.io/filament/Filament.html#listing_approximatedspecularv\nfn V_SmithGGXCorrelated(roughness: f32, NdotV: f32, NdotL: f32) -> f32 {\n    let a2 = roughness * roughness;\n    let lambdaV = NdotL * sqrt((NdotV - a2 * NdotV) * NdotV + a2);\n    let lambdaL = NdotV * sqrt((NdotL - a2 * NdotL) * NdotL + a2);\n    let v = 0.5 / (lambdaV + lambdaL);\n    return v;\n}\n\n// The visibility function, anisotropic variant.\nfn V_GGX_anisotropic(\n    at: f32,\n    ab: f32,\n    NdotL: f32,\n    NdotV: f32,\n    BdotV: f32,\n    TdotV: f32,\n    TdotL: f32,\n    BdotL: f32,\n) -> f32 {\n    let GGX_V = NdotL * length(vec3(at * TdotV, ab * BdotV, NdotV));\n    let GGX_L = NdotV * length(vec3(at * TdotL, ab * BdotL, NdotL));\n    let v = 0.5 / (GGX_V + GGX_L);\n    return saturate(v);\n}\n\n// A simpler, but nonphysical, alternative to Smith-GGX. We use this for\n// clearcoat, per the Filament spec.\n//\n// https://google.github.io/filament/Filament.html#materialsystem/clearcoatmodel#toc4.9.1\nfn V_Kelemen(LdotH: f32) -> f32 {\n    return 0.25 / (LdotH * LdotH);\n}\n\n// Fresnel function\n// see https://google.github.io/filament/Filament.html#citation-schlick94\n// F_Schlick(v,h,f_0,f_90) = f_0 + (f_90 − f_0) (1 − v⋅h)^5\nfn F_Schlick_vec(f0: vec3<f32>, f90: f32, VdotH: f32) -> vec3<f32> {\n    // not using mix to keep the vec3 and float versions identical\n    return f0 + (f90 - f0) * pow(1.0 - VdotH, 5.0);\n}\n\nfn F_Schlick(f0: f32, f90: f32, VdotH: f32) -> f32 {\n    // not using mix to keep the vec3 and float versions identical\n    return f0 + (f90 - f0) * pow(1.0 - VdotH, 5.0);\n}\n\nfn fresnel(f0: vec3<f32>, LdotH: f32) -> vec3<f32> {\n    // f_90 suitable for ambient occlusion\n    // see https://google.github.io/filament/Filament.html#lighting/occlusion\n    let f90 = saturate(dot(f0, vec3<f32>(50.0 * 0.33)));\n    return F_Schlick_vec(f0, f90, LdotH);\n}\n\n// Given distribution, visibility, and Fresnel term, calculates the final\n// specular light.\n//\n// Multiscattering approximation:\n// <https://google.github.io/filament/Filament.html#listing_energycompensationimpl>\nfn specular_multiscatter(\n    input: ptr<function, LightingInput>,\n    D: f32,\n    V: f32,\n    F: vec3<f32>,\n    specular_intensity: f32,\n) -> vec3<f32> {\n    // Unpack.\n    let F0 = (*input).F0_;\n    let F_ab = (*input).F_ab;\n\n    var Fr = (specular_intensity * D * V) * F;\n    Fr *= 1.0 + F0 * (1.0 / F_ab.x - 1.0);\n    return Fr;\n}\n\n// Specular BRDF\n// https://google.github.io/filament/Filament.html#materialsystem/specularbrdf\n\n// N, V, and L must all be normalized.\nfn derive_lighting_input(N: vec3<f32>, V: vec3<f32>, L: vec3<f32>) -> DerivedLightingInput {\n    var input: DerivedLightingInput;\n    var H: vec3<f32> = normalize(L + V);\n    input.H = H;\n    input.NdotL = saturate(dot(N, L));\n    input.NdotH = saturate(dot(N, H));\n    input.LdotH = saturate(dot(L, H));\n    return input;\n}\n\n// Returns L in the `xyz` components and the specular intensity in the `w` component.\nfn compute_specular_layer_values_for_point_light(\n    input: ptr<function, LightingInput>,\n    layer: u32,\n    V: vec3<f32>,\n    light_to_frag: vec3<f32>,\n    light_position_radius: f32,\n) -> vec4<f32> {\n    // Unpack.\n    let R = (*input).layers[layer].R;\n    let a = (*input).layers[layer].roughness;\n\n    // Representative Point Area Lights.\n    // see http://blog.selfshadow.com/publications/s2013-shading-course/karis/s2013_pbs_epic_notes_v2.pdf p14-16\n    let centerToRay = dot(light_to_frag, R) * R - light_to_frag;\n    let closestPoint = light_to_frag + centerToRay * saturate(\n        light_position_radius * inverseSqrt(dot(centerToRay, centerToRay)));\n    let LspecLengthInverse = inverseSqrt(dot(closestPoint, closestPoint));\n    let normalizationFactor = a / saturate(a + (light_position_radius * 0.5 * LspecLengthInverse));\n    let intensity = normalizationFactor * normalizationFactor;\n\n    let L: vec3<f32> = closestPoint * LspecLengthInverse; // normalize() equivalent?\n    return vec4(L, intensity);\n}\n\n// Cook-Torrance approximation of the microfacet model integration using Fresnel law F to model f_m\n// f_r(v,l) = { D(h,α) G(v,l,α) F(v,h,f0) } / { 4 (n⋅v) (n⋅l) }\nfn specular(\n    input: ptr<function, LightingInput>,\n    derived_input: ptr<function, DerivedLightingInput>,\n    specular_intensity: f32,\n) -> vec3<f32> {\n    // Unpack.\n    let roughness = (*input).layers[LAYER_BASE].roughness;\n    let NdotV = (*input).layers[LAYER_BASE].NdotV;\n    let F0 = (*input).F0_;\n    let H = (*derived_input).H;\n    let NdotL = (*derived_input).NdotL;\n    let NdotH = (*derived_input).NdotH;\n    let LdotH = (*derived_input).LdotH;\n\n    // Calculate distribution.\n    let D = D_GGX(roughness, NdotH, H);\n    // Calculate visibility.\n    let V = V_SmithGGXCorrelated(roughness, NdotV, NdotL);\n    // Calculate the Fresnel term.\n    let F = fresnel(F0, LdotH);\n\n    // Calculate the specular light.\n    let Fr = specular_multiscatter(input, D, V, F, specular_intensity);\n    return Fr;\n}\n\n// Calculates the specular light for the clearcoat layer. Returns Fc, the\n// Fresnel term, in the first channel, and Frc, the specular clearcoat light, in\n// the second channel.\n//\n// <https://google.github.io/filament/Filament.html#listing_clearcoatbrdf>\n// TODO(mbr): wesl.lower() revealed that this function contains UB when STANDARD_MATERIAL_CLEARCOAT\n// is unset, because input.layers[1] is out of bounds. Naga will not pass validation if we replace\n// LAYER_CLEARCOAT with its value (1u). So I added the @if here.\n@if(STANDARD_MATERIAL_CLEARCOAT)\nfn specular_clearcoat(\n    input: ptr<function, LightingInput>,\n    derived_input: ptr<function, DerivedLightingInput>,\n    clearcoat_strength: f32,\n    specular_intensity: f32,\n) -> vec2<f32> {\n    // Unpack.\n    let roughness = (*input).layers[LAYER_CLEARCOAT].roughness;\n    let H = (*derived_input).H;\n    let NdotH = (*derived_input).NdotH;\n    let LdotH = (*derived_input).LdotH;\n\n    // Calculate distribution.\n    let Dc = D_GGX(roughness, NdotH, H);\n    // Calculate visibility.\n    let Vc = V_Kelemen(LdotH);\n    // Calculate the Fresnel term.\n    let Fc = F_Schlick(0.04, 1.0, LdotH) * clearcoat_strength;\n    // Calculate the specular light.\n    let Frc = (specular_intensity * Dc * Vc) * Fc;\n    return vec2(Fc, Frc);\n}\n\n@if(STANDARD_MATERIAL_ANISOTROPY)\nfn specular_anisotropy(\n    input: ptr<function, LightingInput>,\n    derived_input: ptr<function, DerivedLightingInput>,\n    L: vec3<f32>,\n    specular_intensity: f32,\n) -> vec3<f32> {\n    // Unpack.\n    let roughness = (*input).layers[LAYER_BASE].roughness;\n    let NdotV = (*input).layers[LAYER_BASE].NdotV;\n    let V = (*input).V;\n    let F0 = (*input).F0_;\n    let anisotropy = (*input).anisotropy;\n    let Ta = (*input).Ta;\n    let Ba = (*input).Ba;\n    let H = (*derived_input).H;\n    let NdotL = (*derived_input).NdotL;\n    let NdotH = (*derived_input).NdotH;\n    let LdotH = (*derived_input).LdotH;\n\n    let TdotL = dot(Ta, L);\n    let BdotL = dot(Ba, L);\n    let TdotH = dot(Ta, H);\n    let BdotH = dot(Ba, H);\n    let TdotV = dot(Ta, V);\n    let BdotV = dot(Ba, V);\n\n    let ab = roughness * roughness;\n    let at = mix(ab, 1.0, anisotropy * anisotropy);\n\n    let Da = D_GGX_anisotropic(at, ab, NdotH, TdotH, BdotH);\n    let Va = V_GGX_anisotropic(at, ab, NdotL, NdotV, BdotV, TdotV, TdotL, BdotL);\n    let Fa = fresnel(F0, LdotH);\n\n    // Calculate the specular light.\n    let Fr = specular_multiscatter(input, Da, Va, Fa, specular_intensity);\n    return Fr;\n}\n\n// Diffuse BRDF\n// https://google.github.io/filament/Filament.html#materialsystem/diffusebrdf\n// fd(v,l) = σ/π * 1 / { |n⋅v||n⋅l| } ∫Ω D(m,α) G(v,l,m) (v⋅m) (l⋅m) dm\n//\n// simplest approximation\n// float Fd_Lambert() {\n//     return 1.0 / PI;\n// }\n//\n// vec3 Fd = diffuseColor * Fd_Lambert();\n//\n// Disney approximation\n// See https://google.github.io/filament/Filament.html#citation-burley12\n// minimal quality difference\nfn Fd_Burley(\n    input: ptr<function, LightingInput>,\n    derived_input: ptr<function, DerivedLightingInput>,\n) -> f32 {\n    // Unpack.\n    let roughness = (*input).layers[LAYER_BASE].roughness;\n    let NdotV = (*input).layers[LAYER_BASE].NdotV;\n    let NdotL = (*derived_input).NdotL;\n    let LdotH = (*derived_input).LdotH;\n\n    let f90 = 0.5 + 2.0 * roughness * LdotH * LdotH;\n    let lightScatter = F_Schlick(1.0, f90, NdotL);\n    let viewScatter = F_Schlick(1.0, f90, NdotV);\n    return lightScatter * viewScatter * (1.0 / PI);\n}\n\n// Scale/bias approximation\n// https://www.unrealengine.com/en-US/blog/physically-based-shading-on-mobile\n// TODO: Use a LUT (more accurate)\nfn F_AB(perceptual_roughness: f32, NdotV: f32) -> vec2<f32> {\n    let c0 = vec4<f32>(-1.0, -0.0275, -0.572, 0.022);\n    let c1 = vec4<f32>(1.0, 0.0425, 1.04, -0.04);\n    let r = perceptual_roughness * c0 + c1;\n    let a004 = min(r.x * r.x, exp2(-9.28 * NdotV)) * r.x + r.y;\n    return vec2<f32>(-1.04, 1.04) * a004 + r.zw;\n}\n\nfn EnvBRDFApprox(F0: vec3<f32>, F_ab: vec2<f32>) -> vec3<f32> {\n    return F0 * F_ab.x + F_ab.y;\n}\n\nfn perceptualRoughnessToRoughness(perceptualRoughness: f32) -> f32 {\n    // clamp perceptual roughness to prevent precision problems\n    // According to Filament design 0.089 is recommended for mobile\n    // Filament uses 0.045 for non-mobile\n    let clampedPerceptualRoughness = clamp(perceptualRoughness, 0.089, 1.0);\n    return clampedPerceptualRoughness * clampedPerceptualRoughness;\n}\n\nfn point_light(\n    light_id: u32,\n    input: ptr<function, LightingInput>,\n    enable_diffuse: bool\n) -> vec3<f32> {\n    // Unpack.\n    let diffuse_color = (*input).diffuse_color;\n    let P = (*input).P;\n    let N = (*input).layers[LAYER_BASE].N;\n    let V = (*input).V;\n\n    let light = &view_bindings::clusterable_objects.data[light_id];\n    let light_to_frag = (*light).position_radius.xyz - P;\n    let L = normalize(light_to_frag);\n    let distance_square = dot(light_to_frag, light_to_frag);\n    let rangeAttenuation = getDistanceAttenuation(distance_square, (*light).color_inverse_square_range.w);\n\n    // Base layer\n\n    let specular_L_intensity = compute_specular_layer_values_for_point_light(\n        input,\n        LAYER_BASE,\n        V,\n        light_to_frag,\n        (*light).position_radius.w,\n    );\n    var specular_derived_input = derive_lighting_input(N, V, specular_L_intensity.xyz);\n\n    let specular_intensity = specular_L_intensity.w;\n\n    @if(STANDARD_MATERIAL_ANISOTROPY)\n    let specular_light = specular_anisotropy(input, &specular_derived_input, L, specular_intensity);\n    @else\n    let specular_light = specular(input, &specular_derived_input, specular_intensity);\n\n    // Clearcoat\n\n    // TODO(mbr): must not be grouped\n    @if(STANDARD_MATERIAL_CLEARCOAT) {\n        // Unpack.\n        let clearcoat_N = (*input).layers[LAYER_CLEARCOAT].N;\n        let clearcoat_strength = (*input).clearcoat_strength;\n\n        // Perform specular input calculations again for the clearcoat layer. We\n        // can't reuse the above because the clearcoat normal might be different\n        // from the main layer normal.\n        let clearcoat_specular_L_intensity = compute_specular_layer_values_for_point_light(\n            input,\n            LAYER_CLEARCOAT,\n            V,\n            light_to_frag,\n            (*light).position_radius.w,\n        );\n        var clearcoat_specular_derived_input =\n            derive_lighting_input(clearcoat_N, V, clearcoat_specular_L_intensity.xyz);\n\n        // Calculate the specular light.\n        let clearcoat_specular_intensity = clearcoat_specular_L_intensity.w;\n        let Fc_Frc = specular_clearcoat(\n            input,\n            &clearcoat_specular_derived_input,\n            clearcoat_strength,\n            clearcoat_specular_intensity\n        );\n        let inv_Fc = 1.0 - Fc_Frc.r;    // Inverse Fresnel term.\n        let Frc = Fc_Frc.g;             // Clearcoat light.\n    }\n\n    // Diffuse.\n    // Comes after specular since its N⋅L is used in the lighting equation.\n    var derived_input = derive_lighting_input(N, V, L);\n    var diffuse = vec3(0.0);\n    if (enable_diffuse) {\n        diffuse = diffuse_color * Fd_Burley(input, &derived_input);\n    }\n\n    // See https://google.github.io/filament/Filament.html#mjx-eqn-pointLightLuminanceEquation\n    // Lout = f(v,l) Φ / { 4 π d^2 }⟨n⋅l⟩\n    // where\n    // f(v,l) = (f_d(v,l) + f_r(v,l)) * light_color\n    // Φ is luminous power in lumens\n    // our rangeAttenuation = 1 / d^2 multiplied with an attenuation factor for smoothing at the edge of the non-physical maximum light radius\n\n    // For a point light, luminous intensity, I, in lumens per steradian is given by:\n    // I = Φ / 4 π\n    // The derivation of this can be seen here: https://google.github.io/filament/Filament.html#mjx-eqn-pointLightLuminousPower\n\n    // NOTE: (*light).color.rgb is premultiplied with (*light).intensity / 4 π (which would be the luminous intensity) on the CPU\n\n    var color: vec3<f32>;\n    @if(STANDARD_MATERIAL_CLEARCOAT) {\n        // Account for the Fresnel term from the clearcoat darkening the main layer.\n        //\n        // <https://google.github.io/filament/Filament.html#materialsystem/clearcoatmodel/integrationinthesurfaceresponse>\n        color = (diffuse + specular_light * inv_Fc) * inv_Fc + Frc;\n    }\n    @else {\n        color = diffuse + specular_light;\n    }\n\n    return color * (*light).color_inverse_square_range.rgb *\n        (rangeAttenuation * derived_input.NdotL);\n}\n\nfn spot_light(\n    light_id: u32,\n    input: ptr<function, LightingInput>,\n    enable_diffuse: bool\n) -> vec3<f32> {\n    // reuse the point light calculations\n    let point_light = point_light(light_id, input, enable_diffuse);\n\n    let light = &view_bindings::clusterable_objects.data[light_id];\n\n    // reconstruct spot dir from x/z and y-direction flag\n    var spot_dir = vec3<f32>((*light).light_custom_data.x, 0.0, (*light).light_custom_data.y);\n    spot_dir.y = sqrt(max(0.0, 1.0 - spot_dir.x * spot_dir.x - spot_dir.z * spot_dir.z));\n    if ((*light).flags & POINT_LIGHT_FLAGS_SPOT_LIGHT_Y_NEGATIVE) != 0u {\n        spot_dir.y = -spot_dir.y;\n    }\n    let light_to_frag = (*light).position_radius.xyz - (*input).P.xyz;\n\n    // calculate attenuation based on filament formula https://google.github.io/filament/Filament.html#listing_glslpunctuallight\n    // spot_scale and spot_offset have been precomputed\n    // note we normalize here to get \"l\" from the filament listing. spot_dir is already normalized\n    let cd = dot(-spot_dir, normalize(light_to_frag));\n    let attenuation = saturate(cd * (*light).light_custom_data.z + (*light).light_custom_data.w);\n    let spot_attenuation = attenuation * attenuation;\n\n    return point_light * spot_attenuation;\n}\n\nfn directional_light(\n    light_id: u32,\n    input: ptr<function, LightingInput>,\n    enable_diffuse: bool\n) -> vec3<f32> {\n    // Unpack.\n    let diffuse_color = (*input).diffuse_color;\n    let NdotV = (*input).layers[LAYER_BASE].NdotV;\n    let N = (*input).layers[LAYER_BASE].N;\n    let V = (*input).V;\n    let roughness = (*input).layers[LAYER_BASE].roughness;\n\n    let light = &view_bindings::lights.directional_lights[light_id];\n\n    let L = (*light).direction_to_light.xyz;\n    var derived_input = derive_lighting_input(N, V, L);\n\n    var diffuse = vec3(0.0);\n    if (enable_diffuse) {\n        diffuse = diffuse_color * Fd_Burley(input, &derived_input);\n    }\n\n    @if(STANDARD_MATERIAL_ANISOTROPY)\n    let specular_light = specular_anisotropy(input, &derived_input, L, 1.0);\n    @else\n    let specular_light = specular(input, &derived_input, 1.0);\n\n    // TODO(mbr): must not be grouped\n    @if(STANDARD_MATERIAL_CLEARCOAT) {\n        let clearcoat_N = (*input).layers[LAYER_CLEARCOAT].N;\n        let clearcoat_strength = (*input).clearcoat_strength;\n\n        // Perform specular input calculations again for the clearcoat layer. We\n        // can't reuse the above because the clearcoat normal might be different\n        // from the main layer normal.\n        var derived_clearcoat_input = derive_lighting_input(clearcoat_N, V, L);\n\n        let Fc_Frc =\n            specular_clearcoat(input, &derived_clearcoat_input, clearcoat_strength, 1.0);\n        let inv_Fc = 1.0 - Fc_Frc.r;\n        let Frc = Fc_Frc.g;\n    }\n\n    var color: vec3<f32>;\n    @if(STANDARD_MATERIAL_CLEARCOAT) {\n        // Account for the Fresnel term from the clearcoat darkening the main layer.\n        //\n        // <https://google.github.io/filament/Filament.html#materialsystem/clearcoatmodel/integrationinthesurfaceresponse>\n        color = (diffuse + specular_light * inv_Fc) * inv_Fc * derived_input.NdotL +\n            Frc * derived_clearcoat_input.NdotL;\n    }\n    @else {\n        color = (diffuse + specular_light) * derived_input.NdotL;\n    }\n\n    return color * (*light).color.rgb;\n}\n","bevy::pbr::lightmap":"import package::pbr::mesh_bindings::mesh;\n\n@if(MULTIPLE_LIGHTMAPS_IN_ARRAY)\n@group(1) @binding(4) var lightmaps_textures: binding_array<texture_2d<f32>, 4>;\n@else\n@group(1) @binding(4) var lightmaps_texture: texture_2d<f32>;\n\n@if(MULTIPLE_LIGHTMAPS_IN_ARRAY)\n@group(1) @binding(5) var lightmaps_samplers: binding_array<sampler, 4>;\n@else\n@group(1) @binding(5) var lightmaps_sampler: sampler;\n\n// Samples the lightmap, if any, and returns indirect illumination from it.\nfn lightmap(uv: vec2<f32>, exposure: f32, instance_index: u32) -> vec3<f32> {\n    let packed_uv_rect = mesh[instance_index].lightmap_uv_rect;\n    let uv_rect = vec4<f32>(\n        unpack2x16unorm(packed_uv_rect.x),\n        unpack2x16unorm(packed_uv_rect.y),\n    );\n    let lightmap_uv = mix(uv_rect.xy, uv_rect.zw, uv);\n    let lightmap_slot = mesh[instance_index].material_and_lightmap_bind_group_slot >> 16u;\n\n    // Bicubic 4-tap\n    // https://developer.nvidia.com/gpugems/gpugems2/part-iii-high-quality-rendering/chapter-20-fast-third-order-texture-filtering\n    // https://advances.realtimerendering.com/s2021/jpatry_advances2021/index.html#/111/0/2\n    @if(LIGHTMAP_BICUBIC_SAMPLING) {\n        let texture_size = vec2<f32>(lightmap_size(lightmap_slot));\n        let texel_size = 1.0 / texture_size;\n        let puv = lightmap_uv * texture_size + 0.5;\n        let iuv = floor(puv);\n        let fuv = fract(puv);\n        let g0x = g0(fuv.x);\n        let g1x = g1(fuv.x);\n        let h0x = h0_approx(fuv.x);\n        let h1x = h1_approx(fuv.x);\n        let h0y = h0_approx(fuv.y);\n        let h1y = h1_approx(fuv.y);\n        let p0 = (vec2(iuv.x + h0x, iuv.y + h0y) - 0.5) * texel_size;\n        let p1 = (vec2(iuv.x + h1x, iuv.y + h0y) - 0.5) * texel_size;\n        let p2 = (vec2(iuv.x + h0x, iuv.y + h1y) - 0.5) * texel_size;\n        let p3 = (vec2(iuv.x + h1x, iuv.y + h1y) - 0.5) * texel_size;\n        let color = g0(fuv.y) * (g0x * sample(p0, lightmap_slot) + g1x * sample(p1, lightmap_slot)) + g1(fuv.y) * (g0x * sample(p2, lightmap_slot) + g1x * sample(p3, lightmap_slot));\n        return color * exposure;\n    }\n    @else {\n        let color = sample(lightmap_uv, lightmap_slot);\n        return color * exposure;\n    }\n}\n\nfn lightmap_size(lightmap_slot: u32) -> vec2<u32> {\n    @if(MULTIPLE_LIGHTMAPS_IN_ARRAY)\n    return textureDimensions(lightmaps_textures[lightmap_slot]);\n    @else\n    return textureDimensions(lightmaps_texture);\n}\n\nfn sample(uv: vec2<f32>, lightmap_slot: u32) -> vec3<f32> {\n    // Mipmapping lightmaps is usually a bad idea due to leaking across UV\n    // islands, so there's no harm in using mip level 0 and it lets us avoid\n    // control flow uniformity problems.\n    @if(MULTIPLE_LIGHTMAPS_IN_ARRAY)\n    return textureSampleLevel(lightmaps_textures[lightmap_slot], lightmaps_samplers[lightmap_slot], uv, 0.0).rgb;\n    @else\n    return textureSampleLevel(lightmaps_texture, lightmaps_sampler, uv, 0.0).rgb;\n}\n\nfn w0(a: f32) -> f32 {\n    return (1.0 / 6.0) * (a * (a * (-a + 3.0) - 3.0) + 1.0);\n}\n\nfn w1(a: f32) -> f32 {\n    return (1.0 / 6.0) * (a * a * (3.0 * a - 6.0) + 4.0);\n}\n\nfn w2(a: f32) -> f32 {\n    return (1.0 / 6.0) * (a * (a * (-3.0 * a + 3.0) + 3.0) + 1.0);\n}\n\nfn w3(a: f32) -> f32 {\n    return (1.0 / 6.0) * (a * a * a);\n}\n\nfn g0(a: f32) -> f32 {\n    return w0(a) + w1(a);\n}\n\nfn g1(a: f32) -> f32 {\n    return w2(a) + w3(a);\n}\n\nfn h0_approx(a: f32) -> f32 {\n    return -0.2 - a * (0.24 * a - 0.44);\n}\n\nfn h1_approx(a: f32) -> f32 {\n    return 1.0 + a * (0.24 * a - 0.04);\n}\n","bevy::pbr::mesh_bindings":"import package::pbr::mesh_types::Mesh;\n\n// TODO(mbr): not sure about the else, did it match both if conditions?\n@if(!MESHLET_MESH_MATERIAL_PASS && PER_OBJECT_BUFFER_BATCH_SIZE)\n@group(1) @binding(0) var<uniform> mesh: array<Mesh, u32(constants::PER_OBJECT_BUFFER_BATCH_SIZE)>;\n@else\n@group(1) @binding(0) var<storage> mesh: array<Mesh>;\n","bevy::pbr::mesh_functions":"import package::pbr::{\n    mesh_view_bindings::{\n        view,\n        visibility_ranges,\n        VISIBILITY_RANGE_UNIFORM_BUFFER_SIZE\n    },\n    mesh_bindings::mesh,\n    mesh_types::MESH_FLAGS_SIGN_DETERMINANT_MODEL_3X3_BIT,\n    view_transformations::position_world_to_clip,\n};\nimport package::render::maths::{affine3_to_square, mat2x4_f32_to_mat3x3_unpack};\n\n\n@if(!MESHLET_MESH_MATERIAL_PASS)\nfn get_world_from_local(instance_index: u32) -> mat4x4<f32> {\n    return affine3_to_square(mesh[instance_index].world_from_local);\n}\n\n@if(!MESHLET_MESH_MATERIAL_PASS)\nfn get_previous_world_from_local(instance_index: u32) -> mat4x4<f32> {\n    return affine3_to_square(mesh[instance_index].previous_world_from_local);\n}\n\n@if(!MESHLET_MESH_MATERIAL_PASS)\nfn get_local_from_world(instance_index: u32) -> mat4x4<f32> {\n    // the model matrix is translation * rotation * scale\n    // the inverse is then scale^-1 * rotation ^-1 * translation^-1        \n    // the 3x3 matrix only contains the information for the rotation and scale\n    let inverse_model_3x3 = transpose(mat2x4_f32_to_mat3x3_unpack(\n        mesh[instance_index].local_from_world_transpose_a,\n        mesh[instance_index].local_from_world_transpose_b,\n    ));\n    // construct scale^-1 * rotation^-1 from the 3x3\n    let inverse_model_4x4_no_trans = mat4x4<f32>(\n        vec4(inverse_model_3x3[0], 0.0),\n        vec4(inverse_model_3x3[1], 0.0),\n        vec4(inverse_model_3x3[2], 0.0),\n        vec4(0.0,0.0,0.0,1.0)\n    );\n    // we can get translation^-1 by negating the translation of the model\n    let model = get_world_from_local(instance_index);\n    let inverse_model_4x4_only_trans = mat4x4<f32>(\n        vec4(1.0,0.0,0.0,0.0),\n        vec4(0.0,1.0,0.0,0.0),\n        vec4(0.0,0.0,1.0,0.0),\n        vec4(-model[3].xyz, 1.0)\n    );\n\n    return inverse_model_4x4_no_trans * inverse_model_4x4_only_trans;\n}\n\nfn mesh_position_local_to_world(world_from_local: mat4x4<f32>, vertex_position: vec4<f32>) -> vec4<f32> {\n    return world_from_local * vertex_position;\n}\n\n// NOTE: The intermediate world_position assignment is important\n// for precision purposes when using the 'equals' depth comparison\n// function.\nfn mesh_position_local_to_clip(world_from_local: mat4x4<f32>, vertex_position: vec4<f32>) -> vec4<f32> {\n    let world_position = mesh_position_local_to_world(world_from_local, vertex_position);\n    return position_world_to_clip(world_position.xyz);\n}\n\n@if(!MESHLET_MESH_MATERIAL_PASS)\nfn mesh_normal_local_to_world(vertex_normal: vec3<f32>, instance_index: u32) -> vec3<f32> {\n    // NOTE: The mikktspace method of normal mapping requires that the world normal is\n    // re-normalized in the vertex shader to match the way mikktspace bakes vertex tangents\n    // and normal maps so that the exact inverse process is applied when shading. Blender, Unity,\n    // Unreal Engine, Godot, and more all use the mikktspace method.\n    // We only skip normalization for invalid normals so that they don't become NaN.\n    // Do not change this code unless you really know what you are doing.\n    // http://www.mikktspace.com/\n    if any(vertex_normal != vec3<f32>(0.0)) {\n        return normalize(\n            mat2x4_f32_to_mat3x3_unpack(\n                mesh[instance_index].local_from_world_transpose_a,\n                mesh[instance_index].local_from_world_transpose_b,\n            ) * vertex_normal\n        );\n    } else {\n        return vertex_normal;\n    }\n}\n\n// Calculates the sign of the determinant of the 3x3 model matrix based on a\n// mesh flag\nfn sign_determinant_model_3x3m(mesh_flags: u32) -> f32 {\n    // bool(u32) is false if 0u else true\n    // f32(bool) is 1.0 if true else 0.0\n    // * 2.0 - 1.0 remaps 0.0 or 1.0 to -1.0 or 1.0 respectively\n    return f32(bool(mesh_flags & MESH_FLAGS_SIGN_DETERMINANT_MODEL_3X3_BIT)) * 2.0 - 1.0;\n}\n\n@if(!MESHLET_MESH_MATERIAL_PASS)\nfn mesh_tangent_local_to_world(world_from_local: mat4x4<f32>, vertex_tangent: vec4<f32>, instance_index: u32) -> vec4<f32> {\n    // NOTE: The mikktspace method of normal mapping requires that the world tangent is\n    // re-normalized in the vertex shader to match the way mikktspace bakes vertex tangents\n    // and normal maps so that the exact inverse process is applied when shading. Blender, Unity,\n    // Unreal Engine, Godot, and more all use the mikktspace method.\n    // We only skip normalization for invalid tangents so that they don't become NaN.\n    // Do not change this code unless you really know what you are doing.\n    // http://www.mikktspace.com/\n    if any(vertex_tangent != vec4<f32>(0.0)) {\n        return vec4<f32>(\n            normalize(\n                mat3x3<f32>(\n                    world_from_local[0].xyz,\n                    world_from_local[1].xyz,\n                    world_from_local[2].xyz,\n                ) * vertex_tangent.xyz\n            ),\n            // NOTE: Multiplying by the sign of the determinant of the 3x3 model matrix accounts for\n            // situations such as negative scaling.\n            vertex_tangent.w * sign_determinant_model_3x3m(mesh[instance_index].flags)\n        );\n    } else {\n        return vertex_tangent;\n    }\n}\n\n// Returns an appropriate dither level for the current mesh instance.\n//\n// This looks up the LOD range in the `visibility_ranges` table and compares the\n// camera distance to determine the dithering level.\n@if(VISIBILITY_RANGE_DITHER)\nfn get_visibility_range_dither_level(instance_index: u32, world_position: vec4<f32>) -> i32 {\n    // If we're using a storage buffer, then the length is variable.\n    @if(AVAILABLE_STORAGE_BUFFER_BINDINGS__GE_6)\n    let visibility_buffer_array_len = arrayLength(&visibility_ranges);\n    // If we're using a uniform buffer, then the length is constant\n    @else\n    let visibility_buffer_array_len = VISIBILITY_RANGE_UNIFORM_BUFFER_SIZE;\n\n    let visibility_buffer_index = mesh[instance_index].flags & 0xffffu;\n    if (visibility_buffer_index > visibility_buffer_array_len) {\n        return -16;\n    }\n\n    let lod_range = visibility_ranges[visibility_buffer_index];\n    let camera_distance = length(view.world_position.xyz - world_position.xyz);\n\n    // This encodes the following mapping:\n    //\n    //     `lod_range.`          x        y        z        w           camera distance\n    //                   ←───────┼────────┼────────┼────────┼────────→\n    //        LOD level  -16    -16       0        0        16      16  LOD level\n    let offset = select(-16, 0, camera_distance >= lod_range.z);\n    let bounds = select(lod_range.xy, lod_range.zw, camera_distance >= lod_range.z);\n    let level = i32(round((camera_distance - bounds.x) / (bounds.y - bounds.x) * 16.0));\n    return offset + clamp(level, 0, 16);\n}\n\n@if(!MESHLET_MESH_MATERIAL_PASS)\nfn get_tag(instance_index: u32) -> u32 {\n    return mesh[instance_index].tag;\n}\n","bevy::pbr::mesh_preprocess_types":"// Types needed for GPU mesh uniform building.\n\n\n// Per-frame data that the CPU supplies to the GPU.\nstruct MeshInput {\n    // The model transform.\n    world_from_local: mat3x4<f32>,\n    // The lightmap UV rect, packed into 64 bits.\n    lightmap_uv_rect: vec2<u32>,\n    // Various flags.\n    flags: u32,\n    previous_input_index: u32,\n    first_vertex_index: u32,\n    first_index_index: u32,\n    index_count: u32,\n    current_skin_index: u32,\n    // Low 16 bits: index of the material inside the bind group data.\n    // High 16 bits: index of the lightmap in the binding array.\n    material_and_lightmap_bind_group_slot: u32,\n    timestamp: u32,\n    // User supplied index to identify the mesh instance\n    tag: u32,\n    pad: u32,\n}\n\n// The `wgpu` indirect parameters structure. This is a union of two structures.\n// For more information, see the corresponding comment in\n// `gpu_preprocessing.rs`.\nstruct IndirectParametersIndexed {\n    // `vertex_count` or `index_count`.\n    index_count: u32,\n    // `instance_count` in both structures.\n    instance_count: u32,\n    // `first_vertex` or `first_index`.\n    first_index: u32,\n    // `base_vertex` or `first_instance`.\n    base_vertex: u32,\n    // A read-only copy of `instance_index`.\n    first_instance: u32,\n}\n\nstruct IndirectParametersNonIndexed {\n    vertex_count: u32,\n    instance_count: u32,\n    base_vertex: u32,\n    first_instance: u32,\n}\n\nstruct IndirectParametersCpuMetadata {\n    base_output_index: u32,\n    batch_set_index: u32,\n}\n\nstruct IndirectParametersGpuMetadata {\n    mesh_index: u32,\n@if(WRITE_INDIRECT_PARAMETERS_METADATA)\n    early_instance_count: atomic<u32>,\n    late_instance_count: atomic<u32>,\n@else // #else   // WRITE_INDIRECT_PARAMETERS_METADATA\n    early_instance_count: u32,\n    late_instance_count: u32,\n// #endif  // WRITE_INDIRECT_PARAMETERS_METADATA\n}\n\nstruct IndirectBatchSet {\n    indirect_parameters_count: atomic<u32>,\n    indirect_parameters_base: u32,\n}\n","bevy::pbr::mesh_types":"struct Mesh {\n    // Affine 4x3 matrices transposed to 3x4\n    // Use package::render::maths::affine3_to_square to unpack\n    world_from_local: mat3x4<f32>,\n    previous_world_from_local: mat3x4<f32>,\n    // 3x3 matrix packed in mat2x4 and f32 as:\n    // [0].xyz, [1].x,\n    // [1].yz, [2].xy\n    // [2].z\n    // Use package::pbr::mesh_functions::mat2x4_f32_to_mat3x3_unpack to unpack\n    local_from_world_transpose_a: mat2x4<f32>,\n    local_from_world_transpose_b: f32,\n    // 'flags' is a bit field indicating various options. u32 is 32 bits so we have up to 32 options.\n    flags: u32,\n    lightmap_uv_rect: vec2<u32>,\n    // The index of the mesh's first vertex in the vertex buffer.\n    first_vertex_index: u32,\n    current_skin_index: u32,\n    // Low 16 bits: index of the material inside the bind group data.\n    // High 16 bits: index of the lightmap in the binding array.\n    material_and_lightmap_bind_group_slot: u32,\n    // User supplied index to identify the mesh instance\n    tag: u32,\n    pad: u32,\n};\n\n@if(SKINNED)\nstruct SkinnedMesh {\n    data: array<mat4x4<f32>, 256u>,\n};\n// #endif\n\n@if(MORPH_TARGETS)\nstruct MorphWeights {\n    weights: array<vec4<f32>, 16u>, // 16 = 64 / 4 (64 = MAX_MORPH_WEIGHTS)\n};\n// #endif\n\n// [2^0, 2^16)\nconst MESH_FLAGS_VISIBILITY_RANGE_INDEX_BITS: u32 = 65535u;\n// 2^28\nconst MESH_FLAGS_NO_FRUSTUM_CULLING_BIT: u32 = 268435456u;\n// 2^29\nconst MESH_FLAGS_SHADOW_RECEIVER_BIT: u32 = 536870912u;\n// 2^30\nconst MESH_FLAGS_TRANSMITTED_SHADOW_RECEIVER_BIT: u32 = 1073741824u;\n// 2^31 - if the flag is set, the sign is positive, else it is negative\nconst MESH_FLAGS_SIGN_DETERMINANT_MODEL_3X3_BIT: u32 = 2147483648u;\n","bevy::pbr::mesh_view_bindings":"import package::pbr::mesh_view_types as types;\nimport package::render::{\n    view::View,\n    globals::Globals,\n};\n\n@group(0) @binding(0) var<uniform> view: View;\n@group(0) @binding(1) var<uniform> lights: types::Lights;\n\n@if(NO_CUBE_ARRAY_TEXTURES_SUPPORT)\n@group(0) @binding(2) var point_shadow_textures: texture_depth_cube;\n@else\n@group(0) @binding(2) var point_shadow_textures: texture_depth_cube_array;\n\n@group(0) @binding(3) var point_shadow_textures_comparison_sampler: sampler_comparison;\n\n@if(PCSS_SAMPLERS_AVAILABLE)\n@group(0) @binding(4) var point_shadow_textures_linear_sampler: sampler;\n\n@if(NO_ARRAY_TEXTURES_SUPPORT)\n@group(0) @binding(5) var directional_shadow_textures: texture_depth_2d;\n@else\n@group(0) @binding(5) var directional_shadow_textures: texture_depth_2d_array;\n\n@group(0) @binding(6) var directional_shadow_textures_comparison_sampler: sampler_comparison;\n\n@if(PCSS_SAMPLERS_AVAILABLE)\n@group(0) @binding(7) var directional_shadow_textures_linear_sampler: sampler;\n\n@if(AVAILABLE_STORAGE_BUFFER_BINDINGS__GE_3)\n@group(0) @binding(8) var<storage> clusterable_objects: types::ClusterableObjects;\n@else\n@group(0) @binding(8) var<uniform> clusterable_objects: types::ClusterableObjects;\n\n@if(AVAILABLE_STORAGE_BUFFER_BINDINGS__GE_3)\n@group(0) @binding(9) var<storage> clusterable_object_index_lists: types::ClusterLightIndexLists;\n@else\n@group(0) @binding(9) var<uniform> clusterable_object_index_lists: types::ClusterLightIndexLists;\n\n@if(AVAILABLE_STORAGE_BUFFER_BINDINGS__GE_3)\n@group(0) @binding(10) var<storage> cluster_offsets_and_counts: types::ClusterOffsetsAndCounts;\n@else\n@group(0) @binding(10) var<uniform> cluster_offsets_and_counts: types::ClusterOffsetsAndCounts;\n\n@group(0) @binding(11) var<uniform> globals: Globals;\n@group(0) @binding(12) var<uniform> fog: types::Fog;\n@group(0) @binding(13) var<uniform> light_probes: types::LightProbes;\n\nconst VISIBILITY_RANGE_UNIFORM_BUFFER_SIZE: u32 = 64u;\n@if(AVAILABLE_STORAGE_BUFFER_BINDINGS__GE_6)\n@group(0) @binding(14) var<storage> visibility_ranges: array<vec4<f32>>;\n@else\n@group(0) @binding(14) var<uniform> visibility_ranges: array<vec4<f32>, VISIBILITY_RANGE_UNIFORM_BUFFER_SIZE>;\n\n@group(0) @binding(15) var<uniform> ssr_settings: types::ScreenSpaceReflectionsSettings;\n@group(0) @binding(16) var screen_space_ambient_occlusion_texture: texture_2d<f32>;\n\n@if(MULTIPLE_LIGHT_PROBES_IN_ARRAY)\n@group(0) @binding(17) var diffuse_environment_maps: binding_array<texture_cube<f32>, 8u>;\n@else\n@group(0) @binding(17) var diffuse_environment_map: texture_cube<f32>;\n\n@if(MULTIPLE_LIGHT_PROBES_IN_ARRAY)\n@group(0) @binding(18) var specular_environment_maps: binding_array<texture_cube<f32>, 8u>;\n@else\n@group(0) @binding(18) var specular_environment_map: texture_cube<f32>;\n\n@group(0) @binding(19) var environment_map_sampler: sampler;\n@group(0) @binding(20) var<uniform> environment_map_uniform: types::EnvironmentMapUniform;\n\n@if(IRRADIANCE_VOLUMES_ARE_USABLE && MULTIPLE_LIGHT_PROBES_IN_ARRAY)\n@group(0) @binding(21) var irradiance_volumes: binding_array<texture_3d<f32>, 8u>;\n@elif(IRRADIANCE_VOLUMES_ARE_USABLE)\n@group(0) @binding(21) var irradiance_volume: texture_3d<f32>;\n@if(IRRADIANCE_VOLUMES_ARE_USABLE)\n@group(0) @binding(22) var irradiance_volume_sampler: sampler;\n\n@if(CLUSTERED_DECALS_ARE_USABLE)\n@group(0) @binding(23) var<storage> clustered_decals: types::ClusteredDecals;\n\n@if(CLUSTERED_DECALS_ARE_USABLE)\n@group(0) @binding(24) var clustered_decal_textures: binding_array<texture_2d<f32>, 8u>;\n\n@if(CLUSTERED_DECALS_ARE_USABLE)\n@group(0) @binding(25) var clustered_decal_sampler: sampler;\n\n// NB: If you change these, make sure to update `tonemapping_shared.wgsl` too.\n@group(0) @binding(26) var dt_lut_texture: texture_3d<f32>;\n@group(0) @binding(27) var dt_lut_sampler: sampler;\n\n@if(MULTISAMPLED && DEPTH_PREPASS)\n@group(0) @binding(28) var depth_prepass_texture: texture_depth_multisampled_2d;\n\n@if(MULTISAMPLED && NORMAL_PREPASS)\n@group(0) @binding(29) var normal_prepass_texture: texture_multisampled_2d<f32>;\n\n@if(MULTISAMPLED && MOTION_VECTOR_PREPASS)\n@group(0) @binding(30) var motion_vector_prepass_texture: texture_multisampled_2d<f32>;\n\n@if(!MULTISAMPLED && DEPTH_PREPASS)\n@group(0) @binding(28) var depth_prepass_texture: texture_depth_2d;\n\n@if(!MULTISAMPLED && NORMAL_PREPASS)\n@group(0) @binding(29) var normal_prepass_texture: texture_2d<f32>;\n\n@if(!MULTISAMPLED && MOTION_VECTOR_PREPASS)\n@group(0) @binding(30) var motion_vector_prepass_texture: texture_2d<f32>;\n\n@if(DEFERRED_PREPASS)\n@group(0) @binding(31) var deferred_prepass_texture: texture_2d<u32>;\n\n@group(0) @binding(32) var view_transmission_texture: texture_2d<f32>;\n@group(0) @binding(33) var view_transmission_sampler: sampler;\n\n@if(OIT_ENABLED)\n@group(0) @binding(34) var<storage, read_write> oit_layers: array<vec2<u32>>;\n@if(OIT_ENABLED)\n@group(0) @binding(35) var<storage, read_write> oit_layer_ids: array<atomic<i32>>;\n@if(OIT_ENABLED)\n@group(0) @binding(36) var<uniform> oit_settings: types::OrderIndependentTransparencySettings;\n","bevy::pbr::mesh_view_types":"struct ClusterableObject {\n    // For point lights: the lower-right 2x2 values of the projection matrix [2][2] [2][3] [3][2] [3][3]\n    // For spot lights: the direction (x,z), spot_scale and spot_offset\n    light_custom_data: vec4<f32>,\n    color_inverse_square_range: vec4<f32>,\n    position_radius: vec4<f32>,\n    // 'flags' is a bit field indicating various options. u32 is 32 bits so we have up to 32 options.\n    flags: u32,\n    shadow_depth_bias: f32,\n    shadow_normal_bias: f32,\n    spot_light_tan_angle: f32,\n    soft_shadow_size: f32,\n    shadow_map_near_z: f32,\n    texture_index: u32,\n    pad: f32,\n};\n\nconst POINT_LIGHT_FLAGS_SHADOWS_ENABLED_BIT: u32                    = 1u;\nconst POINT_LIGHT_FLAGS_SPOT_LIGHT_Y_NEGATIVE: u32                  = 2u;\nconst POINT_LIGHT_FLAGS_VOLUMETRIC_BIT: u32                         = 4u;\nconst POINT_LIGHT_FLAGS_AFFECTS_LIGHTMAPPED_MESH_DIFFUSE_BIT: u32   = 8u;\n\nstruct DirectionalCascade {\n    clip_from_world: mat4x4<f32>,\n    texel_size: f32,\n    far_bound: f32,\n}\n\nstruct DirectionalLight {\n    cascades: array<DirectionalCascade, constants::MAX_CASCADES_PER_LIGHT>,\n    color: vec4<f32>,\n    direction_to_light: vec3<f32>,\n    // 'flags' is a bit field indicating various options. u32 is 32 bits so we have up to 32 options.\n    flags: u32,\n    soft_shadow_size: f32,\n    shadow_depth_bias: f32,\n    shadow_normal_bias: f32,\n    num_cascades: u32,\n    cascades_overlap_proportion: f32,\n    depth_texture_base_index: u32,\n    skip: u32,\n};\n\nconst DIRECTIONAL_LIGHT_FLAGS_SHADOWS_ENABLED_BIT: u32                  = 1u;\nconst DIRECTIONAL_LIGHT_FLAGS_VOLUMETRIC_BIT: u32                       = 2u;\nconst DIRECTIONAL_LIGHT_FLAGS_AFFECTS_LIGHTMAPPED_MESH_DIFFUSE_BIT: u32 = 4u;\n\nstruct Lights {\n    // NOTE: this array size must be kept in sync with the constants defined in bevy/pbr/src/render/light.rs\n    directional_lights: array<DirectionalLight, u32(constants::MAX_DIRECTIONAL_LIGHTS)>,\n    ambient_color: vec4<f32>,\n    // x/y/z dimensions and n_clusters in w\n    cluster_dimensions: vec4<u32>,\n    // xy are vec2<f32>(cluster_dimensions.xy) / vec2<f32>(view.width, view.height)\n    //\n    // For perspective projections:\n    // z is cluster_dimensions.z / log(far / near)\n    // w is cluster_dimensions.z * log(near) / log(far / near)\n    //\n    // For orthographic projections:\n    // NOTE: near and far are +ve but -z is infront of the camera\n    // z is -near\n    // w is cluster_dimensions.z / (-far - -near)\n    cluster_factors: vec4<f32>,\n    n_directional_lights: u32,\n    spot_light_shadowmap_offset: i32,\n    environment_map_smallest_specular_mip_level: u32,\n    environment_map_intensity: f32,\n};\n\nstruct Fog {\n    base_color: vec4<f32>,\n    directional_light_color: vec4<f32>,\n    // `be` and `bi` are allocated differently depending on the fog mode\n    //\n    // For Linear Fog:\n    //     be.x = start, be.y = end\n    // For Exponential and ExponentialSquared Fog:\n    //     be.x = density\n    // For Atmospheric Fog:\n    //     be = per-channel extinction density\n    //     bi = per-channel inscattering density\n    be: vec3<f32>,\n    directional_light_exponent: f32,\n    bi: vec3<f32>,\n    mode: u32,\n}\n\n// Important: These must be kept in sync with `fog.rs`\nconst FOG_MODE_OFF: u32                   = 0u;\nconst FOG_MODE_LINEAR: u32                = 1u;\nconst FOG_MODE_EXPONENTIAL: u32           = 2u;\nconst FOG_MODE_EXPONENTIAL_SQUARED: u32   = 3u;\nconst FOG_MODE_ATMOSPHERIC: u32           = 4u;\n\n@if(AVAILABLE_STORAGE_BUFFER_BINDINGS__GE_3)\nstruct ClusterableObjects {\n    data: array<ClusterableObject>,\n};\n@else\nstruct ClusterableObjects {\n    data: array<ClusterableObject, 204u>,\n};\n@if(AVAILABLE_STORAGE_BUFFER_BINDINGS__GE_3)\nstruct ClusterLightIndexLists {\n    data: array<u32>,\n};\n@else\nstruct ClusterLightIndexLists {\n    // each u32 contains 4 u8 indices into the ClusterableObjects array\n    data: array<vec4<u32>, 1024u>,\n};\n@if(AVAILABLE_STORAGE_BUFFER_BINDINGS__GE_3)\nstruct ClusterOffsetsAndCounts {\n    data: array<array<vec4<u32>, 2>>,\n};\n@else\nstruct ClusterOffsetsAndCounts {\n    // each u32 contains a 24-bit index into ClusterLightIndexLists in the high 24 bits\n    // and an 8-bit count of the number of lights in the low 8 bits\n    data: array<vec4<u32>, 1024u>,\n};\n// #endif\n\nstruct LightProbe {\n    // This is stored as the transpose in order to save space in this structure.\n    // It'll be transposed in the `environment_map_light` function.\n    light_from_world_transposed: mat3x4<f32>,\n    cubemap_index: i32,\n    intensity: f32,\n    // Whether this light probe contributes diffuse light to lightmapped meshes.\n    affects_lightmapped_mesh_diffuse: u32,\n};\n\nstruct LightProbes {\n    // This must match `MAX_VIEW_REFLECTION_PROBES` on the Rust side.\n    reflection_probes: array<LightProbe, 8u>,\n    irradiance_volumes: array<LightProbe, 8u>,\n    reflection_probe_count: i32,\n    irradiance_volume_count: i32,\n    // The index of the view environment map cubemap binding, or -1 if there's\n    // no such cubemap.\n    view_cubemap_index: i32,\n    // The smallest valid mipmap level for the specular environment cubemap\n    // associated with the view.\n    smallest_specular_mip_level_for_view: u32,\n    // The intensity of the environment map associated with the view.\n    intensity_for_view: f32,\n    // Whether the environment map attached to the view affects the diffuse\n    // lighting for lightmapped meshes.\n    view_environment_map_affects_lightmapped_mesh_diffuse: u32,\n};\n\n// Settings for screen space reflections.\n//\n// For more information on these settings, see the documentation for\n// `package::pbr::ssr::ScreenSpaceReflections`.\nstruct ScreenSpaceReflectionsSettings {\n    perceptual_roughness_threshold: f32,\n    thickness: f32,\n    linear_steps: u32,\n    linear_march_exponent: f32,\n    bisection_steps: u32,\n    use_secant: u32,\n};\n\nstruct EnvironmentMapUniform {\n    // Transformation matrix for the environment cubemaps in world space.\n    transform: mat4x4<f32>,\n};\n\n// Shader version of the order independent transparency settings component.\nstruct OrderIndependentTransparencySettings {\n  layers_count: i32,\n  alpha_threshold: f32,\n};\n\nstruct ClusteredDecal {\n    local_from_world: mat4x4<f32>,\n    image_index: i32,\n    tag: u32,\n    pad_a: u32,\n    pad_b: u32,\n}\n\nstruct ClusteredDecals {\n    decals: array<ClusteredDecal>,\n}\n","bevy::pbr::meshlet_bindings":"import package::pbr::mesh_types::Mesh;\nimport package::render::view::View;\nimport package::pbr::prepass_bindings::PreviousViewUniforms;\nimport package::pbr::utils::octahedral_decode_signed;\n\nstruct Meshlet {\n    start_vertex_position_bit: u32,\n    start_vertex_attribute_id: u32,\n    start_index_id: u32,\n    packed_a: u32,\n    packed_b: u32,\n    min_vertex_position_channel_x: f32,\n    min_vertex_position_channel_y: f32,\n    min_vertex_position_channel_z: f32,\n}\n\nfn get_meshlet_vertex_count(meshlet: ptr<function, Meshlet>) -> u32 {\n    return extractBits((*meshlet).packed_a, 0u, 8u);\n}\n\nfn get_meshlet_triangle_count(meshlet: ptr<function, Meshlet>) -> u32 {\n    return extractBits((*meshlet).packed_a, 8u, 8u);\n}\n\nstruct MeshletBoundingSpheres {\n    culling_sphere: MeshletBoundingSphere,\n    lod_group_sphere: MeshletBoundingSphere,\n    lod_parent_group_sphere: MeshletBoundingSphere,\n}\n\nstruct MeshletBoundingSphere {\n    center: vec3<f32>,\n    radius: f32,\n}\n\nstruct DispatchIndirectArgs {\n    x: atomic<u32>,\n    y: u32,\n    z: u32,\n}\n\nstruct DrawIndirectArgs {\n    vertex_count: u32,\n    instance_count: atomic<u32>,\n    first_vertex: u32,\n    first_instance: u32,\n}\n\nconst CENTIMETERS_PER_METER = 100.0;\n\n@if(MESHLET_FILL_CLUSTER_BUFFERS_PASS)\nvar<push_constant> scene_instance_count: u32;\n@if(MESHLET_FILL_CLUSTER_BUFFERS_PASS)\n@group(0) @binding(0) var<storage, read> meshlet_instance_meshlet_counts: array<u32>; // Per entity instance\n@if(MESHLET_FILL_CLUSTER_BUFFERS_PASS)\n@group(0) @binding(1) var<storage, read> meshlet_instance_meshlet_slice_starts: array<u32>; // Per entity instance\n@if(MESHLET_FILL_CLUSTER_BUFFERS_PASS)\n@group(0) @binding(2) var<storage, read_write> meshlet_cluster_instance_ids: array<u32>; // Per cluster\n@if(MESHLET_FILL_CLUSTER_BUFFERS_PASS)\n@group(0) @binding(3) var<storage, read_write> meshlet_cluster_meshlet_ids: array<u32>; // Per cluster\n@if(MESHLET_FILL_CLUSTER_BUFFERS_PASS)\n@group(0) @binding(4) var<storage, read_write> meshlet_global_cluster_count: atomic<u32>; // Single object shared between all workgroups\n\n@if(MESHLET_CULLING_PASS)\nstruct Constants { scene_cluster_count: u32, meshlet_raster_cluster_rightmost_slot: u32 }\n@if(MESHLET_CULLING_PASS)\nvar<push_constant> constants: Constants;\n@if(MESHLET_CULLING_PASS)\n@group(0) @binding(0) var<storage, read> meshlet_cluster_meshlet_ids: array<u32>; // Per cluster\n@if(MESHLET_CULLING_PASS)\n@group(0) @binding(1) var<storage, read> meshlet_bounding_spheres: array<MeshletBoundingSpheres>; // Per meshlet\n@if(MESHLET_CULLING_PASS)\n@group(0) @binding(2) var<storage, read> meshlet_simplification_errors: array<u32>; // Per meshlet\n@if(MESHLET_CULLING_PASS)\n@group(0) @binding(3) var<storage, read> meshlet_cluster_instance_ids: array<u32>; // Per cluster\n@if(MESHLET_CULLING_PASS)\n@group(0) @binding(4) var<storage, read> meshlet_instance_uniforms: array<Mesh>; // Per entity instance\n@if(MESHLET_CULLING_PASS)\n@group(0) @binding(5) var<storage, read> meshlet_view_instance_visibility: array<u32>; // 1 bit per entity instance, packed as a bitmask\n@if(MESHLET_CULLING_PASS)\n@group(0) @binding(6) var<storage, read_write> meshlet_second_pass_candidates: array<atomic<u32>>; // 1 bit per cluster , packed as a bitmask\n@if(MESHLET_CULLING_PASS)\n@group(0) @binding(7) var<storage, read_write> meshlet_software_raster_indirect_args: DispatchIndirectArgs; // Single object shared between all workgroups\n@if(MESHLET_CULLING_PASS)\n@group(0) @binding(8) var<storage, read_write> meshlet_hardware_raster_indirect_args: DrawIndirectArgs; // Single object shared between all workgroups\n@if(MESHLET_CULLING_PASS)\n@group(0) @binding(9) var<storage, read_write> meshlet_raster_clusters: array<u32>; // Single object shared between all workgroups\n@if(MESHLET_CULLING_PASS)\n@group(0) @binding(10) var depth_pyramid: texture_2d<f32>; // From the end of the last frame for the first culling pass, and from the first raster pass for the second culling pass\n@if(MESHLET_CULLING_PASS)\n@group(0) @binding(11) var<uniform> view: View;\n@if(MESHLET_CULLING_PASS)\n@group(0) @binding(12) var<uniform> previous_view: PreviousViewUniforms;\n\n@if(MESHLET_CULLING_PASS)\nfn should_cull_instance(instance_id: u32) -> bool {\n    let bit_offset = instance_id % 32u;\n    let packed_visibility = meshlet_view_instance_visibility[instance_id / 32u];\n    return bool(extractBits(packed_visibility, bit_offset, 1u));\n}\n\n// TODO: Load 4x per workgroup instead of once per thread?\n@if(MESHLET_CULLING_PASS)\nfn cluster_is_second_pass_candidate(cluster_id: u32) -> bool {\n    let packed_candidates = meshlet_second_pass_candidates[cluster_id / 32u];\n    let bit_offset = cluster_id % 32u;\n    return bool(extractBits(packed_candidates, bit_offset, 1u));\n}\n\n@if(MESHLET_VISIBILITY_BUFFER_RASTER_PASS)\n@group(0) @binding(0) var<storage, read> meshlet_cluster_meshlet_ids: array<u32>; // Per cluster\n@if(MESHLET_VISIBILITY_BUFFER_RASTER_PASS)\n@group(0) @binding(1) var<storage, read> meshlets: array<Meshlet>; // Per meshlet\n@if(MESHLET_VISIBILITY_BUFFER_RASTER_PASS)\n@group(0) @binding(2) var<storage, read> meshlet_indices: array<u32>; // Many per meshlet\n@if(MESHLET_VISIBILITY_BUFFER_RASTER_PASS)\n@group(0) @binding(3) var<storage, read> meshlet_vertex_positions: array<u32>; // Many per meshlet\n@if(MESHLET_VISIBILITY_BUFFER_RASTER_PASS)\n@group(0) @binding(4) var<storage, read> meshlet_cluster_instance_ids: array<u32>; // Per cluster\n@if(MESHLET_VISIBILITY_BUFFER_RASTER_PASS)\n@group(0) @binding(5) var<storage, read> meshlet_instance_uniforms: array<Mesh>; // Per entity instance\n@if(MESHLET_VISIBILITY_BUFFER_RASTER_PASS)\n@group(0) @binding(6) var<storage, read> meshlet_raster_clusters: array<u32>; // Single object shared between all workgroups\n@if(MESHLET_VISIBILITY_BUFFER_RASTER_PASS)\n@group(0) @binding(7) var<storage, read> meshlet_software_raster_cluster_count: u32;\n@if(MESHLET_VISIBILITY_BUFFER_RASTER_PASS && MESHLET_VISIBILITY_BUFFER_RASTER_PASS_OUTPUT)\n@group(0) @binding(8) var meshlet_visibility_buffer: texture_storage_2d<r64uint, atomic>;\n@elif(MESHLET_VISIBILITY_BUFFER_RASTER_PASS)\n@group(0) @binding(8) var meshlet_visibility_buffer: texture_storage_2d<r32uint, atomic>;\n@if(MESHLET_VISIBILITY_BUFFER_RASTER_PASS)\n@group(0) @binding(9) var<uniform> view: View;\n\n// TODO: Load only twice, instead of 3x in cases where you load 3 indices per thread?\n@if(MESHLET_VISIBILITY_BUFFER_RASTER_PASS)\nfn get_meshlet_vertex_id(index_id: u32) -> u32 {\n    let packed_index = meshlet_indices[index_id / 4u];\n    let bit_offset = (index_id % 4u) * 8u;\n    return extractBits(packed_index, bit_offset, 8u);\n}\n\n@if(MESHLET_VISIBILITY_BUFFER_RASTER_PASS)\nfn get_meshlet_vertex_position(meshlet: ptr<function, Meshlet>, vertex_id: u32) -> vec3<f32> {\n    // Get bitstream start for the vertex\n    let unpacked = unpack4xU8((*meshlet).packed_b);\n    let bits_per_channel = unpacked.xyz;\n    let bits_per_vertex = bits_per_channel.x + bits_per_channel.y + bits_per_channel.z;\n    var start_bit = (*meshlet).start_vertex_position_bit + (vertex_id * bits_per_vertex);\n\n    // Read each vertex channel from the bitstream\n    var vertex_position_packed = vec3(0u);\n    for (var i = 0u; i < 3u; i++) {\n        let lower_word_index = start_bit / 32u;\n        let lower_word_bit_offset = start_bit & 31u;\n        var next_32_bits = meshlet_vertex_positions[lower_word_index] >> lower_word_bit_offset;\n        if lower_word_bit_offset + bits_per_channel[i] > 32u {\n            next_32_bits |= meshlet_vertex_positions[lower_word_index + 1u] << (32u - lower_word_bit_offset);\n        }\n        vertex_position_packed[i] = extractBits(next_32_bits, 0u, bits_per_channel[i]);\n        start_bit += bits_per_channel[i];\n    }\n\n    // Remap [0, range_max - range_min] vec3<u32> to [range_min, range_max] vec3<f32>\n    var vertex_position = vec3<f32>(vertex_position_packed) + vec3(\n        (*meshlet).min_vertex_position_channel_x,\n        (*meshlet).min_vertex_position_channel_y,\n        (*meshlet).min_vertex_position_channel_z,\n    );\n\n    // Reverse vertex quantization\n    let vertex_position_quantization_factor = unpacked.w;\n    vertex_position /= f32(1u << vertex_position_quantization_factor) * CENTIMETERS_PER_METER;\n\n    return vertex_position;\n}\n\n@if(MESHLET_MESH_MATERIAL_PASS)\n@group(1) @binding(0) var meshlet_visibility_buffer: texture_storage_2d<r64uint, read>;\n@if(MESHLET_MESH_MATERIAL_PASS)\n@group(1) @binding(1) var<storage, read> meshlet_cluster_meshlet_ids: array<u32>; // Per cluster\n@if(MESHLET_MESH_MATERIAL_PASS)\n@group(1) @binding(2) var<storage, read> meshlets: array<Meshlet>; // Per meshlet\n@if(MESHLET_MESH_MATERIAL_PASS)\n@group(1) @binding(3) var<storage, read> meshlet_indices: array<u32>; // Many per meshlet\n@if(MESHLET_MESH_MATERIAL_PASS)\n@group(1) @binding(4) var<storage, read> meshlet_vertex_positions: array<u32>; // Many per meshlet\n@if(MESHLET_MESH_MATERIAL_PASS)\n@group(1) @binding(5) var<storage, read> meshlet_vertex_normals: array<u32>; // Many per meshlet\n@if(MESHLET_MESH_MATERIAL_PASS)\n@group(1) @binding(6) var<storage, read> meshlet_vertex_uvs: array<vec2<f32>>; // Many per meshlet\n@if(MESHLET_MESH_MATERIAL_PASS)\n@group(1) @binding(7) var<storage, read> meshlet_cluster_instance_ids: array<u32>; // Per cluster\n@if(MESHLET_MESH_MATERIAL_PASS)\n@group(1) @binding(8) var<storage, read> meshlet_instance_uniforms: array<Mesh>; // Per entity instance\n\n// TODO: Load only twice, instead of 3x in cases where you load 3 indices per thread?\n@if(MESHLET_MESH_MATERIAL_PASS)\nfn get_meshlet_vertex_id(index_id: u32) -> u32 {\n    let packed_index = meshlet_indices[index_id / 4u];\n    let bit_offset = (index_id % 4u) * 8u;\n    return extractBits(packed_index, bit_offset, 8u);\n}\n\n@if(MESHLET_MESH_MATERIAL_PASS)\nfn get_meshlet_vertex_position(meshlet: ptr<function, Meshlet>, vertex_id: u32) -> vec3<f32> {\n    // Get bitstream start for the vertex\n    let unpacked = unpack4xU8((*meshlet).packed_b);\n    let bits_per_channel = unpacked.xyz;\n    let bits_per_vertex = bits_per_channel.x + bits_per_channel.y + bits_per_channel.z;\n    var start_bit = (*meshlet).start_vertex_position_bit + (vertex_id * bits_per_vertex);\n\n    // Read each vertex channel from the bitstream\n    var vertex_position_packed = vec3(0u);\n    for (var i = 0u; i < 3u; i++) {\n        let lower_word_index = start_bit / 32u;\n        let lower_word_bit_offset = start_bit & 31u;\n        var next_32_bits = meshlet_vertex_positions[lower_word_index] >> lower_word_bit_offset;\n        if lower_word_bit_offset + bits_per_channel[i] > 32u {\n            next_32_bits |= meshlet_vertex_positions[lower_word_index + 1u] << (32u - lower_word_bit_offset);\n        }\n        vertex_position_packed[i] = extractBits(next_32_bits, 0u, bits_per_channel[i]);\n        start_bit += bits_per_channel[i];\n    }\n\n    // Remap [0, range_max - range_min] vec3<u32> to [range_min, range_max] vec3<f32>\n    var vertex_position = vec3<f32>(vertex_position_packed) + vec3(\n        (*meshlet).min_vertex_position_channel_x,\n        (*meshlet).min_vertex_position_channel_y,\n        (*meshlet).min_vertex_position_channel_z,\n    );\n\n    // Reverse vertex quantization\n    let vertex_position_quantization_factor = unpacked.w;\n    vertex_position /= f32(1u << vertex_position_quantization_factor) * CENTIMETERS_PER_METER;\n\n    return vertex_position;\n}\n\n@if(MESHLET_MESH_MATERIAL_PASS)\nfn get_meshlet_vertex_normal(meshlet: ptr<function, Meshlet>, vertex_id: u32) -> vec3<f32> {\n    let packed_normal = meshlet_vertex_normals[(*meshlet).start_vertex_attribute_id + vertex_id];\n    return octahedral_decode_signed(unpack2x16snorm(packed_normal));\n}\n\n@if(MESHLET_MESH_MATERIAL_PASS)\nfn get_meshlet_vertex_uv(meshlet: ptr<function, Meshlet>, vertex_id: u32) -> vec2<f32> {\n    return meshlet_vertex_uvs[(*meshlet).start_vertex_attribute_id + vertex_id];\n}\n","bevy::pbr::meshlet_visibility_buffer_resolve":"import package::pbr::{\n    meshlet_bindings::{\n        Meshlet,\n        meshlet_visibility_buffer,\n        meshlet_cluster_meshlet_ids,\n        meshlets,\n        meshlet_cluster_instance_ids,\n        meshlet_instance_uniforms,\n        get_meshlet_vertex_id,\n        get_meshlet_vertex_position,\n        get_meshlet_vertex_normal,\n        get_meshlet_vertex_uv,\n    },\n    mesh_view_bindings::view,\n    mesh_functions::mesh_position_local_to_world,\n    mesh_types::Mesh,\n    view_transformations::{position_world_to_clip, frag_coord_to_ndc},\n};\nimport package::render::maths::{affine3_to_square, mat2x4_f32_to_mat3x3_unpack};\n\n@if(PREPASS_FRAGMENT && MOTION_VECTOR_PREPASS)\nimport package::pbr::{\n    prepass_bindings::previous_view_uniforms,\n    pbr_prepass_functions::calculate_motion_vector,\n};\n\n/// Functions to be used by materials for reading from a meshlet visibility buffer texture.\n\n\n@if(MESHLET_MESH_MATERIAL_PASS)\nstruct PartialDerivatives {\n    barycentrics: vec3<f32>,\n    ddx: vec3<f32>,\n    ddy: vec3<f32>,\n}\n\n// https://github.com/ConfettiFX/The-Forge/blob/9d43e69141a9cd0ce2ce2d2db5122234d3a2d5b5/Common_3/Renderer/VisibilityBuffer2/Shaders/FSL/vb_shading_utilities.h.fsl#L90-L150\n@if(MESHLET_MESH_MATERIAL_PASS)\nfn compute_partial_derivatives(vertex_world_positions: array<vec4<f32>, 3>, ndc_uv: vec2<f32>, half_screen_size: vec2<f32>) -> PartialDerivatives {\n    var result: PartialDerivatives;\n\n    let vertex_clip_position_0 = position_world_to_clip(vertex_world_positions[0].xyz);\n    let vertex_clip_position_1 = position_world_to_clip(vertex_world_positions[1].xyz);\n    let vertex_clip_position_2 = position_world_to_clip(vertex_world_positions[2].xyz);\n\n    let inv_w = 1.0 / vec3(vertex_clip_position_0.w, vertex_clip_position_1.w, vertex_clip_position_2.w);\n    let ndc_0 = vertex_clip_position_0.xy * inv_w[0];\n    let ndc_1 = vertex_clip_position_1.xy * inv_w[1];\n    let ndc_2 = vertex_clip_position_2.xy * inv_w[2];\n\n    let inv_det = 1.0 / determinant(mat2x2(ndc_2 - ndc_1, ndc_0 - ndc_1));\n    result.ddx = vec3(ndc_1.y - ndc_2.y, ndc_2.y - ndc_0.y, ndc_0.y - ndc_1.y) * inv_det * inv_w;\n    result.ddy = vec3(ndc_2.x - ndc_1.x, ndc_0.x - ndc_2.x, ndc_1.x - ndc_0.x) * inv_det * inv_w;\n\n    var ddx_sum = dot(result.ddx, vec3(1.0));\n    var ddy_sum = dot(result.ddy, vec3(1.0));\n\n    let delta_v = ndc_uv - ndc_0;\n    let interp_inv_w = inv_w.x + delta_v.x * ddx_sum + delta_v.y * ddy_sum;\n    let interp_w = 1.0 / interp_inv_w;\n\n    result.barycentrics = vec3(\n        interp_w * (inv_w[0] + delta_v.x * result.ddx.x + delta_v.y * result.ddy.x),\n        interp_w * (delta_v.x * result.ddx.y + delta_v.y * result.ddy.y),\n        interp_w * (delta_v.x * result.ddx.z + delta_v.y * result.ddy.z),\n    );\n\n    result.ddx *= half_screen_size.x;\n    result.ddy *= half_screen_size.y;\n    ddx_sum *= half_screen_size.x;\n    ddy_sum *= half_screen_size.y;\n\n    result.ddy *= -1.0;\n    ddy_sum *= -1.0;\n\n    let interp_ddx_w = 1.0 / (interp_inv_w + ddx_sum);\n    let interp_ddy_w = 1.0 / (interp_inv_w + ddy_sum);\n\n    result.ddx = interp_ddx_w * (result.barycentrics * interp_inv_w + result.ddx) - result.barycentrics;\n    result.ddy = interp_ddy_w * (result.barycentrics * interp_inv_w + result.ddy) - result.barycentrics;\n    return result;\n}\n\n@if(MESHLET_MESH_MATERIAL_PASS)\nstruct VertexOutput {\n    position: vec4<f32>,\n    world_position: vec4<f32>,\n    world_normal: vec3<f32>,\n    uv: vec2<f32>,\n    ddx_uv: vec2<f32>,\n    ddy_uv: vec2<f32>,\n    world_tangent: vec4<f32>,\n    mesh_flags: u32,\n    cluster_id: u32,\n    material_bind_group_slot: u32,\n    @if(PREPASS_FRAGMENT && MOTION_VECTOR_PREPASS)\n    motion_vector: vec2<f32>,\n}\n\n/// Load the visibility buffer texture and resolve it into a VertexOutput.\n@if(MESHLET_MESH_MATERIAL_PASS)\nfn resolve_vertex_output(frag_coord: vec4<f32>) -> VertexOutput {\n    let packed_ids = u32(textureLoad(meshlet_visibility_buffer, vec2<u32>(frag_coord.xy)).r);\n    let cluster_id = packed_ids >> 7u;\n    let meshlet_id = meshlet_cluster_meshlet_ids[cluster_id];\n    var meshlet = meshlets[meshlet_id];\n\n    let triangle_id = extractBits(packed_ids, 0u, 7u);\n    let index_ids = meshlet.start_index_id + (triangle_id * 3u) + vec3(0u, 1u, 2u);\n    let vertex_ids = vec3(get_meshlet_vertex_id(index_ids[0]), get_meshlet_vertex_id(index_ids[1]), get_meshlet_vertex_id(index_ids[2]));\n    let vertex_0 = load_vertex(&meshlet, vertex_ids[0]);\n    let vertex_1 = load_vertex(&meshlet, vertex_ids[1]);\n    let vertex_2 = load_vertex(&meshlet, vertex_ids[2]);\n\n    let instance_id = meshlet_cluster_instance_ids[cluster_id];\n    var instance_uniform = meshlet_instance_uniforms[instance_id];\n\n    let world_from_local = affine3_to_square(instance_uniform.world_from_local);\n    let world_position_0 = mesh_position_local_to_world(world_from_local, vec4(vertex_0.position, 1.0));\n    let world_position_1 = mesh_position_local_to_world(world_from_local, vec4(vertex_1.position, 1.0));\n    let world_position_2 = mesh_position_local_to_world(world_from_local, vec4(vertex_2.position, 1.0));\n\n    let frag_coord_ndc = frag_coord_to_ndc(frag_coord).xy;\n    let partial_derivatives = compute_partial_derivatives(\n        array(world_position_0, world_position_1, world_position_2),\n        frag_coord_ndc,\n        view.viewport.zw / 2.0,\n    );\n\n    let world_position = mat3x4(world_position_0, world_position_1, world_position_2) * partial_derivatives.barycentrics;\n    let world_positions_camera_relative = mat3x3(\n        world_position_0.xyz - view.world_position,\n        world_position_1.xyz - view.world_position,\n        world_position_2.xyz - view.world_position,\n    );\n    let ddx_world_position = world_positions_camera_relative * partial_derivatives.ddx;\n    let ddy_world_position = world_positions_camera_relative * partial_derivatives.ddy;\n\n    let world_normal = mat3x3(\n        normal_local_to_world(vertex_0.normal, &instance_uniform),\n        normal_local_to_world(vertex_1.normal, &instance_uniform),\n        normal_local_to_world(vertex_2.normal, &instance_uniform),\n    ) * partial_derivatives.barycentrics;\n\n    let uv = mat3x2(vertex_0.uv, vertex_1.uv, vertex_2.uv) * partial_derivatives.barycentrics;\n    let ddx_uv = mat3x2(vertex_0.uv, vertex_1.uv, vertex_2.uv) * partial_derivatives.ddx;\n    let ddy_uv = mat3x2(vertex_0.uv, vertex_1.uv, vertex_2.uv) * partial_derivatives.ddy;\n\n    let world_tangent = calculate_world_tangent(world_normal, ddx_world_position, ddy_world_position, ddx_uv, ddy_uv);\n\n    // TODO(mbr): this is not ideal\n    @if(PREPASS_FRAGMENT && MOTION_VECTOR_PREPASS) {\n        let previous_world_from_local = affine3_to_square(instance_uniform.previous_world_from_local);\n        let previous_world_position_0 = mesh_position_local_to_world(previous_world_from_local, vec4(vertex_0.position, 1.0));\n        let previous_world_position_1 = mesh_position_local_to_world(previous_world_from_local, vec4(vertex_1.position, 1.0));\n        let previous_world_position_2 = mesh_position_local_to_world(previous_world_from_local, vec4(vertex_2.position, 1.0));\n        let previous_world_position = mat3x4(previous_world_position_0, previous_world_position_1, previous_world_position_2) * partial_derivatives.barycentrics;\n        let motion_vector = calculate_motion_vector(world_position, previous_world_position);\n\n        return VertexOutput(\n            frag_coord,\n            world_position,\n            world_normal,\n            uv,\n            ddx_uv,\n            ddy_uv,\n            world_tangent,\n            instance_uniform.flags,\n            instance_id ^ meshlet_id,\n            instance_uniform.material_and_lightmap_bind_group_slot & 0xffffu,\n            motion_vector\n        );\n    }\n    @else {\n        return VertexOutput(\n            frag_coord,\n            world_position,\n            world_normal,\n            uv,\n            ddx_uv,\n            ddy_uv,\n            world_tangent,\n            instance_uniform.flags,\n            instance_id ^ meshlet_id,\n            instance_uniform.material_and_lightmap_bind_group_slot & 0xffffu,\n        );\n    }\n}\n\n@if(MESHLET_MESH_MATERIAL_PASS)\nstruct MeshletVertex {\n    position: vec3<f32>,\n    normal: vec3<f32>,\n    uv: vec2<f32>,\n}\n\n@if(MESHLET_MESH_MATERIAL_PASS)\nfn load_vertex(meshlet: ptr<function, Meshlet>, vertex_id: u32) -> MeshletVertex {\n    return MeshletVertex(\n        get_meshlet_vertex_position(meshlet, vertex_id),\n        get_meshlet_vertex_normal(meshlet, vertex_id),\n        get_meshlet_vertex_uv(meshlet, vertex_id),\n    );\n}\n\n@if(MESHLET_MESH_MATERIAL_PASS)\nfn normal_local_to_world(vertex_normal: vec3<f32>, instance_uniform: ptr<function, Mesh>) -> vec3<f32> {\n    if any(vertex_normal != vec3<f32>(0.0)) {\n        return normalize(\n            mat2x4_f32_to_mat3x3_unpack(\n                (*instance_uniform).local_from_world_transpose_a,\n                (*instance_uniform).local_from_world_transpose_b,\n            ) * vertex_normal\n        );\n    } else {\n        return vertex_normal;\n    }\n}\n\n// https://www.jeremyong.com/graphics/2023/12/16/surface-gradient-bump-mapping/#surface-gradient-from-a-tangent-space-normal-vector-without-an-explicit-tangent-basis\n@if(MESHLET_MESH_MATERIAL_PASS)\nfn calculate_world_tangent(\n    world_normal: vec3<f32>,\n    ddx_world_position: vec3<f32>,\n    ddy_world_position: vec3<f32>,\n    ddx_uv: vec2<f32>,\n    ddy_uv: vec2<f32>,\n) -> vec4<f32> {\n    // Project the position gradients onto the tangent plane\n    let ddx_world_position_s = ddx_world_position - dot(ddx_world_position, world_normal) * world_normal;\n    let ddy_world_position_s = ddy_world_position - dot(ddy_world_position, world_normal) * world_normal;\n\n    // Compute the jacobian matrix to leverage the chain rule\n    let jacobian_sign = sign(ddx_uv.x * ddy_uv.y - ddx_uv.y * ddy_uv.x);\n\n    var world_tangent = jacobian_sign * (ddy_uv.y * ddx_world_position_s - ddx_uv.y * ddy_world_position_s);\n\n    // The sign intrinsic returns 0 if the argument is 0\n    if jacobian_sign != 0.0 {\n        world_tangent = normalize(world_tangent);\n    }\n\n    // The second factor here ensures a consistent handedness between\n    // the tangent frame and surface basis w.r.t. screenspace.\n    let w = jacobian_sign * sign(dot(ddy_world_position, cross(world_normal, ddx_world_position)));\n\n    return vec4(world_tangent, -w); // TODO: Unclear why we need to negate this to match mikktspace generated tangents\n}\n","bevy::pbr::morph":"import package::pbr::mesh_types::MorphWeights;\n\n@if(!MORPH_TARGETS)\nconst module_requires_flag_MORPH_TARGETS = false;\n@if(!MORPH_TARGETS)\nconst_assert module_requires_flag_MORPH_TARGETS; // module requires feature flag MORPH_TARGETS\n\n@group(1) @binding(2) var<uniform> morph_weights: MorphWeights;\n@group(1) @binding(3) var morph_targets: texture_3d<f32>;\n@group(1) @binding(7) var<uniform> prev_morph_weights: MorphWeights;\n\n// NOTE: Those are the \"hardcoded\" values found in `MorphAttributes` struct\n// in crates/bevy/render/src/mesh/morph/visitors.rs\n// In an ideal world, the offsets are established dynamically and passed as #defines\n// to the shader, but it's out of scope for the initial implementation of morph targets.\nconst position_offset: u32 = 0u;\nconst normal_offset: u32 = 3u;\nconst tangent_offset: u32 = 6u;\nconst total_component_count: u32 = 9u;\n\nfn layer_count() -> u32 {\n    let dimensions = textureDimensions(morph_targets);\n    return u32(dimensions.z);\n}\nfn component_texture_coord(vertex_index: u32, component_offset: u32) -> vec2<u32> {\n    let width = u32(textureDimensions(morph_targets).x);\n    let component_index = total_component_count * vertex_index + component_offset;\n    return vec2<u32>(component_index % width, component_index / width);\n}\nfn weight_at(weight_index: u32) -> f32 {\n    let i = weight_index;\n    return morph_weights.weights[i / 4u][i % 4u];\n}\nfn prev_weight_at(weight_index: u32) -> f32 {\n    let i = weight_index;\n    return prev_morph_weights.weights[i / 4u][i % 4u];\n}\nfn morph_pixel(vertex: u32, component: u32, weight: u32) -> f32 {\n    let coord = component_texture_coord(vertex, component);\n    // Due to https://gpuweb.github.io/gpuweb/wgsl/#texel-formats\n    // While the texture stores a f32, the textureLoad returns a vec4<>, where\n    // only the first component is set.\n    return textureLoad(morph_targets, vec3(coord, weight), 0).r;\n}\nfn morph(vertex_index: u32, component_offset: u32, weight_index: u32) -> vec3<f32> {\n    return vec3<f32>(\n        morph_pixel(vertex_index, component_offset, weight_index),\n        morph_pixel(vertex_index, component_offset + 1u, weight_index),\n        morph_pixel(vertex_index, component_offset + 2u, weight_index),\n    );\n}\n","bevy::pbr::occlusion_culling":"// Occlusion culling utility functions.\n\nfn get_aabb_size_in_pixels(aabb: vec4<f32>, depth_pyramid: texture_2d<f32>) -> vec2<f32> {\n    let depth_pyramid_size_mip_0 = vec2<f32>(textureDimensions(depth_pyramid, 0));\n    let aabb_width_pixels = (aabb.z - aabb.x) * depth_pyramid_size_mip_0.x;\n    let aabb_height_pixels = (aabb.w - aabb.y) * depth_pyramid_size_mip_0.y;\n    return vec2(aabb_width_pixels, aabb_height_pixels);\n}\n\nfn get_occluder_depth(\n    aabb: vec4<f32>,\n    aabb_pixel_size: vec2<f32>,\n    depth_pyramid: texture_2d<f32>\n) -> f32 {\n    let aabb_width_pixels = aabb_pixel_size.x;\n    let aabb_height_pixels = aabb_pixel_size.y;\n\n    let depth_pyramid_size_mip_0 = vec2<f32>(textureDimensions(depth_pyramid, 0));\n    let depth_level = max(0, i32(ceil(log2(max(aabb_width_pixels, aabb_height_pixels))))); // TODO: Naga doesn't like this being a u32\n    let depth_pyramid_size = vec2<f32>(textureDimensions(depth_pyramid, depth_level));\n    let aabb_top_left = vec2<u32>(aabb.xy * depth_pyramid_size);\n\n    let depth_quad_a = textureLoad(depth_pyramid, aabb_top_left, depth_level).x;\n    let depth_quad_b = textureLoad(depth_pyramid, aabb_top_left + vec2(1u, 0u), depth_level).x;\n    let depth_quad_c = textureLoad(depth_pyramid, aabb_top_left + vec2(0u, 1u), depth_level).x;\n    let depth_quad_d = textureLoad(depth_pyramid, aabb_top_left + vec2(1u, 1u), depth_level).x;\n    return min(min(depth_quad_a, depth_quad_b), min(depth_quad_c, depth_quad_d));\n}\n","bevy::pbr::parallax_mapping":"@if(BINDLESS)\nimport package::render::bindless::{bindless_samplers_filtering, bindless_textures_2d};\n\nimport package::pbr::{\n    pbr_bindings::{depth_map_texture, depth_map_sampler},\n    mesh_bindings::mesh,\n};\n\n@if(BINDLESS)\nimport package::pbr::pbr_bindings::material_indices;\n\nfn sample_depth_map(uv: vec2<f32>, material_bind_group_slot: u32) -> f32 {\n    // We use `textureSampleLevel` over `textureSample` because the wgpu DX12\n    // backend (Fxc) panics when using \"gradient instructions\" inside a loop.\n    // It results in the whole loop being unrolled by the shader compiler,\n    // which it can't do because the upper limit of the loop in steep parallax\n    // mapping is a variable set by the user.\n    // The \"gradient instructions\" comes from `textureSample` computing MIP level\n    // based on UV derivative. With `textureSampleLevel`, we provide ourselves\n    // the MIP level, so no gradient instructions are used, and we can use\n    // sample_depth_map in our loop.\n    // See https://stackoverflow.com/questions/56581141/direct3d11-gradient-instruction-used-in-a-loop-with-varying-iteration-forcing\n    @if(BINDLESS) {\n        return textureSampleLevel(\n            bindless_textures_2d[material_indices[material_bind_group_slot].depth_map_texture],\n            bindless_samplers_filtering[material_indices[material_bind_group_slot].depth_map_sampler],\n            uv,\n            0.0\n        ).r;\n    }\n    @else {\n        return textureSampleLevel(\n            depth_map_texture,\n            depth_map_sampler,\n            uv,\n            0.0\n        ).r;\n    }\n}\n\n// An implementation of parallax mapping, see https://en.wikipedia.org/wiki/Parallax_mapping\n// Code derived from: https://web.archive.org/web/20150419215321/http://sunandblackcat.com/tipFullView.php?l=eng&topicid=28\nfn parallaxed_uv(\n    depth_scale: f32,\n    max_layer_count: f32,\n    max_steps: u32,\n    // The original interpolated uv\n    original_uv: vec2<f32>,\n    // The vector from the camera to the fragment at the surface in tangent space\n    Vt: vec3<f32>,\n    material_bind_group_slot: u32,\n) -> vec2<f32> {\n    if max_layer_count < 1.0 {\n        return original_uv;\n    }\n    var uv = original_uv;\n\n    // Steep Parallax Mapping\n    // ======================\n    // Split the depth map into `layer_count` layers.\n    // When Vt hits the surface of the mesh (excluding depth displacement),\n    // if the depth is not below or on surface including depth displacement (textureSample), then\n    // look forward (+= delta_uv) on depth texture according to\n    // Vt and distance between hit surface and depth map surface,\n    // repeat until below the surface.\n    //\n    // Where `layer_count` is interpolated between `1.0` and\n    // `max_layer_count` according to the steepness of Vt.\n\n    let view_steepness = abs(Vt.z);\n    // We mix with minimum value 1.0 because otherwise,\n    // with 0.0, we get a division by zero in surfaces parallel to viewport,\n    // resulting in a singularity.\n    let layer_count = mix(max_layer_count, 1.0, view_steepness);\n    let layer_depth = 1.0 / layer_count;\n    var delta_uv = depth_scale * layer_depth * Vt.xy * vec2(1.0, -1.0) / view_steepness;\n\n    var current_layer_depth = 0.0;\n    var texture_depth = sample_depth_map(uv, material_bind_group_slot);\n\n    // texture_depth > current_layer_depth means the depth map depth is deeper\n    // than the depth the ray would be at this UV offset so the ray has not\n    // intersected the surface\n    for (var i: i32 = 0; texture_depth > current_layer_depth && i <= i32(layer_count); i++) {\n        current_layer_depth += layer_depth;\n        uv += delta_uv;\n        texture_depth = sample_depth_map(uv, material_bind_group_slot);\n    }\n\n    @if(RELIEF_MAPPING) {\n        // Relief Mapping\n        // ==============\n        // \"Refine\" the rough result from Steep Parallax Mapping\n        // with a **binary search** between the layer selected by steep parallax\n        // and the next one to find a point closer to the depth map surface.\n        // This reduces the jaggy step artifacts from steep parallax mapping.\n\n        delta_uv *= 0.5;\n        var delta_depth = 0.5 * layer_depth;\n\n        uv -= delta_uv;\n        current_layer_depth -= delta_depth;\n\n        for (var i: u32 = 0u; i < max_steps; i++) {\n            texture_depth = sample_depth_map(uv, material_bind_group_slot);\n\n            // Halve the deltas for the next step\n            delta_uv *= 0.5;\n            delta_depth *= 0.5;\n\n            // Step based on whether the current depth is above or below the depth map\n            if (texture_depth > current_layer_depth) {\n                uv += delta_uv;\n                current_layer_depth += delta_depth;\n            } else {\n                uv -= delta_uv;\n                current_layer_depth -= delta_depth;\n            }\n        }\n    }\n    @else {\n        // Parallax Occlusion mapping\n        // ==========================\n        // \"Refine\" Steep Parallax Mapping by interpolating between the\n        // previous layer's depth and the computed layer depth.\n        // Only requires a single lookup, unlike Relief Mapping, but\n        // may skip small details and result in writhing material artifacts.\n        let previous_uv = uv - delta_uv;\n        let next_depth = texture_depth - current_layer_depth;\n        let previous_depth = sample_depth_map(previous_uv, material_bind_group_slot) -\n            current_layer_depth + layer_depth;\n\n        let weight = next_depth / (next_depth - previous_depth);\n\n        uv = mix(uv, previous_uv, weight);\n\n        current_layer_depth += mix(next_depth, previous_depth, weight);\n    }\n\n    // Note: `current_layer_depth` is not returned, but may be useful\n    // for light computation later on in future improvements of the pbr shader.\n    return uv;\n}\n","bevy::pbr::pbr_bindings":"import package::pbr::pbr_types::StandardMaterial;\n\n@if(BINDLESS)\nstruct StandardMaterialBindings {\n    material: u32,                      // 0\n    base_color_texture: u32,            // 1\n    base_color_sampler: u32,            // 2\n    emissive_texture: u32,              // 3\n    emissive_sampler: u32,              // 4\n    metallic_roughness_texture: u32,    // 5\n    metallic_roughness_sampler: u32,    // 6\n    occlusion_texture: u32,             // 7\n    occlusion_sampler: u32,             // 8\n    normal_map_texture: u32,            // 9\n    normal_map_sampler: u32,            // 10\n    depth_map_texture: u32,             // 11\n    depth_map_sampler: u32,             // 12\n    anisotropy_texture: u32,            // 13\n    anisotropy_sampler: u32,            // 14\n    specular_transmission_texture: u32, // 15\n    specular_transmission_sampler: u32, // 16\n    thickness_texture: u32,             // 17\n    thickness_sampler: u32,             // 18\n    diffuse_transmission_texture: u32,  // 19\n    diffuse_transmission_sampler: u32,  // 20\n    clearcoat_texture: u32,             // 21\n    clearcoat_sampler: u32,             // 22\n    clearcoat_roughness_texture: u32,   // 23\n    clearcoat_roughness_sampler: u32,   // 24\n    clearcoat_normal_texture: u32,      // 25\n    clearcoat_normal_sampler: u32,      // 26\n    specular_texture: u32,              // 27\n    specular_sampler: u32,              // 28\n    specular_tint_texture: u32,         // 29\n    specular_tint_sampler: u32,         // 30\n}\n\n@if(BINDLESS)\n@group(2) @binding(0) var<storage> material_indices: array<StandardMaterialBindings>;\n@if(BINDLESS)\n@group(2) @binding(10) var<storage> material_array: array<StandardMaterial>;\n\n@if(!BINDLESS)\n@group(2) @binding(0) var<uniform> material: StandardMaterial;\n@if(!BINDLESS)\n@group(2) @binding(1) var base_color_texture: texture_2d<f32>;\n@if(!BINDLESS)\n@group(2) @binding(2) var base_color_sampler: sampler;\n@if(!BINDLESS)\n@group(2) @binding(3) var emissive_texture: texture_2d<f32>;\n@if(!BINDLESS)\n@group(2) @binding(4) var emissive_sampler: sampler;\n@if(!BINDLESS)\n@group(2) @binding(5) var metallic_roughness_texture: texture_2d<f32>;\n@if(!BINDLESS)\n@group(2) @binding(6) var metallic_roughness_sampler: sampler;\n@if(!BINDLESS)\n@group(2) @binding(7) var occlusion_texture: texture_2d<f32>;\n@if(!BINDLESS)\n@group(2) @binding(8) var occlusion_sampler: sampler;\n@if(!BINDLESS)\n@group(2) @binding(9) var normal_map_texture: texture_2d<f32>;\n@if(!BINDLESS)\n@group(2) @binding(10) var normal_map_sampler: sampler;\n@if(!BINDLESS)\n@group(2) @binding(11) var depth_map_texture: texture_2d<f32>;\n@if(!BINDLESS)\n@group(2) @binding(12) var depth_map_sampler: sampler;\n\n@if(!BINDLESS && PBR_ANISOTROPY_TEXTURE_SUPPORTED)\n@group(2) @binding(13) var anisotropy_texture: texture_2d<f32>;\n@if(!BINDLESS && PBR_ANISOTROPY_TEXTURE_SUPPORTED)\n@group(2) @binding(14) var anisotropy_sampler: sampler;\n\n@if(!BINDLESS && PBR_TRANSMISSION_TEXTURES_SUPPORTED)\n@group(2) @binding(15) var specular_transmission_texture: texture_2d<f32>;\n@if(!BINDLESS && PBR_TRANSMISSION_TEXTURES_SUPPORTED)\n@group(2) @binding(16) var specular_transmission_sampler: sampler;\n@if(!BINDLESS && PBR_TRANSMISSION_TEXTURES_SUPPORTED)\n@group(2) @binding(17) var thickness_texture: texture_2d<f32>;\n@if(!BINDLESS && PBR_TRANSMISSION_TEXTURES_SUPPORTED)\n@group(2) @binding(18) var thickness_sampler: sampler;\n@if(!BINDLESS && PBR_TRANSMISSION_TEXTURES_SUPPORTED)\n@group(2) @binding(19) var diffuse_transmission_texture: texture_2d<f32>;\n@if(!BINDLESS && PBR_TRANSMISSION_TEXTURES_SUPPORTED)\n@group(2) @binding(20) var diffuse_transmission_sampler: sampler;\n\n@if(!BINDLESS && PBR_MULTI_LAYER_MATERIAL_TEXTURES_SUPPORTED)\n@group(2) @binding(21) var clearcoat_texture: texture_2d<f32>;\n@if(!BINDLESS && PBR_MULTI_LAYER_MATERIAL_TEXTURES_SUPPORTED)\n@group(2) @binding(22) var clearcoat_sampler: sampler;\n@if(!BINDLESS && PBR_MULTI_LAYER_MATERIAL_TEXTURES_SUPPORTED)\n@group(2) @binding(23) var clearcoat_roughness_texture: texture_2d<f32>;\n@if(!BINDLESS && PBR_MULTI_LAYER_MATERIAL_TEXTURES_SUPPORTED)\n@group(2) @binding(24) var clearcoat_roughness_sampler: sampler;\n@if(!BINDLESS && PBR_MULTI_LAYER_MATERIAL_TEXTURES_SUPPORTED)\n@group(2) @binding(25) var clearcoat_normal_texture: texture_2d<f32>;\n@if(!BINDLESS && PBR_MULTI_LAYER_MATERIAL_TEXTURES_SUPPORTED)\n@group(2) @binding(26) var clearcoat_normal_sampler: sampler;\n\n@if(!BINDLESS && PBR_SPECULAR_TEXTURES_SUPPORTED)\n@group(2) @binding(27) var specular_texture: texture_2d<f32>;\n@if(!BINDLESS && PBR_SPECULAR_TEXTURES_SUPPORTED)\n@group(2) @binding(28) var specular_sampler: sampler;\n@if(!BINDLESS && PBR_SPECULAR_TEXTURES_SUPPORTED)\n@group(2) @binding(29) var specular_tint_texture: texture_2d<f32>;\n@if(!BINDLESS && PBR_SPECULAR_TEXTURES_SUPPORTED)\n@group(2) @binding(30) var specular_tint_sampler: sampler;\n","bevy::pbr::pbr_deferred_functions":"import package::pbr::{\n    pbr_types::{PbrInput, pbr_input_new, STANDARD_MATERIAL_FLAGS_UNLIT_BIT},\n    pbr_deferred_types as deferred_types,\n    pbr_functions,\n    rgb9e5,\n    mesh_view_bindings::view,\n    utils::{octahedral_encode, octahedral_decode},\n    prepass_io::FragmentOutput,\n    view_transformations::{position_ndc_to_world, frag_coord_to_ndc},\n};\n\n@if(MESHLET_MESH_MATERIAL_PASS)\nimport package::pbr::meshlet_visibility_buffer_resolve::VertexOutput;\n@else\nimport package::pbr::prepass_io::VertexOutput;\n\n@if(MOTION_VECTOR_PREPASS)\nimport package::pbr::pbr_prepass_functions::calculate_motion_vector;\n\n// Creates the deferred gbuffer from a PbrInput.\nfn deferred_gbuffer_from_pbr_input(in: PbrInput) -> vec4<u32> {\n    // Only monochrome occlusion supported. May not be worth including at all.\n    // Some models have baked occlusion, GLTF only supports monochrome.\n    // Real time occlusion is applied in the deferred lighting pass.\n    // Deriving luminance via Rec. 709. coefficients\n    // https://en.wikipedia.org/wiki/Rec._709\n    let rec_709_coeffs = vec3<f32>(0.2126, 0.7152, 0.0722);\n    let diffuse_occlusion = dot(in.diffuse_occlusion, rec_709_coeffs);\n    // Only monochrome specular supported.\n    let reflectance = dot(in.material.reflectance, rec_709_coeffs);\n\n    @if(WEBGL2)\n    var props = deferred_types::pack_unorm3x4_plus_unorm_20_(vec4(\n        reflectance,\n        in.material.metallic,\n        diffuse_occlusion,\n        in.frag_coord.z));\n    @else\n    var props = deferred_types::pack_unorm4x8_(vec4(\n        reflectance, // could be fewer bits\n        in.material.metallic, // could be fewer bits\n        diffuse_occlusion, // is this worth including?\n        0.0)); // spare\n\n    let flags = deferred_types::deferred_flags_from_mesh_material_flags(in.flags, in.material.flags);\n    let octahedral_normal = octahedral_encode(normalize(in.N));\n    var base_color_srgb = vec3(0.0);\n    var emissive = in.material.emissive.rgb;\n    if ((in.material.flags & STANDARD_MATERIAL_FLAGS_UNLIT_BIT) != 0u) {\n        // Material is unlit, use emissive component of gbuffer for color data.\n        // Unlit materials are effectively emissive.\n        emissive = in.material.base_color.rgb;\n    } else {\n        base_color_srgb = pow(in.material.base_color.rgb, vec3(1.0 / 2.2));\n    }\n\n    // Utilize the emissive channel to transmit the lightmap data. To ensure\n    // it matches the output in forward shading, pre-multiply it with the \n    // calculated diffuse color.\n    let base_color = in.material.base_color.rgb;\n    let metallic = in.material.metallic;\n    let specular_transmission = in.material.specular_transmission;\n    let diffuse_transmission = in.material.diffuse_transmission;\n    let diffuse_color = pbr_functions::calculate_diffuse_color(\n        base_color,\n        metallic,\n        specular_transmission,\n        diffuse_transmission\n    );\n    emissive += in.lightmap_light * diffuse_color * view.exposure;\n\n    let deferred = vec4(\n        deferred_types::pack_unorm4x8_(vec4(base_color_srgb, in.material.perceptual_roughness)),\n        rgb9e5::vec3_to_rgb9e5_(emissive),\n        props,\n        deferred_types::pack_24bit_normal_and_flags(octahedral_normal, flags),\n    );\n    return deferred;\n}\n\n// Creates a PbrInput from the deferred gbuffer.\nfn pbr_input_from_deferred_gbuffer(frag_coord: vec4<f32>, gbuffer: vec4<u32>) -> PbrInput {\n    var pbr = pbr_input_new();\n\n    let flags = deferred_types::unpack_flags(gbuffer.a);\n    let deferred_flags = deferred_types::mesh_material_flags_from_deferred_flags(flags);\n    pbr.flags = deferred_flags.x;\n    pbr.material.flags = deferred_flags.y;\n\n    let base_rough = deferred_types::unpack_unorm4x8_(gbuffer.r);\n    pbr.material.perceptual_roughness = base_rough.a;\n    let emissive = rgb9e5::rgb9e5_to_vec3_(gbuffer.g);\n    if ((pbr.material.flags & STANDARD_MATERIAL_FLAGS_UNLIT_BIT) != 0u) {\n        pbr.material.base_color = vec4(emissive, 1.0);\n        pbr.material.emissive = vec4(vec3(0.0), 0.0);\n    } else {\n        pbr.material.base_color = vec4(pow(base_rough.rgb, vec3(2.2)), 1.0);\n        pbr.material.emissive = vec4(emissive, 0.0);\n    }\n    @if(WEBGL2)\n    let props = deferred_types::unpack_unorm3x4_plus_unorm_20_(gbuffer.b);\n    // Bias to 0.5 since that's the value for almost all materials.\n    @else\n    let props = deferred_types::unpack_unorm4x8_(gbuffer.b);\n\n    @if(WEBGL2) {\n        pbr.material.reflectance = vec3(saturate(props.r - 0.03333333333));\n    }\n    @else {\n        pbr.material.reflectance = vec3(props.r);\n    }\n\n    pbr.material.metallic = props.g;\n    pbr.diffuse_occlusion = vec3(props.b);\n    let octahedral_normal = deferred_types::unpack_24bit_normal(gbuffer.a);\n    let N = octahedral_decode(octahedral_normal);\n\n    let world_position = vec4(position_ndc_to_world(frag_coord_to_ndc(frag_coord)), 1.0);\n    let is_orthographic = view.clip_from_view[3].w == 1.0;\n    let V = pbr_functions::calculate_view(world_position, is_orthographic);\n\n    pbr.frag_coord = frag_coord;\n    pbr.world_normal = N;\n    pbr.world_position = world_position;\n    pbr.N = N;\n    pbr.V = V;\n    pbr.is_orthographic = is_orthographic;\n\n    return pbr;\n}\n\n@if(PREPASS_PIPELINE)\nfn deferred_output(in: VertexOutput, pbr_input: PbrInput) -> FragmentOutput {\n    var out: FragmentOutput;\n\n    // gbuffer\n    out.deferred = deferred_gbuffer_from_pbr_input(pbr_input);\n    // lighting pass id (used to determine which lighting shader to run for the fragment)\n    out.deferred_lighting_pass_id = pbr_input.material.deferred_lighting_pass_id;\n    // normal if required\n    @if(NORMAL_PREPASS) {\n        out.normal = vec4(in.world_normal * 0.5 + vec3(0.5), 1.0);\n    }\n    // motion vectors if required\n    @if(MOTION_VECTOR_PREPASS) {\n        @if(MESHLET_MESH_MATERIAL_PASS) {\n            out.motion_vector = in.motion_vector;\n        }\n        @else {\n            out.motion_vector = calculate_motion_vector(in.world_position, in.previous_world_position);\n        }\n    }\n\n    return out;\n}\n","bevy::pbr::pbr_deferred_types":"import package::pbr::{\n    mesh_types::MESH_FLAGS_SHADOW_RECEIVER_BIT,\n    pbr_types::{STANDARD_MATERIAL_FLAGS_FOG_ENABLED_BIT, STANDARD_MATERIAL_FLAGS_UNLIT_BIT},\n};\n\n// Maximum of 8 bits available\nconst DEFERRED_FLAGS_UNLIT_BIT: u32                 = 1u;\nconst DEFERRED_FLAGS_FOG_ENABLED_BIT: u32           = 2u;\nconst DEFERRED_MESH_FLAGS_SHADOW_RECEIVER_BIT: u32  = 4u;\n\nfn deferred_flags_from_mesh_material_flags(mesh_flags: u32, mat_flags: u32) -> u32 {\n    var flags = 0u;\n    flags |= u32((mesh_flags & MESH_FLAGS_SHADOW_RECEIVER_BIT) != 0u) * DEFERRED_MESH_FLAGS_SHADOW_RECEIVER_BIT;\n    flags |= u32((mat_flags & STANDARD_MATERIAL_FLAGS_FOG_ENABLED_BIT) != 0u) * DEFERRED_FLAGS_FOG_ENABLED_BIT;\n    flags |= u32((mat_flags & STANDARD_MATERIAL_FLAGS_UNLIT_BIT) != 0u) * DEFERRED_FLAGS_UNLIT_BIT;\n    return flags;\n}\n\nfn mesh_material_flags_from_deferred_flags(deferred_flags: u32) -> vec2<u32> {\n    var mat_flags = 0u;\n    var mesh_flags = 0u;\n    mesh_flags |= u32((deferred_flags & DEFERRED_MESH_FLAGS_SHADOW_RECEIVER_BIT) != 0u) * MESH_FLAGS_SHADOW_RECEIVER_BIT;\n    mat_flags |= u32((deferred_flags & DEFERRED_FLAGS_FOG_ENABLED_BIT) != 0u) * STANDARD_MATERIAL_FLAGS_FOG_ENABLED_BIT;\n    mat_flags |= u32((deferred_flags & DEFERRED_FLAGS_UNLIT_BIT) != 0u) * STANDARD_MATERIAL_FLAGS_UNLIT_BIT;\n    return vec2(mesh_flags, mat_flags);\n}\n\nconst U12MAXF = 4095.0;\nconst U16MAXF = 65535.0;\nconst U20MAXF = 1048575.0;\n\n// Storing normals as oct24.\n// Flags are stored in the remaining 8 bits.\n// https://jcgt.org/published/0003/02/01/paper.pdf\n// Could possibly go down to oct20 if the space is needed.\n\nfn pack_24bit_normal_and_flags(octahedral_normal: vec2<f32>, flags: u32) -> u32 {\n    let unorm1 = u32(saturate(octahedral_normal.x) * U12MAXF + 0.5);\n    let unorm2 = u32(saturate(octahedral_normal.y) * U12MAXF + 0.5);\n    return (unorm1 & 0xFFFu) | ((unorm2 & 0xFFFu) << 12u) | ((flags & 0xFFu) << 24u);\n}\n\nfn unpack_24bit_normal(packed: u32) -> vec2<f32> {\n    let unorm1 = packed & 0xFFFu;\n    let unorm2 = (packed >> 12u) & 0xFFFu;\n    return vec2(f32(unorm1) / U12MAXF, f32(unorm2) / U12MAXF);\n}\n\nfn unpack_flags(packed: u32) -> u32 {\n    return (packed >> 24u) & 0xFFu;\n}\n\n// The builtin one didn't work in webgl.\n// \"'unpackUnorm4x8' : no matching overloaded function found\"\n// https://github.com/gfx-rs/naga/issues/2006\nfn unpack_unorm4x8_(v: u32) -> vec4<f32> {\n    return vec4(\n        f32(v & 0xFFu),\n        f32((v >> 8u) & 0xFFu),\n        f32((v >> 16u) & 0xFFu),\n        f32((v >> 24u) & 0xFFu)\n    ) / 255.0;\n}\n\n// 'packUnorm4x8' : no matching overloaded function found\n// https://github.com/gfx-rs/naga/issues/2006\nfn pack_unorm4x8_(values: vec4<f32>) -> u32 {\n    let v = vec4<u32>(saturate(values) * 255.0 + 0.5);\n    return (v.w << 24u) | (v.z << 16u) | (v.y << 8u) | v.x;\n}\n\n// Pack 3x 4bit unorm + 1x 20bit\nfn pack_unorm3x4_plus_unorm_20_(v: vec4<f32>) -> u32 {\n    let sm = vec3<u32>(saturate(v.xyz) * 15.0 + 0.5);\n    let bg = u32(saturate(v.w) * U20MAXF + 0.5);\n    return (bg << 12u) | (sm.z << 8u) | (sm.y << 4u) | sm.x;\n}\n\n// Unpack 3x 4bit unorm + 1x 20bit\nfn unpack_unorm3x4_plus_unorm_20_(v: u32) -> vec4<f32> {\n    return vec4(\n        f32(v & 0xfu) / 15.0,\n        f32((v >> 4u) & 0xFu) / 15.0,\n        f32((v >> 8u) & 0xFu) / 15.0,\n        f32((v >> 12u) & 0xFFFFFFu) / U20MAXF,\n    );\n}\n","bevy::pbr::pbr_fragment":"@if(BINDLESS)\nimport package::render::bindless::{bindless_samplers_filtering, bindless_textures_2d};\n\nimport package::pbr::{\n    pbr_functions,\n    pbr_functions::SampleBias,\n    pbr_bindings,\n    pbr_types,\n    prepass_utils,\n    lighting,\n    mesh_bindings::mesh,\n    mesh_view_bindings::view,\n    parallax_mapping::parallaxed_uv,\n    lightmap::lightmap,\n};\n\n@if(SCREEN_SPACE_AMBIENT_OCCLUSION)\nimport package::pbr::mesh_view_bindings::screen_space_ambient_occlusion_texture;\n@if(SCREEN_SPACE_AMBIENT_OCCLUSION)\nimport package::pbr::ssao_utils::ssao_multibounce;\n\n@if(MESHLET_MESH_MATERIAL_PASS)\nimport package::pbr::meshlet_visibility_buffer_resolve::VertexOutput;\n@elif(PREPASS_PIPELINE)\nimport package::pbr::prepass_io::VertexOutput;\n@else\nimport package::pbr::forward_io::VertexOutput;\n\n@if(BINDLESS)\nimport package::pbr::pbr_bindings::material_indices;\n\n// prepare a basic PbrInput from the vertex stage output, mesh binding and view binding\nfn pbr_input_from_vertex_output(\n    in: VertexOutput,\n    is_front: bool,\n    double_sided: bool,\n) -> pbr_types::PbrInput {\n    var pbr_input: pbr_types::PbrInput = pbr_types::pbr_input_new();\n\n    @if(MESHLET_MESH_MATERIAL_PASS) {\n        pbr_input.flags = in.mesh_flags;\n    }\n    @else {\n        pbr_input.flags = mesh[in.instance_index].flags;\n    }\n\n    pbr_input.is_orthographic = view.clip_from_view[3].w == 1.0;\n    pbr_input.V = pbr_functions::calculate_view(in.world_position, pbr_input.is_orthographic);\n    pbr_input.frag_coord = in.position;\n    pbr_input.world_position = in.world_position;\n\n    @if(VERTEX_COLORS) {\n        pbr_input.material.base_color = in.color;\n    }\n\n    pbr_input.world_normal = pbr_functions::prepare_world_normal(\n        in.world_normal,\n        double_sided,\n        is_front,\n    );\n\n    @if(LOAD_PREPASS_NORMALS) {\n        pbr_input.N = prepass_utils::prepass_normal(in.position, 0u);\n    }\n    @else {\n        pbr_input.N = normalize(pbr_input.world_normal);\n    }\n\n    return pbr_input;\n}\n\n// Prepare a full PbrInput by sampling all textures to resolve\n// the material members\nfn pbr_input_from_standard_material(\n    in: VertexOutput,\n    is_front: bool,\n) -> pbr_types::PbrInput {\n    @if(MESHLET_MESH_MATERIAL_PASS)\n    let slot = in.material_bind_group_slot;\n    @else\n    let slot = mesh[in.instance_index].material_and_lightmap_bind_group_slot & 0xffffu;\n\n    @if(BINDLESS)\n    let material = pbr_bindings::material_array[material_indices[slot].material];\n    @else\n    let material = pbr_bindings::material;\n\n    let double_sided = (material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_DOUBLE_SIDED_BIT) != 0u;\n\n    var pbr_input: pbr_types::PbrInput = pbr_input_from_vertex_output(in, is_front, double_sided);\n    pbr_input.material.flags = material.flags;\n    pbr_input.material.base_color *= material.base_color;\n    pbr_input.material.deferred_lighting_pass_id = material.deferred_lighting_pass_id;\n\n    // Neubelt and Pettineo 2013, \"Crafting a Next-gen Material Pipeline for The Order: 1886\"\n    let NdotV = max(dot(pbr_input.N, pbr_input.V), 0.0001);\n\n    // Fill in the sample bias so we can sample from textures.\n    var bias: SampleBias;\n    @if(MESHLET_MESH_MATERIAL_PASS) {\n        bias.ddx_uv = in.ddx_uv;\n        bias.ddy_uv = in.ddy_uv;\n    }\n    @else {\n        bias.mip_bias = view.mip_bias;\n    }\n\n    // TODO(mbr): control-flow violation: VERTEX_UVS_A must be set when VERTEX_UVS is set.\n    @if(VERTEX_UVS && VERTEX_UVS_A)\n    var uv = (material.uv_transform * vec3(in.uv, 1.0)).xy;\n    @if(VERTEX_UVS && VERTEX_UVS_B)\n    var uv_b = (material.uv_transform * vec3(in.uv_b, 1.0)).xy;\n    @elif(VERTEX_UVS)\n    var uv_b = uv; // TODO(mbr): control-flow violation here\n\n    // TODO: Transforming UVs mean we need to apply derivative chain rule for meshlet mesh material pass\n    @if(VERTEX_UVS && VERTEX_TANGENTS)\n    if ((material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_DEPTH_MAP_BIT) != 0u) {\n        let V = pbr_input.V;\n        let TBN = pbr_functions::calculate_tbn_mikktspace(in.world_normal, in.world_tangent);\n        let T = TBN[0];\n        let B = TBN[1];\n        let N = TBN[2];\n        // Transform V from fragment to camera in world space to tangent space.\n        let Vt = vec3(dot(V, T), dot(V, B), dot(V, N));\n\n        @if(VERTEX_UVS_A) {\n            // TODO: Transforming UVs mean we need to apply derivative chain rule for meshlet mesh material pass\n            uv = parallaxed_uv(\n                material.parallax_depth_scale,\n                material.max_parallax_layer_count,\n                material.max_relief_mapping_search_steps,\n                uv,\n                // Flip the direction of Vt to go toward the surface to make the\n                // parallax mapping algorithm easier to understand and reason\n                // about.\n                -Vt,\n                slot,\n            );\n        }\n\n        @if(VERTEX_UVS_B) {\n            // TODO: Transforming UVs mean we need to apply derivative chain rule for meshlet mesh material pass\n            uv_b = parallaxed_uv(\n                material.parallax_depth_scale,\n                material.max_parallax_layer_count,\n                material.max_relief_mapping_search_steps,\n                uv_b,\n                // Flip the direction of Vt to go toward the surface to make the\n                // parallax mapping algorithm easier to understand and reason\n                // about.\n                -Vt,\n                slot,\n            );\n        }\n        @else { // TODO(mbr) control-flow violation here too\n            uv_b = uv;\n        }\n    }\n\n    @if(VERTEX_UVS)\n    if ((material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_BASE_COLOR_TEXTURE_BIT) != 0u) {\n        // TODO(mbr): see refactor here: https://github.com/wgsl-tooling-wg/wesl-spec/issues/63\n        @if(BINDLESS)\n        let texture = bindless_textures_2d[material_indices[slot].base_color_texture];\n        @else \n        let texture = pbr_bindings::base_color_texture;\n\n        @if(BINDLESS)\n        let sampler_ = bindless_samplers_filtering[material_indices[slot].base_color_sampler];\n        @else\n        let sampler_ = pbr_bindings::base_color_sampler;\n\n        @if(STANDARD_MATERIAL_BASE_COLOR_UV_B)\n        let coords = uv_b;\n        @else\n        let coords = uv;\n\n        @if(MESHLET_MESH_MATERIAL_PASS)\n        let sample = textureSampleGrad(texture, sampler_, coords, bias.ddx_uv, bias.ddy_uv);\n        @else\n        let sample = textureSampleBias(texture, sampler_, coords, bias.mip_bias);\n\n        pbr_input.material.base_color *= sample;\n\n        @if(ALPHA_TO_COVERAGE) {\n            // Sharpen alpha edges.\n            //\n            // https://bgolus.medium.com/anti-aliased-alpha-test-the-esoteric-alpha-to-coverage-8b177335ae4f\n            let alpha_mode = material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_ALPHA_MODE_RESERVED_BITS;\n            if alpha_mode == pbr_types::STANDARD_MATERIAL_FLAGS_ALPHA_MODE_ALPHA_TO_COVERAGE {\n                let alpha = pbr_input.material.base_color.a;\n                pbr_input.material.base_color.a = (alpha - material.alpha_cutoff)\n                    / max(fwidth(alpha), 0.0001) + 0.5;\n            }\n        }\n    }\n\n    // NOTE: Unlit bit not set means == 0 is true, so the true case is if lit\n    if ((material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_UNLIT_BIT) == 0u) {\n        pbr_input.material.ior = material.ior;\n        pbr_input.material.attenuation_color = material.attenuation_color;\n        pbr_input.material.attenuation_distance = material.attenuation_distance;\n        pbr_input.material.alpha_cutoff = material.alpha_cutoff;\n\n        // the following material values can be updated by texture sampling below.\n        pbr_input.material.reflectance = material.reflectance;\n        pbr_input.material.emissive = material.emissive;\n        pbr_input.material.metallic = material.metallic;\n        pbr_input.material.perceptual_roughness = material.perceptual_roughness;\n        pbr_input.material.clearcoat = material.clearcoat;\n        pbr_input.material.clearcoat_perceptual_roughness = material.clearcoat_perceptual_roughness;\n        // TODO(mbr): in the original code in BINDLESS case there was this: \n        // var specular_transmission: f32 = pbr_bindings::material_array[slot].specular_transmission\n        // I suspect this was a bug; should have been:\n        // var specular_transmission: f32 = pbr_bindings::material_array[material_indices[slot].material].specular_transmission\n        pbr_input.material.specular_transmission = material.specular_transmission;\n        pbr_input.material.thickness = material.thickness;\n        pbr_input.material.diffuse_transmission = material.diffuse_transmission;\n        pbr_input.diffuse_occlusion = vec3(1.0);\n        pbr_input.specular_occlusion = 1.0;\n\n        // Specular texture\n        @if(VERTEX_UVS && PBR_SPECULAR_TEXTURES_SUPPORTED)\n        if ((material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_SPECULAR_TEXTURE_BIT) != 0u) {\n            @if(BINDLESS)\n            let texture = bindless_textures_2d[material_indices[slot].specular_texture];\n            @else\n            let texture = pbr_bindings::specular_texture;\n\n            @if(BINDLESS)\n            let sampler_ = bindless_samplers_filtering[material_indices[slot].specular_sampler];\n            @else\n            let sampler_ = pbr_bindings::specular_sampler;\n\n            @if(STANDARD_MATERIAL_SPECULAR_UV_B)\n            let coords = uv_b;\n            @else\n            let coords = uv;\n\n            @if(MESHLET_MESH_MATERIAL_PASS)\n            let sample = textureSampleGrad(texture, sampler_, coords, bias.ddx_uv, bias.ddy_uv).a;\n            @else\n            let sample = textureSampleBias(texture, sampler_, coords, bias.mip_bias).a;\n\n            // This 0.5 factor is from the `KHR_materials_specular` specification:\n            // <https://github.com/KhronosGroup/glTF/tree/main/extensions/2.0/Khronos/KHR_materials_specular#materials-with-reflectance-parameter>\n            pbr_input.material.reflectance *= sample * 0.5;\n        }\n\n        // Specular tint texture\n        @if(VERTEX_UVS && PBR_SPECULAR_TEXTURES_SUPPORTED)\n        if ((material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_SPECULAR_TINT_TEXTURE_BIT) != 0u) {\n            @if(BINDLESS)\n            let texture = bindless_textures_2d[material_indices[slot].specular_tint_texture];\n            @else\n            let texture = pbr_bindings::specular_tint_texture;\n\n            @if(BINDLESS)\n            let sampler_ = bindless_samplers_filtering[material_indices[slot].specular_tint_sampler];\n            @else\n            let sampler_ = pbr_bindings::specular_tint_sampler;\n\n            @if(STANDARD_MATERIAL_SPECULAR_TINT_UV_B)\n            let coords = uv_b;\n            @else\n            let coords = uv;\n\n            @if(MESHLET_MESH_MATERIAL_PASS)\n            let sample = textureSampleGrad(texture, sampler_, coords, bias.ddx_uv, bias.ddy_uv).rgb;\n            @else\n            let sample = textureSampleBias(texture, sampler_, coords, bias.mip_bias).rgb;\n\n            pbr_input.material.reflectance *= sample;\n        }\n\n        // emissive\n        @if(VERTEX_UVS)\n        if ((material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_EMISSIVE_TEXTURE_BIT) != 0u) {\n            @if(BINDLESS)\n            let texture = bindless_textures_2d[material_indices[slot].emissive_texture];\n            @else\n            let texture = pbr_bindings::emissive_texture;\n\n            @if(BINDLESS)\n            let sampler_ = bindless_samplers_filtering[material_indices[slot].emissive_sampler];\n            @else\n            let sampler_ = pbr_bindings::emissive_sampler;\n\n            @if(STANDARD_MATERIAL_EMISSIVE_UV_B)\n            let coords = uv_b;\n            @else\n            let coords = uv;\n\n            @if(MESHLET_MESH_MATERIAL_PASS)\n            let sample = textureSampleGrad(texture, sampler_, coords, bias.ddx_uv, bias.ddy_uv).rgb;\n            @else\n            let sample = textureSampleBias(texture, sampler_, coords, bias.mip_bias).rgb;\n\n            pbr_input.material.emissive = vec4<f32>(\n                material.emissive.rgb * sample,\n                material.emissive.a\n            );\n        }\n\n        // metallic and perceptual roughness\n        let roughness = lighting::perceptualRoughnessToRoughness(material.perceptual_roughness);\n        @if(VERTEX_UVS)\n        if ((material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_METALLIC_ROUGHNESS_TEXTURE_BIT) != 0u) {\n            @if(BINDLESS)\n            let texture = bindless_textures_2d[material_indices[slot].metallic_roughness_texture];\n            @else\n            let texture = pbr_bindings::metallic_roughness_texture;\n\n            @if(BINDLESS)\n            let sampler_ = bindless_samplers_filtering[material_indices[slot].metallic_roughness_sampler];\n            @else\n            let sampler_ = pbr_bindings::metallic_roughness_sampler;\n\n            @if(STANDARD_MATERIAL_METALLIC_ROUGHNESS_UV_B)\n            let coords = uv_b;\n            @else\n            let coords = uv;\n\n            @if(MESHLET_MESH_MATERIAL_PASS)\n            let sample = textureSampleGrad(texture, sampler_, coords, bias.ddx_uv, bias.ddy_uv);\n            @else\n            let sample = textureSampleBias(texture, sampler_, coords, bias.mip_bias);\n\n            // Sampling from GLTF standard channels for now\n            pbr_input.material.metallic *= sample.b;\n            pbr_input.material.perceptual_roughness *= sample.g;\n        }\n\n        // Clearcoat factor\n        @if(VERTEX_UVS && PBR_MULTI_LAYER_MATERIAL_TEXTURES_SUPPORTED)\n        if ((material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_CLEARCOAT_TEXTURE_BIT) != 0u) {\n            @if(BINDLESS)\n            let texture = bindless_textures_2d[material_indices[slot].clearcoat_texture];\n            @else\n            let texture = pbr_bindings::clearcoat_texture;\n\n            @if(BINDLESS)\n            let sampler_ = bindless_samplers_filtering[material_indices[slot].clearcoat_sampler];\n            @else\n            let sampler_ = pbr_bindings::clearcoat_sampler;\n\n            @if(STANDARD_MATERIAL_CLEARCOAT_UV_B)\n            let coords = uv_b;\n            @else\n            let coords = uv;\n\n            @if(MESHLET_MESH_MATERIAL_PASS)\n            let sample = textureSampleGrad(texture, sampler_, coords, bias.ddx_uv, bias.ddy_uv).r;\n            @else\n            let sample = textureSampleBias(texture, sampler_, coords, bias.mip_bias).r;\n\n            pbr_input.material.clearcoat *= sample;\n        }\n\n        // Clearcoat roughness\n        @if(VERTEX_UVS && PBR_MULTI_LAYER_MATERIAL_TEXTURES_SUPPORTED)\n        if ((material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_CLEARCOAT_ROUGHNESS_TEXTURE_BIT) != 0u) {\n            @if(BINDLESS)\n            let texture = bindless_textures_2d[material_indices[slot].clearcoat_roughness_texture];\n            @else\n            let texture = pbr_bindings::clearcoat_roughness_texture;\n\n            @if(BINDLESS)\n            let sampler_ = bindless_samplers_filtering[material_indices[slot].clearcoat_roughness_sampler];\n            @else\n            let sampler_ = pbr_bindings::clearcoat_roughness_sampler;\n\n            @if(STANDARD_MATERIAL_CLEARCOAT_ROUGHNESS_UV_B)\n            let coords = uv_b;\n            @else\n            let coords = uv;\n\n            @if(MESHLET_MESH_MATERIAL_PASS)\n            let sample = textureSampleGrad(texture, sampler_, coords, bias.ddx_uv, bias.ddy_uv).r;\n            @else\n            let sample = textureSampleBias(texture, sampler_, coords, bias.mip_bias).r;\n\n            pbr_input.material.clearcoat_perceptual_roughness *= sample;\n        }\n\n        // Specular transmission\n        @if(VERTEX_UVS && PBR_TRANSMISSION_SUPPORTED)\n        if ((material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_SPECULAR_TRANSMISSION_TEXTURE_BIT) != 0u) {\n            @if(BINDLESS)\n            let texture = bindless_textures_2d[material_indices[slot].specular_transmission_texture];\n            @else\n            let texture = pbr_bindings::specular_transmission_texture;\n\n            @if(BINDLESS)\n            let sampler_ = bindless_samplers_filtering[material_indices[slot].specular_transmission_sampler];\n            @else\n            let sampler_ = pbr_bindings::specular_transmission_sampler;\n\n            @if(STANDARD_MATERIAL_SPECULAR_TRANSMISSION_UV_B)\n            let coords = uv_b;\n            @else\n            let coords = uv;\n\n            @if(MESHLET_MESH_MATERIAL_PASS)\n            let sample = textureSampleGrad(texture, sampler_, coords, bias.ddx_uv, bias.ddy_uv).r;\n            @else\n            let sample = textureSampleBias(texture, sampler_, coords, bias.mip_bias).r;\n\n            pbr_input.material.specular_transmission *= sample;\n        }\n\n        // Thickness\n        @if(VERTEX_UVS && PBR_TRANSMISSION_SUPPORTED)\n        if ((material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_THICKNESS_TEXTURE_BIT) != 0u) {\n            @if(BINDLESS)\n            let texture = bindless_textures_2d[material_indices[slot].thickness_texture];\n            @else\n            let texture = pbr_bindings::thickness_texture;\n\n            @if(BINDLESS)\n            let sampler_ = bindless_samplers_filtering[material_indices[slot].thickness_sampler];\n            @else\n            let sampler_ = pbr_bindings::thickness_sampler;\n\n            @if(STANDARD_MATERIAL_THICKNESS_UV_B)\n            let coords = uv_b;\n            @else\n            let coords = uv;\n\n            @if(MESHLET_MESH_MATERIAL_PASS)\n            let sample = textureSampleGrad(texture, sampler_, coords, bias.ddx_uv, bias.ddy_uv).g;\n            @else\n            let sample = textureSampleBias(texture, sampler_, coords, bias.mip_bias).g;\n\n            pbr_input.material.thickness *= sample;\n        }\n\n        // scale thickness, accounting for non-uniform scaling (e.g. a “squished” mesh)\n        // TODO: Meshlet support\n        @if(!MESHLET_MESH_MATERIAL_PASS) {\n            pbr_input.material.thickness *= length(\n                (transpose(mesh[in.instance_index].world_from_local) * vec4(pbr_input.N, 0.0)).xyz\n            );\n        }\n\n        // Diffuse transmission\n        @if(VERTEX_UVS && PBR_TRANSMISSION_TEXTURES_SUPPORTED)\n        if ((material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_DIFFUSE_TRANSMISSION_TEXTURE_BIT) != 0u) {\n            @if(BINDLESS)\n            let texture = bindless_textures_2d[material_indices[slot].diffuse_transmission_texture];\n            @else\n            let texture = pbr_bindings::diffuse_transmission_texture;\n\n            @if(BINDLESS)\n            let sampler_ = bindless_samplers_filtering[material_indices[slot].diffuse_transmission_sampler];\n            @else\n            let sampler_ = pbr_bindings::diffuse_transmission_sampler;\n\n            @if(STANDARD_MATERIAL_DIFFUSE_TRANSMISSION_UV_B)\n            let coords = uv_b;\n            @else\n            let coords = uv;\n\n            @if(MESHLET_MESH_MATERIAL_PASS)\n            let sample = textureSampleGrad(texture, sampler_, coords, bias.ddx_uv, bias.ddy_uv).a;\n            @else\n            let sample = textureSampleBias(texture, sampler_, coords, bias.mip_bias).a;\n\n            pbr_input.material.diffuse_transmission *= sample;\n        }\n\n\n        // Occlusion\n        @if(VERTEX_UVS)\n        if ((material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_OCCLUSION_TEXTURE_BIT) != 0u) {\n            @if(BINDLESS)\n            let texture = bindless_textures_2d[material_indices[slot].occlusion_texture];\n            @else\n            let texture = pbr_bindings::occlusion_texture;\n\n            @if(BINDLESS)\n            let sampler_ = bindless_samplers_filtering[material_indices[slot].occlusion_sampler];\n            @else\n            let sampler_ = pbr_bindings::occlusion_sampler;\n\n            @if(STANDARD_MATERIAL_OCCLUSION_UV_B)\n            let coords = uv_b;\n            @else\n            let coords = uv;\n\n            @if(MESHLET_MESH_MATERIAL_PASS)\n            let sample = textureSampleGrad(texture, sampler_, coords, bias.ddx_uv, bias.ddy_uv).r;\n            @else\n            let sample = textureSampleBias(texture, sampler_, coords, bias.mip_bias).r;\n\n            pbr_input.diffuse_occlusion *= sample;\n        }\n\n        @if(SCREEN_SPACE_AMBIENT_OCCLUSION) {\n            let ssao = textureLoad(screen_space_ambient_occlusion_texture, vec2<i32>(in.position.xy), 0i).r;\n            let ssao_multibounce = ssao_multibounce(ssao, pbr_input.material.base_color.rgb);\n            pbr_input.diffuse_occlusion = min(pbr_input.diffuse_occlusion, ssao_multibounce);\n            // Use SSAO to estimate the specular occlusion.\n            // Lagarde and Rousiers 2014, \"Moving Frostbite to Physically Based Rendering\"\n            pbr_input.specular_occlusion = saturate(pow(NdotV + ssao, exp2(-16.0 * pbr_input.material.roughness - 1.0)) - 1.0 + ssao);\n        }\n\n        // N (normal vector)\n        @if(!LOAD_PREPASS_NORMALS) {\n\n            pbr_input.N = normalize(pbr_input.world_normal);\n            pbr_input.clearcoat_N = pbr_input.N;\n\n            @if(VERTEX_UVS && VERTEX_TANGENTS) {\n                let TBN = pbr_functions::calculate_tbn_mikktspace(pbr_input.world_normal, in.world_tangent);\n\n                @if(STANDARD_MATERIAL_NORMAL_MAP) {\n                    @if(BINDLESS)\n                    let texture = bindless_textures_2d[material_indices[slot].normal_map_texture];\n                    @else\n                    let texture = pbr_bindings::normal_map_texture;\n\n                    @if(BINDLESS)\n                    let sampler_ = bindless_samplers_filtering[material_indices[slot].normal_map_sampler];\n                    @else\n                    let sampler_ = pbr_bindings::normal_map_sampler;\n\n                    @if(STANDARD_MATERIAL_NORMAL_MAP_UV_B)\n                    let coords = uv_b;\n                    @else\n                    let coords = uv;\n\n                    @if(MESHLET_MESH_MATERIAL_PASS)\n                    let Nt = textureSampleGrad(texture, sampler_, coords, bias.ddx_uv, bias.ddy_uv).rgb;\n                    @else\n                    let Nt = textureSampleBias(texture, sampler_, coords, bias.mip_bias).rgb;\n\n                    pbr_input.N = pbr_functions::apply_normal_mapping(material.flags, TBN, double_sided, is_front, Nt);\n                } // NORMAL_MAP\n                \n                @if(STANDARD_MATERIAL_CLEARCOAT && STANDARD_MATERIAL_CLEARCOAT_NORMAL_MAP) {\n                    // Note: `KHR_materials_clearcoat` specifies that, if there's no\n                    // clearcoat normal map, we must set the normal to the mesh's normal,\n                    // and not to the main layer's bumped normal.\n\n                    @if(BINDLESS)\n                    let texture = bindless_textures_2d[material_indices[slot].clearcoat_normal_texture];\n                    @else\n                    let texture = pbr_bindings::clearcoat_normal_texture;\n\n                    @if(BINDLESS)\n                    let sampler_ = bindless_samplers_filtering[material_indices[slot].clearcoat_normal_sampler];\n                    @else\n                    let sampler_ = pbr_bindings::clearcoat_normal_sampler;\n\n                    @if(STANDARD_MATERIAL_CLEARCOAT_NORMAL_MAP_UV_B)\n                    let coords = uv_b;\n                    @else\n                    let coords = uv;\n\n                    @if(MESHLET_MESH_MATERIAL_PASS)\n                    let clearcoat_Nt = textureSampleGrad(texture, sampler_, coords, bias.ddx_uv, bias.ddy_uv).rgb;\n                    @else\n                    let clearcoat_Nt = textureSampleBias(texture, sampler_, coords, bias.mip_bias).rgb;\n\n                    pbr_input.clearcoat_N = pbr_functions::apply_normal_mapping(\n                        material.flags,\n                        TBN,\n                        double_sided,\n                        is_front,\n                        clearcoat_Nt,\n                    );\n                }\n\n            } // VERTEX_UVS && VERTEX_TANGENTS\n\n            // Take anisotropy into account.\n            //\n            // This code comes from the `KHR_materials_anisotropy` spec:\n            // <https://github.com/KhronosGroup/glTF/blob/main/extensions/2.0/Khronos/KHR_materials_anisotropy/README.md#individual-lights>\n            // Adjust based on the anisotropy map if there is one.\n            @if(PBR_ANISOTROPY_TEXTURE_SUPPORTED && VERTEX_TANGENTS && STANDARD_MATERIAL_ANISOTROPY) {\n                var anisotropy_strength = material.anisotropy_strength;\n                var anisotropy_direction = material.anisotropy_rotation;\n                \n                if ((flags & pbr_types::STANDARD_MATERIAL_FLAGS_ANISOTROPY_TEXTURE_BIT) != 0u) {\n                    @if(BINDLESS)\n                    let texture = bindless_textures_2d[material_indices[slot].anisotropy_texture];\n                    @else\n                    let texture = pbr_bindings::anisotropy_texture;\n\n                    @if(BINDLESS)\n                    let sampler_ = bindless_samplers_filtering[material_indices[slot].anisotropy_sampler];\n                    @else\n                    let sampler_ = pbr_bindings::anisotropy_sampler;\n\n                    @if(STANDARD_MATERIAL_ANISOTROPY_MAP_UV_B)\n                    let coords = uv_b;\n                    @else\n                    let coords = uv;\n\n                    @if(MESHLET_MESH_MATERIAL_PASS)\n                    let anisotropy_texel = textureSampleGrad(texture, sampler_, coords, bias.ddx_uv, bias.ddy_uv).rgb;\n                    @else\n                    let anisotropy_texel = textureSampleBias(texture, sampler_, coords, bias.mip_bias).rgb;\n\n                    let anisotropy_direction_from_texture = normalize(anisotropy_texel.rg * 2.0 - 1.0);\n                    // Rotate by the anisotropy direction.\n                    anisotropy_direction =\n                        mat2x2(anisotropy_direction.xy, anisotropy_direction.yx * vec2(-1.0, 1.0)) *\n                        anisotropy_direction_from_texture;\n                    anisotropy_strength *= anisotropy_texel.b;\n                }\n\n                pbr_input.anisotropy_strength = anisotropy_strength;\n\n                let anisotropy_T = normalize(TBN * vec3(anisotropy_direction, 0.0));\n                let anisotropy_B = normalize(cross(pbr_input.world_normal, anisotropy_T));\n                pbr_input.anisotropy_T = anisotropy_T;\n                pbr_input.anisotropy_B = anisotropy_B;\n            }\n        } // @if(!LOAD_PREPASS_NORMALS)\n\n        // TODO: Meshlet support\n        @if(LIGHTMAP) {\n            pbr_input.lightmap_light = lightmap(in.uv_b, material.lightmap_exposure, in.instance_index);\n        }\n    }\n\n    return pbr_input;\n}\n","bevy::pbr::pbr_functions":"import package::pbr::{\n    pbr_types,\n    pbr_bindings,\n    mesh_view_bindings as view_bindings,\n    mesh_view_types,\n    lighting,\n    lighting::{LAYER_BASE, LAYER_CLEARCOAT},\n    transmission,\n    clustered_forward as clustering,\n    shadows,\n    ambient,\n    irradiance_volume,\n    mesh_types::{MESH_FLAGS_SHADOW_RECEIVER_BIT, MESH_FLAGS_TRANSMITTED_SHADOW_RECEIVER_BIT},\n};\nimport package::render::maths::{E, powsafe};\n\n@if(MESHLET_MESH_MATERIAL_PASS)\nimport package::pbr::meshlet_visibility_buffer_resolve::VertexOutput;\n@elif(PREPASS_PIPELINE)\nimport package::pbr::prepass_io::VertexOutput;\n@else \nimport package::pbr::forward_io::VertexOutput;\n\n@if(ENVIRONMENT_MAP)\nimport package::pbr::environment_map;\n\n@if(TONEMAP_IN_SHADER)\nimport package::core_pipeline::tonemapping::{tone_mapping, screen_space_dither};\n\n\n// Biasing info needed to sample from a texture. How this is done depends on\n// whether we're rendering meshlets or regular meshes.\nstruct SampleBias {\n    @if(MESHLET_MESH_MATERIAL_PASS)\n    ddx_uv: vec2<f32>,\n    @if(MESHLET_MESH_MATERIAL_PASS)\n    ddy_uv: vec2<f32>,\n    @else\n    mip_bias: f32,\n}\n\n// This is the standard 4x4 ordered dithering pattern from [1].\n//\n// We can't use `array<vec4<u32>, 4>` because they can't be indexed dynamically\n// due to Naga limitations. So instead we pack into a single `vec4` and extract\n// individual bytes.\n//\n// [1]: https://en.wikipedia.org/wiki/Ordered_dithering#Threshold_map\nconst DITHER_THRESHOLD_MAP: vec4<u32> = vec4(\n    0x0a020800,\n    0x060e040c,\n    0x09010b03,\n    0x050d070f\n);\n\n// Processes a visibility range dither value and discards the fragment if\n// needed.\n//\n// Visibility ranges, also known as HLODs, are crossfades between different\n// levels of detail.\n//\n// The `dither` value ranges from [-16, 16]. When zooming out, positive values\n// are used for meshes that are in the process of disappearing, while negative\n// values are used for meshes that are in the process of appearing. In other\n// words, when the camera is moving backwards, the `dither` value counts up from\n// -16 to 0 when the object is fading in, stays at 0 while the object is\n// visible, and then counts up to 16 while the object is fading out.\n// Distinguishing between negative and positive values allows the dither\n// patterns for different LOD levels of a single mesh to mesh together properly.\n@if(VISIBILITY_RANGE_DITHER)\nfn visibility_range_dither(frag_coord: vec4<f32>, dither: i32) {\n    // If `dither` is 0, the object is visible.\n    if (dither == 0) {\n        return;\n    }\n\n    // If `dither` is less than -15 or greater than 15, the object is culled.\n    if (dither <= -16 || dither >= 16) {\n        discard;\n    }\n\n    // Otherwise, check the dither pattern.\n    let coords = vec2<u32>(floor(frag_coord.xy)) % 4u;\n    let threshold = i32((DITHER_THRESHOLD_MAP[coords.y] >> (coords.x * 8)) & 0xff);\n    if ((dither >= 0 && dither + threshold >= 16) || (dither < 0 && 1 + dither + threshold <= 0)) {\n        discard;\n    }\n}\n\nfn alpha_discard(material: pbr_types::StandardMaterial, output_color: vec4<f32>) -> vec4<f32> {\n    var color = output_color;\n    let alpha_mode = material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_ALPHA_MODE_RESERVED_BITS;\n    if alpha_mode == pbr_types::STANDARD_MATERIAL_FLAGS_ALPHA_MODE_OPAQUE {\n        // NOTE: If rendering as opaque, alpha should be ignored so set to 1.0\n        color.a = 1.0;\n    }\n\n    else {\n        // NOTE: `MAY_DISCARD` is only defined in the alpha to coverage case if MSAA\n        // was off. This special situation causes alpha to coverage to fall back to\n        // alpha mask.\n        @if(MAY_DISCARD)\n        if alpha_mode == pbr_types::STANDARD_MATERIAL_FLAGS_ALPHA_MODE_MASK ||\n                alpha_mode == pbr_types::STANDARD_MATERIAL_FLAGS_ALPHA_MODE_ALPHA_TO_COVERAGE {\n            if color.a >= material.alpha_cutoff {\n                // NOTE: If rendering as masked alpha and >= the cutoff, render as fully opaque\n                color.a = 1.0;\n            } else {\n                // NOTE: output_color.a < in.material.alpha_cutoff should not be rendered\n                discard;\n            }\n        }\n    }\n\n    return color;\n}\n\nfn prepare_world_normal(\n    world_normal: vec3<f32>,\n    double_sided: bool,\n    is_front: bool,\n) -> vec3<f32> {\n    var output: vec3<f32> = world_normal;\n    @if(!VERTEX_TANGENTS && !STANDARD_MATERIAL_NORMAL_MAP) {\n        // NOTE: When NOT using normal-mapping, if looking at the back face of a double-sided\n        // material, the normal needs to be inverted. This is a branchless version of that.\n        output = (f32(!double_sided || is_front) * 2.0 - 1.0) * output;\n    }\n    return output;\n}\n\n// Calculates the three TBN vectors according to [mikktspace]. Returns a matrix\n// with T, B, N columns in that order.\n//\n// [mikktspace]: http://www.mikktspace.com/\nfn calculate_tbn_mikktspace(world_normal: vec3<f32>, world_tangent: vec4<f32>) -> mat3x3<f32> {\n    // NOTE: The mikktspace method of normal mapping explicitly requires that the world normal NOT\n    // be re-normalized in the fragment shader. This is primarily to match the way mikktspace\n    // bakes vertex tangents and normal maps so that this is the exact inverse. Blender, Unity,\n    // Unreal Engine, Godot, and more all use the mikktspace method. Do not change this code\n    // unless you really know what you are doing.\n    // http://www.mikktspace.com/\n    var N: vec3<f32> = world_normal;\n\n    // NOTE: The mikktspace method of normal mapping explicitly requires that these NOT be\n    // normalized nor any Gram-Schmidt applied to ensure the vertex normal is orthogonal to the\n    // vertex tangent! Do not change this code unless you really know what you are doing.\n    // http://www.mikktspace.com/\n    var T: vec3<f32> = world_tangent.xyz;\n    var B: vec3<f32> = world_tangent.w * cross(N, T);\n\n    @if(MESHLET_MESH_MATERIAL_PASS) {\n        // https://www.jeremyong.com/graphics/2023/12/16/surface-gradient-bump-mapping/#a-note-on-mikktspace-usage\n        let inverse_length_n = 1.0 / length(N);\n        T *= inverse_length_n;\n        B *= inverse_length_n;\n        N *= inverse_length_n;\n    }\n\n    return mat3x3(T, B, N);\n}\n\nfn apply_normal_mapping(\n    standard_material_flags: u32,\n    TBN: mat3x3<f32>,\n    double_sided: bool,\n    is_front: bool,\n    in_Nt: vec3<f32>,\n) -> vec3<f32> {\n    // Unpack the TBN vectors.\n    var T = TBN[0];\n    var B = TBN[1];\n    var N = TBN[2];\n\n    // Nt is the tangent-space normal.\n    var Nt = in_Nt;\n    if (standard_material_flags & pbr_types::STANDARD_MATERIAL_FLAGS_TWO_COMPONENT_NORMAL_MAP) != 0u {\n        // Only use the xy components and derive z for 2-component normal maps.\n        Nt = vec3<f32>(Nt.rg * 2.0 - 1.0, 0.0);\n        Nt.z = sqrt(1.0 - Nt.x * Nt.x - Nt.y * Nt.y);\n    } else {\n        Nt = Nt * 2.0 - 1.0;\n    }\n    // Normal maps authored for DirectX require flipping the y component\n    if (standard_material_flags & pbr_types::STANDARD_MATERIAL_FLAGS_FLIP_NORMAL_MAP_Y) != 0u {\n        Nt.y = -Nt.y;\n    }\n\n    if double_sided && !is_front {\n        Nt = -Nt;\n    }\n\n    // NOTE: The mikktspace method of normal mapping applies maps the tangent-space normal from\n    // the normal map texture in this way to be an EXACT inverse of how the normal map baker\n    // calculates the normal maps so there is no error introduced. Do not change this code\n    // unless you really know what you are doing.\n    // http://www.mikktspace.com/\n    N = Nt.x * T + Nt.y * B + Nt.z * N;\n\n    return normalize(N);\n}\n\n// Modifies the normal to achieve a better approximate direction from the\n// environment map when using anisotropy.\n//\n// This follows the suggested implementation in the `KHR_materials_anisotropy` specification:\n// https://github.com/KhronosGroup/glTF/blob/main/extensions/2.0/Khronos/KHR_materials_anisotropy/README.md#image-based-lighting\n@if(STANDARD_MATERIAL_ANISOTROPY)\nfn bend_normal_for_anisotropy(lighting_input: ptr<function, lighting::LightingInput>) {\n    // Unpack.\n    let N = (*lighting_input).layers[LAYER_BASE].N;\n    let roughness = (*lighting_input).layers[LAYER_BASE].roughness;\n    let V = (*lighting_input).V;\n    let anisotropy = (*lighting_input).anisotropy;\n    let Ba = (*lighting_input).Ba;\n\n    var bent_normal = normalize(cross(cross(Ba, V), Ba));\n\n    // The `KHR_materials_anisotropy` spec states:\n    //\n    // > This heuristic can probably be improved upon\n    let a = pow(2.0, pow(2.0, 1.0 - anisotropy * (1.0 - roughness)));\n    bent_normal = normalize(mix(bent_normal, N, a));\n\n    // The `KHR_materials_anisotropy` spec states:\n    //\n    // > Mixing the reflection with the normal is more accurate both with and\n    // > without anisotropy and keeps rough objects from gathering light from\n    // > behind their tangent plane.\n    let R = normalize(mix(reflect(-V, bent_normal), bent_normal, roughness * roughness));\n\n    (*lighting_input).layers[LAYER_BASE].N = bent_normal;\n    (*lighting_input).layers[LAYER_BASE].R = R;\n}\n\n// NOTE: Correctly calculates the view vector depending on whether\n// the projection is orthographic or perspective.\nfn calculate_view(\n    world_position: vec4<f32>,\n    is_orthographic: bool,\n) -> vec3<f32> {\n    var V: vec3<f32>;\n    if is_orthographic {\n        // Orthographic view vector\n        V = normalize(vec3<f32>(view_bindings::view.clip_from_world[0].z, view_bindings::view.clip_from_world[1].z, view_bindings::view.clip_from_world[2].z));\n    } else {\n        // Only valid for a perspective projection\n        V = normalize(view_bindings::view.world_position.xyz - world_position.xyz);\n    }\n    return V;\n}\n\n// Diffuse strength is inversely related to metallicity, specular and diffuse transmission\nfn calculate_diffuse_color(\n    base_color: vec3<f32>,\n    metallic: f32,\n    specular_transmission: f32,\n    diffuse_transmission: f32\n) -> vec3<f32> {\n    return base_color * (1.0 - metallic) * (1.0 - specular_transmission) *\n        (1.0 - diffuse_transmission);\n}\n\n// Remapping [0,1] reflectance to F0\n// See https://google.github.io/filament/Filament.html#materialsystem/parameterization/remapping\nfn calculate_F0(base_color: vec3<f32>, metallic: f32, reflectance: vec3<f32>) -> vec3<f32> {\n    return 0.16 * reflectance * reflectance * (1.0 - metallic) + base_color * metallic;\n}\n\n@if(!PREPASS_FRAGMENT)\nfn apply_pbr_lighting(\n    in: pbr_types::PbrInput,\n) -> vec4<f32> {\n    var output_color: vec4<f32> = in.material.base_color;\n\n    let emissive = in.material.emissive;\n\n    // calculate non-linear roughness from linear perceptualRoughness\n    let metallic = in.material.metallic;\n    let perceptual_roughness = in.material.perceptual_roughness;\n    let roughness = lighting::perceptualRoughnessToRoughness(perceptual_roughness);\n    let ior = in.material.ior;\n    let thickness = in.material.thickness;\n    let reflectance = in.material.reflectance;\n    let diffuse_transmission = in.material.diffuse_transmission;\n    let specular_transmission = in.material.specular_transmission;\n\n    let specular_transmissive_color = specular_transmission * in.material.base_color.rgb;\n\n    let diffuse_occlusion = in.diffuse_occlusion;\n    let specular_occlusion = in.specular_occlusion;\n\n    // Neubelt and Pettineo 2013, \"Crafting a Next-gen Material Pipeline for The Order: 1886\"\n    let NdotV = max(dot(in.N, in.V), 0.0001);\n    let R = reflect(-in.V, in.N);\n\n    // Do the above calculations again for the clearcoat layer. Remember that\n    // the clearcoat can have its own roughness and its own normal.\n    // TODO(mbr): this is not ideal\n    @if(STANDARD_MATERIAL_CLEARCOAT)\n    let clearcoat = in.material.clearcoat;\n    @if(STANDARD_MATERIAL_CLEARCOAT)\n    let clearcoat_perceptual_roughness = in.material.clearcoat_perceptual_roughness;\n    @if(STANDARD_MATERIAL_CLEARCOAT)\n    let clearcoat_roughness = lighting::perceptualRoughnessToRoughness(clearcoat_perceptual_roughness);\n    @if(STANDARD_MATERIAL_CLEARCOAT)\n    let clearcoat_N = in.clearcoat_N;\n    @if(STANDARD_MATERIAL_CLEARCOAT)\n    let clearcoat_NdotV = max(dot(clearcoat_N, in.V), 0.0001);\n    @if(STANDARD_MATERIAL_CLEARCOAT)\n    let clearcoat_R = reflect(-in.V, clearcoat_N);\n\n    let diffuse_color = calculate_diffuse_color(\n        output_color.rgb,\n        metallic,\n        specular_transmission,\n        diffuse_transmission\n    );\n\n    // Diffuse transmissive strength is inversely related to metallicity and specular transmission, but directly related to diffuse transmission\n    let diffuse_transmissive_color = output_color.rgb * (1.0 - metallic) * (1.0 - specular_transmission) * diffuse_transmission;\n\n    // Calculate the world position of the second Lambertian lobe used for diffuse transmission, by subtracting material thickness\n    let diffuse_transmissive_lobe_world_position = in.world_position - vec4<f32>(in.world_normal, 0.0) * thickness;\n\n    let F0 = calculate_F0(output_color.rgb, metallic, reflectance);\n    let F_ab = lighting::F_AB(perceptual_roughness, NdotV);\n\n    var direct_light: vec3<f32> = vec3<f32>(0.0);\n\n    // Transmitted Light (Specular and Diffuse)\n    var transmitted_light: vec3<f32> = vec3<f32>(0.0);\n\n    // Pack all the values into a structure.\n    var lighting_input: lighting::LightingInput;\n    lighting_input.layers[LAYER_BASE].NdotV = NdotV;\n    lighting_input.layers[LAYER_BASE].N = in.N;\n    lighting_input.layers[LAYER_BASE].R = R;\n    lighting_input.layers[LAYER_BASE].perceptual_roughness = perceptual_roughness;\n    lighting_input.layers[LAYER_BASE].roughness = roughness;\n    lighting_input.P = in.world_position.xyz;\n    lighting_input.V = in.V;\n    lighting_input.diffuse_color = diffuse_color;\n    lighting_input.F0_ = F0;\n    lighting_input.F_ab = F_ab;\n    @if(STANDARD_MATERIAL_CLEARCOAT) {\n        lighting_input.layers[LAYER_CLEARCOAT].NdotV = clearcoat_NdotV;\n        lighting_input.layers[LAYER_CLEARCOAT].N = clearcoat_N;\n        lighting_input.layers[LAYER_CLEARCOAT].R = clearcoat_R;\n        lighting_input.layers[LAYER_CLEARCOAT].perceptual_roughness = clearcoat_perceptual_roughness;\n        lighting_input.layers[LAYER_CLEARCOAT].roughness = clearcoat_roughness;\n        lighting_input.clearcoat_strength = clearcoat;\n    }\n    @if(STANDARD_MATERIAL_ANISOTROPY) {\n        lighting_input.anisotropy = in.anisotropy_strength;\n        lighting_input.Ta = in.anisotropy_T;\n        lighting_input.Ba = in.anisotropy_B;\n    }\n\n    // And do the same for transmissive if we need to.\n    @if(STANDARD_MATERIAL_DIFFUSE_TRANSMISSION)\n    var transmissive_lighting_input: lighting::LightingInput;\n    @if(STANDARD_MATERIAL_DIFFUSE_TRANSMISSION) {\n        transmissive_lighting_input.layers[LAYER_BASE].NdotV = 1.0;\n        transmissive_lighting_input.layers[LAYER_BASE].N = -in.N;\n        transmissive_lighting_input.layers[LAYER_BASE].R = vec3(0.0);\n        transmissive_lighting_input.layers[LAYER_BASE].perceptual_roughness = 1.0;\n        transmissive_lighting_input.layers[LAYER_BASE].roughness = 1.0;\n        transmissive_lighting_input.P = diffuse_transmissive_lobe_world_position.xyz;\n        transmissive_lighting_input.V = -in.V;\n        transmissive_lighting_input.diffuse_color = diffuse_transmissive_color;\n        transmissive_lighting_input.F0_ = vec3(0.0);\n        transmissive_lighting_input.F_ab = vec2(0.1);\n        \n        @if(STANDARD_MATERIAL_CLEARCOAT) {\n            transmissive_lighting_input.layers[LAYER_CLEARCOAT].NdotV = 0.0;\n            transmissive_lighting_input.layers[LAYER_CLEARCOAT].N = vec3(0.0);\n            transmissive_lighting_input.layers[LAYER_CLEARCOAT].R = vec3(0.0);\n            transmissive_lighting_input.layers[LAYER_CLEARCOAT].perceptual_roughness = 0.0;\n            transmissive_lighting_input.layers[LAYER_CLEARCOAT].roughness = 0.0;\n            transmissive_lighting_input.clearcoat_strength = 0.0;\n        }\n        @if(STANDARD_MATERIAL_ANISOTROPY) {\n            transmissive_lighting_input.anisotropy = in.anisotropy_strength;\n            transmissive_lighting_input.Ta = in.anisotropy_T;\n            transmissive_lighting_input.Ba = in.anisotropy_B;\n        }\n    }\n\n    let view_z = dot(vec4<f32>(\n        view_bindings::view.view_from_world[0].z,\n        view_bindings::view.view_from_world[1].z,\n        view_bindings::view.view_from_world[2].z,\n        view_bindings::view.view_from_world[3].z\n    ), in.world_position);\n    let cluster_index = clustering::fragment_cluster_index(in.frag_coord.xy, view_z, in.is_orthographic);\n    var clusterable_object_index_ranges =\n        clustering::unpack_clusterable_object_index_ranges(cluster_index);\n\n    // Point lights (direct)\n    for (var i: u32 = clusterable_object_index_ranges.first_point_light_index_offset;\n            i < clusterable_object_index_ranges.first_spot_light_index_offset;\n            i = i + 1u) {\n        let light_id = clustering::get_clusterable_object_id(i);\n\n        // If we're lightmapped, disable diffuse contribution from the light if\n        // requested, to avoid double-counting light.\n        @if(LIGHTMAP)\n        let enable_diffuse =\n            (view_bindings::clusterable_objects.data[light_id].flags &\n                mesh_view_types::POINT_LIGHT_FLAGS_AFFECTS_LIGHTMAPPED_MESH_DIFFUSE_BIT) != 0u;\n        @else\n        let enable_diffuse = true;\n\n        var shadow: f32 = 1.0;\n        if ((in.flags & MESH_FLAGS_SHADOW_RECEIVER_BIT) != 0u\n                && (view_bindings::clusterable_objects.data[light_id].flags & mesh_view_types::POINT_LIGHT_FLAGS_SHADOWS_ENABLED_BIT) != 0u) {\n            shadow = shadows::fetch_point_shadow(light_id, in.world_position, in.world_normal);\n        }\n\n        let light_contrib = lighting::point_light(light_id, &lighting_input, enable_diffuse);\n        direct_light += light_contrib * shadow;\n\n        @if(STANDARD_MATERIAL_DIFFUSE_TRANSMISSION) {\n            // NOTE: We use the diffuse transmissive color, the second Lambertian lobe's calculated\n            // world position, inverted normal and view vectors, and the following simplified\n            // values for a fully diffuse transmitted light contribution approximation:\n            //\n            // roughness = 1.0;\n            // NdotV = 1.0;\n            // R = vec3<f32>(0.0) // doesn't really matter\n            // F_ab = vec2<f32>(0.1)\n            // F0 = vec3<f32>(0.0)\n            var transmitted_shadow: f32 = 1.0;\n            if ((in.flags & (MESH_FLAGS_SHADOW_RECEIVER_BIT | MESH_FLAGS_TRANSMITTED_SHADOW_RECEIVER_BIT)) == (MESH_FLAGS_SHADOW_RECEIVER_BIT | MESH_FLAGS_TRANSMITTED_SHADOW_RECEIVER_BIT)\n                    && (view_bindings::clusterable_objects.data[light_id].flags & mesh_view_types::POINT_LIGHT_FLAGS_SHADOWS_ENABLED_BIT) != 0u) {\n                transmitted_shadow = shadows::fetch_point_shadow(light_id, diffuse_transmissive_lobe_world_position, -in.world_normal);\n            }\n\n            let transmitted_light_contrib =\n                lighting::point_light(light_id, &transmissive_lighting_input, enable_diffuse);\n            transmitted_light += transmitted_light_contrib * transmitted_shadow;\n        }\n    }\n\n    // Spot lights (direct)\n    for (var i: u32 = clusterable_object_index_ranges.first_spot_light_index_offset;\n            i < clusterable_object_index_ranges.first_reflection_probe_index_offset;\n            i = i + 1u) {\n        let light_id = clustering::get_clusterable_object_id(i);\n\n        // If we're lightmapped, disable diffuse contribution from the light if\n        // requested, to avoid double-counting light.\n        @if(LIGHTMAP)\n        let enable_diffuse =\n            (view_bindings::clusterable_objects.data[light_id].flags &\n                mesh_view_types::POINT_LIGHT_FLAGS_AFFECTS_LIGHTMAPPED_MESH_DIFFUSE_BIT) != 0u;\n        @else\n        let enable_diffuse = true;\n\n        var shadow: f32 = 1.0;\n        if ((in.flags & MESH_FLAGS_SHADOW_RECEIVER_BIT) != 0u\n                && (view_bindings::clusterable_objects.data[light_id].flags &\n                    mesh_view_types::POINT_LIGHT_FLAGS_SHADOWS_ENABLED_BIT) != 0u) {\n            shadow = shadows::fetch_spot_shadow(\n                light_id,\n                in.world_position,\n                in.world_normal,\n                view_bindings::clusterable_objects.data[light_id].shadow_map_near_z,\n            );\n        }\n\n        let light_contrib = lighting::spot_light(light_id, &lighting_input, enable_diffuse);\n        direct_light += light_contrib * shadow;\n\n        @if(STANDARD_MATERIAL_DIFFUSE_TRANSMISSION) {\n            // NOTE: We use the diffuse transmissive color, the second Lambertian lobe's calculated\n            // world position, inverted normal and view vectors, and the following simplified\n            // values for a fully diffuse transmitted light contribution approximation:\n            //\n            // roughness = 1.0;\n            // NdotV = 1.0;\n            // R = vec3<f32>(0.0) // doesn't really matter\n            // F_ab = vec2<f32>(0.1)\n            // F0 = vec3<f32>(0.0)\n            var transmitted_shadow: f32 = 1.0;\n            if ((in.flags & (MESH_FLAGS_SHADOW_RECEIVER_BIT | MESH_FLAGS_TRANSMITTED_SHADOW_RECEIVER_BIT)) == (MESH_FLAGS_SHADOW_RECEIVER_BIT | MESH_FLAGS_TRANSMITTED_SHADOW_RECEIVER_BIT)\n                    && (view_bindings::clusterable_objects.data[light_id].flags & mesh_view_types::POINT_LIGHT_FLAGS_SHADOWS_ENABLED_BIT) != 0u) {\n                transmitted_shadow = shadows::fetch_spot_shadow(\n                    light_id,\n                    diffuse_transmissive_lobe_world_position,\n                    -in.world_normal,\n                    view_bindings::clusterable_objects.data[light_id].shadow_map_near_z,\n                );\n            }\n\n            let transmitted_light_contrib =\n                lighting::spot_light(light_id, &transmissive_lighting_input, enable_diffuse);\n            transmitted_light += transmitted_light_contrib * transmitted_shadow;\n        }\n    }\n\n    // directional lights (direct)\n    let n_directional_lights = view_bindings::lights.n_directional_lights;\n    for (var i: u32 = 0u; i < n_directional_lights; i = i + 1u) {\n        // check if this light should be skipped, which occurs if this light does not intersect with the view\n        // note point and spot lights aren't skippable, as the relevant lights are filtered in `assign_lights_to_clusters`\n        let light = &view_bindings::lights.directional_lights[i];\n        if (*light).skip != 0u {\n            continue;\n        }\n\n        // If we're lightmapped, disable diffuse contribution from the light if\n        // requested, to avoid double-counting light.\n        @if(LIGHTMAP)\n        let enable_diffuse =\n            ((*light).flags &\n                mesh_view_types::DIRECTIONAL_LIGHT_FLAGS_AFFECTS_LIGHTMAPPED_MESH_DIFFUSE_BIT) !=\n                0u;\n        @else\n        let enable_diffuse = true;\n\n        var shadow: f32 = 1.0;\n        if ((in.flags & MESH_FLAGS_SHADOW_RECEIVER_BIT) != 0u\n                && (view_bindings::lights.directional_lights[i].flags & mesh_view_types::DIRECTIONAL_LIGHT_FLAGS_SHADOWS_ENABLED_BIT) != 0u) {\n            shadow = shadows::fetch_directional_shadow(i, in.world_position, in.world_normal, view_z);\n        }\n\n        var light_contrib = lighting::directional_light(i, &lighting_input, enable_diffuse);\n\n        @if(DIRECTIONAL_LIGHT_SHADOW_MAP_DEBUG_CASCADES) {\n            light_contrib = shadows::cascade_debug_visualization(light_contrib, i, view_z);\n        }\n        direct_light += light_contrib * shadow;\n\n        @if(STANDARD_MATERIAL_DIFFUSE_TRANSMISSION) {\n            // NOTE: We use the diffuse transmissive color, the second Lambertian lobe's calculated\n            // world position, inverted normal and view vectors, and the following simplified\n            // values for a fully diffuse transmitted light contribution approximation:\n            //\n            // roughness = 1.0;\n            // NdotV = 1.0;\n            // R = vec3<f32>(0.0) // doesn't really matter\n            // F_ab = vec2<f32>(0.1)\n            // F0 = vec3<f32>(0.0)\n            var transmitted_shadow: f32 = 1.0;\n            if ((in.flags & (MESH_FLAGS_SHADOW_RECEIVER_BIT | MESH_FLAGS_TRANSMITTED_SHADOW_RECEIVER_BIT)) == (MESH_FLAGS_SHADOW_RECEIVER_BIT | MESH_FLAGS_TRANSMITTED_SHADOW_RECEIVER_BIT)\n                    && (view_bindings::lights.directional_lights[i].flags & mesh_view_types::DIRECTIONAL_LIGHT_FLAGS_SHADOWS_ENABLED_BIT) != 0u) {\n                transmitted_shadow = shadows::fetch_directional_shadow(i, diffuse_transmissive_lobe_world_position, -in.world_normal, view_z);\n            }\n\n            let transmitted_light_contrib =\n                lighting::directional_light(i, &transmissive_lighting_input, enable_diffuse);\n            transmitted_light += transmitted_light_contrib * transmitted_shadow;\n        }\n    }\n\n    @if(STANDARD_MATERIAL_DIFFUSE_TRANSMISSION) {\n        // NOTE: We use the diffuse transmissive color, the second Lambertian lobe's calculated\n        // world position, inverted normal and view vectors, and the following simplified\n        // values for a fully diffuse transmitted light contribution approximation:\n        //\n        // perceptual_roughness = 1.0;\n        // NdotV = 1.0;\n        // F0 = vec3<f32>(0.0)\n        // diffuse_occlusion = vec3<f32>(1.0)\n        transmitted_light += ambient::ambient_light(diffuse_transmissive_lobe_world_position, -in.N, -in.V, 1.0, diffuse_transmissive_color, vec3<f32>(0.0), 1.0, vec3<f32>(1.0));\n    }\n\n    // Diffuse indirect lighting can come from a variety of sources. The\n    // priority goes like this:\n    //\n    // 1. Lightmap (highest)\n    // 2. Irradiance volume\n    // 3. Environment map (lowest)\n    //\n    // When we find a source of diffuse indirect lighting, we stop accumulating\n    // any more diffuse indirect light. This avoids double-counting if, for\n    // example, both lightmaps and irradiance volumes are present.\n\n    var indirect_light = vec3(0.0f);\n    var found_diffuse_indirect = false;\n\n    @if(LIGHTMAP) {\n        indirect_light += in.lightmap_light * diffuse_color;\n        found_diffuse_indirect = true;\n    }\n\n    // Irradiance volume light (indirect)\n    @if(IRRADIANCE_VOLUME)\n    if (!found_diffuse_indirect) {\n        let irradiance_volume_light = irradiance_volume::irradiance_volume_light(\n            in.world_position.xyz,\n            in.N,\n            &clusterable_object_index_ranges,\n        );\n        indirect_light += irradiance_volume_light * diffuse_color * diffuse_occlusion;\n        found_diffuse_indirect = true;\n    }\n\n    // Environment map light (indirect)\n    @if(ENVIRONMENT_MAP) {\n        @if(STANDARD_MATERIAL_ANISOTROPY)\n        var bent_normal_lighting_input = lighting_input;\n        @if(STANDARD_MATERIAL_ANISOTROPY)\n        bend_normal_for_anisotropy(&bent_normal_lighting_input);\n        @if(STANDARD_MATERIAL_ANISOTROPY)\n        let environment_map_lighting_input = &bent_normal_lighting_input;\n        @else\n        let environment_map_lighting_input = &lighting_input;\n\n        let environment_light = environment_map::environment_map_light(\n            environment_map_lighting_input,\n            &clusterable_object_index_ranges,\n            found_diffuse_indirect,\n        );\n\n        // If screen space reflections are going to be used for this material, don't\n        // accumulate environment map light yet. The SSR shader will do it.\n        @if(SCREEN_SPACE_REFLECTIONS)\n        let use_ssr = perceptual_roughness <=\n            view_bindings::ssr_settings.perceptual_roughness_threshold;\n        @else\n        let use_ssr = false;\n\n        if (!use_ssr) {\n            let environment_light = environment_map::environment_map_light(\n                &lighting_input,\n                &clusterable_object_index_ranges,\n                found_diffuse_indirect\n            );\n\n            indirect_light += environment_light.diffuse * diffuse_occlusion +\n                environment_light.specular * specular_occlusion;\n        }\n    }\n\n    // Ambient light (indirect)\n    indirect_light += ambient::ambient_light(in.world_position, in.N, in.V, NdotV, diffuse_color, F0, perceptual_roughness, diffuse_occlusion);\n\n    // we'll use the specular component of the transmitted environment\n    // light in the call to `specular_transmissive_light()` below\n    var specular_transmitted_environment_light = vec3<f32>(0.0);\n\n    @if(ENVIRONMENT_MAP) {\n        @if(STANDARD_MATERIAL_DIFFUSE_OR_SPECULAR_TRANSMISSION) {\n            // NOTE: We use the diffuse transmissive color, inverted normal and view vectors,\n            // and the following simplified values for the transmitted environment light contribution\n            // approximation:\n            //\n            // diffuse_color = vec3<f32>(1.0) // later we use `diffuse_transmissive_color` and `specular_transmissive_color`\n            // NdotV = 1.0;\n            // R = T // see definition below\n            // F0 = vec3<f32>(1.0)\n            // diffuse_occlusion = 1.0\n            //\n            // (This one is slightly different from the other light types above, because the environment\n            // map light returns both diffuse and specular components separately, and we want to use both)\n\n            let T = -normalize(\n                in.V + // start with view vector at entry point\n                refract(in.V, -in.N, 1.0 / ior) * thickness // add refracted vector scaled by thickness, towards exit point\n            ); // normalize to find exit point view vector\n\n            var transmissive_environment_light_input: lighting::LightingInput;\n            transmissive_environment_light_input.diffuse_color = vec3(1.0);\n            transmissive_environment_light_input.layers[LAYER_BASE].NdotV = 1.0;\n            transmissive_environment_light_input.P = in.world_position.xyz;\n            transmissive_environment_light_input.layers[LAYER_BASE].N = -in.N;\n            transmissive_environment_light_input.V = in.V;\n            transmissive_environment_light_input.layers[LAYER_BASE].R = T;\n            transmissive_environment_light_input.layers[LAYER_BASE].perceptual_roughness = perceptual_roughness;\n            transmissive_environment_light_input.layers[LAYER_BASE].roughness = roughness;\n            transmissive_environment_light_input.F0_ = vec3<f32>(1.0);\n            transmissive_environment_light_input.F_ab = vec2(0.1);\n            @if(STANDARD_MATERIAL_CLEARCOAT) {\n                // No clearcoat.\n                transmissive_environment_light_input.clearcoat_strength = 0.0;\n                transmissive_environment_light_input.layers[LAYER_CLEARCOAT].NdotV = 0.0;\n                transmissive_environment_light_input.layers[LAYER_CLEARCOAT].N = in.N;\n                transmissive_environment_light_input.layers[LAYER_CLEARCOAT].R = vec3(0.0);\n                transmissive_environment_light_input.layers[LAYER_CLEARCOAT].perceptual_roughness = 0.0;\n                transmissive_environment_light_input.layers[LAYER_CLEARCOAT].roughness = 0.0;\n            }\n\n            let transmitted_environment_light = environment_map::environment_map_light(\n                &transmissive_environment_light_input,\n                &clusterable_object_index_ranges,\n                false,\n            );\n\n            @if(STANDARD_MATERIAL_DIFFUSE_TRANSMISSION) {\n                transmitted_light += transmitted_environment_light.diffuse * diffuse_transmissive_color;\n            }\n            @if(STANDARD_MATERIAL_SPECULAR_TRANSMISSION) {\n                specular_transmitted_environment_light = transmitted_environment_light.specular * specular_transmissive_color;\n            }\n        }\n    } // ENVIRONMENT_MAP\n\n    var emissive_light = emissive.rgb * output_color.a;\n\n    // \"The clearcoat layer is on top of emission in the layering stack.\n    // Consequently, the emission is darkened by the Fresnel term.\"\n    //\n    // <https://github.com/KhronosGroup/glTF/blob/main/extensions/2.0/Khronos/KHR_materials_clearcoat/README.md#emission>\n    @if(STANDARD_MATERIAL_CLEARCOAT) {\n        emissive_light = emissive_light * (0.04 + (1.0 - 0.04) * pow(1.0 - clearcoat_NdotV, 5.0));\n    }\n\n    emissive_light = emissive_light * mix(1.0, view_bindings::view.exposure, emissive.a);\n\n    @if(STANDARD_MATERIAL_SPECULAR_TRANSMISSION) {\n        transmitted_light += transmission::specular_transmissive_light(in.world_position, in.frag_coord.xyz, view_z, in.N, in.V, F0, ior, thickness, perceptual_roughness, specular_transmissive_color, specular_transmitted_environment_light).rgb;\n\n        if (in.material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_ATTENUATION_ENABLED_BIT) != 0u {\n            // We reuse the `atmospheric_fog()` function here, as it's fundamentally\n            // equivalent to the attenuation that takes place inside the material volume,\n            // and will allow us to eventually hook up subsurface scattering more easily\n            var attenuation_fog: mesh_view_types::Fog;\n            attenuation_fog.base_color.a = 1.0;\n            attenuation_fog.be = pow(1.0 - in.material.attenuation_color.rgb, vec3<f32>(E)) / in.material.attenuation_distance;\n            // TODO: Add the subsurface scattering factor below\n            // attenuation_fog.bi = /* ... */\n            transmitted_light = package::pbr::fog::atmospheric_fog(\n                attenuation_fog, vec4<f32>(transmitted_light, 1.0), thickness,\n                vec3<f32>(0.0) // TODO: Pass in (pre-attenuated) scattered light contribution here\n            ).rgb;\n        }\n    }\n\n    // Total light\n    output_color = vec4<f32>(\n        (view_bindings::view.exposure * (transmitted_light + direct_light + indirect_light)) + emissive_light,\n        output_color.a\n    );\n\n    output_color = clustering::cluster_debug_visualization(\n        output_color,\n        view_z,\n        in.is_orthographic,\n        clusterable_object_index_ranges,\n        cluster_index,\n    );\n\n    return output_color;\n}\n\n@if(DISTANCE_FOG)\nfn apply_fog(fog_params: mesh_view_types::Fog, input_color: vec4<f32>, fragment_world_position: vec3<f32>, view_world_position: vec3<f32>) -> vec4<f32> {\n    let view_to_world = fragment_world_position.xyz - view_world_position.xyz;\n\n    // `length()` is used here instead of just `view_to_world.z` since that produces more\n    // high quality results, especially for denser/smaller fogs. we get a \"curved\"\n    // fog shape that remains consistent with camera rotation, instead of a \"linear\"\n    // fog shape that looks a bit fake\n    let distance = length(view_to_world);\n\n    var scattering = vec3<f32>(0.0);\n    if fog_params.directional_light_color.a > 0.0 {\n        let view_to_world_normalized = view_to_world / distance;\n        let n_directional_lights = view_bindings::lights.n_directional_lights;\n        for (var i: u32 = 0u; i < n_directional_lights; i = i + 1u) {\n            let light = view_bindings::lights.directional_lights[i];\n            scattering += pow(\n                max(\n                    dot(view_to_world_normalized, light.direction_to_light),\n                    0.0\n                ),\n                fog_params.directional_light_exponent\n            ) * light.color.rgb * view_bindings::view.exposure;\n        }\n    }\n\n    if fog_params.mode == mesh_view_types::FOG_MODE_LINEAR {\n        return package::pbr::fog::linear_fog(fog_params, input_color, distance, scattering);\n    } else if fog_params.mode == mesh_view_types::FOG_MODE_EXPONENTIAL {\n        return package::pbr::fog::exponential_fog(fog_params, input_color, distance, scattering);\n    } else if fog_params.mode == mesh_view_types::FOG_MODE_EXPONENTIAL_SQUARED {\n        return package::pbr::fog::exponential_squared_fog(fog_params, input_color, distance, scattering);\n    } else if fog_params.mode == mesh_view_types::FOG_MODE_ATMOSPHERIC {\n        return package::pbr::fog::atmospheric_fog(fog_params, input_color, distance, scattering);\n    } else {\n        return input_color;\n    }\n}\n\n@if(PREMULTIPLY_ALPHA)\nfn premultiply_alpha(standard_material_flags: u32, color: vec4<f32>) -> vec4<f32> {\n    // `Blend`, `Premultiplied` and `Alpha` all share the same `BlendState`. Depending\n    // on the alpha mode, we premultiply the color channels by the alpha channel value,\n    // (and also optionally replace the alpha value with 0.0) so that the result produces\n    // the desired blend mode when sent to the blending operation.\n    @if(BLEND_PREMULTIPLIED_ALPHA) {\n        // For `BlendState::PREMULTIPLIED_ALPHA_BLENDING` the blend function is:\n        //\n        //     result = 1 * src_color + (1 - src_alpha) * dst_color\n        let alpha_mode = standard_material_flags & pbr_types::STANDARD_MATERIAL_FLAGS_ALPHA_MODE_RESERVED_BITS;\n        if alpha_mode == pbr_types::STANDARD_MATERIAL_FLAGS_ALPHA_MODE_ADD {\n            // Here, we premultiply `src_color` by `src_alpha`, and replace `src_alpha` with 0.0:\n            //\n            //     src_color *= src_alpha\n            //     src_alpha = 0.0\n            //\n            // We end up with:\n            //\n            //     result = 1 * (src_alpha * src_color) + (1 - 0) * dst_color\n            //     result = src_alpha * src_color + 1 * dst_color\n            //\n            // Which is the blend operation for additive blending\n            return vec4<f32>(color.rgb * color.a, 0.0);\n        } else {\n            // Here, we don't do anything, so that we get premultiplied alpha blending. (As expected)\n            return color.rgba;\n        }\n    }\n    // `Multiply` uses its own `BlendState`, but we still need to premultiply here in the\n    // shader so that we get correct results as we tweak the alpha channel\n    @if(BLEND_MULTIPLY) {\n        // The blend function is:\n        //\n        //     result = dst_color * src_color + (1 - src_alpha) * dst_color\n        //\n        // We premultiply `src_color` by `src_alpha`:\n        //\n        //     src_color *= src_alpha\n        //\n        // We end up with:\n        //\n        //     result = dst_color * (src_color * src_alpha) + (1 - src_alpha) * dst_color\n        //     result = src_alpha * (src_color * dst_color) + (1 - src_alpha) * dst_color\n        //\n        // Which is the blend operation for multiplicative blending with arbitrary mixing\n        // controlled by the source alpha channel\n        return vec4<f32>(color.rgb * color.a, color.a);\n    }\n}\n\n// fog, alpha premultiply\n// for non-hdr cameras, tonemapping and debanding\nfn main_pass_post_lighting_processing(\n    pbr_input: pbr_types::PbrInput,\n    input_color: vec4<f32>,\n) -> vec4<f32> {\n    var output_color = input_color;\n\n    // fog\n    @if(DISTANCE_FOG)\n    if ((pbr_input.material.flags & pbr_types::STANDARD_MATERIAL_FLAGS_FOG_ENABLED_BIT) != 0u) {\n        output_color = apply_fog(view_bindings::fog, output_color, pbr_input.world_position.xyz, view_bindings::view.world_position.xyz);\n    }\n\n    @if(TONEMAP_IN_SHADER) {\n        output_color = tone_mapping(output_color, view_bindings::view.color_grading);\n        @if(DEBAND_DITHER) {\n            var output_rgb = output_color.rgb;\n            output_rgb = powsafe(output_rgb, 1.0 / 2.2);\n            output_rgb += screen_space_dither(pbr_input.frag_coord.xy);\n            // This conversion back to linear space is required because our output texture format is\n            // SRGB; the GPU will assume our output is linear and will apply an SRGB conversion.\n            output_rgb = powsafe(output_rgb, 2.2);\n            output_color = vec4(output_rgb, output_color.a);\n        }\n    }\n    @if(PREMULTIPLY_ALPHA) {\n        output_color = premultiply_alpha(pbr_input.material.flags, output_color);\n    }\n    return output_color;\n}\n","bevy::pbr::pbr_prepass_functions":"@if(BINDLESS)\nimport package::render::bindless::{bindless_samplers_filtering, bindless_textures_2d};\n\nimport package::pbr::{\n    prepass_io::VertexOutput,\n    prepass_bindings::previous_view_uniforms,\n    mesh_bindings::mesh,\n    mesh_view_bindings::view,\n    pbr_bindings,\n    pbr_types,\n};\n\n@if(BINDLESS)\nimport package::pbr::pbr_bindings::material_indices;\n\n// Cutoff used for the premultiplied alpha modes BLEND, ADD, and ALPHA_TO_COVERAGE.\nconst PREMULTIPLIED_ALPHA_CUTOFF = 0.05;\n\n// We can use a simplified version of alpha_discard() here since we only need to handle the alpha_cutoff\nfn prepass_alpha_discard(in: VertexOutput) {\n    @if(!MAY_DISCARD)\n    return;\n\n    @if(BINDLESS)\n    let slot = mesh[in.instance_index].material_and_lightmap_bind_group_slot & 0xffffu;\n\n    @if(BINDLESS)\n    var output_color: vec4<f32> = pbr_bindings::material_array[material_indices[slot].material].base_color;\n    @else\n    var output_color: vec4<f32> = pbr_bindings::material.base_color;\n\n    @if(BINDLESS)\n    let flags = pbr_bindings::material_array[material_indices[slot].material].flags;\n    @else\n    let flags = pbr_bindings::material.flags;\n\n    @if(VERTEX_UVS) {\n        @if(STANDARD_MATERIAL_BASE_COLOR_UV_B)\n        var uv = in.uv_b;\n        @else\n        var uv = in.uv;\n\n        @if(BINDLESS)\n        let uv_transform = pbr_bindings::material_array[material_indices[slot].material].uv_transform;\n        @else\n        let uv_transform = pbr_bindings::material.uv_transform;\n\n        uv = (uv_transform * vec3(uv, 1.0)).xy;\n        if (flags & pbr_types::STANDARD_MATERIAL_FLAGS_BASE_COLOR_TEXTURE_BIT) != 0u {\n            @if(BINDLESS) {\n                output_color = output_color * textureSampleBias(\n                    bindless_textures_2d[material_indices[slot].base_color_texture],\n                    bindless_samplers_filtering[material_indices[slot].base_color_sampler],\n                    uv,\n                    view.mip_bias\n                );\n            }\n            @else {\n                output_color = output_color * textureSampleBias(\n                    pbr_bindings::base_color_texture,\n                    pbr_bindings::base_color_sampler,\n                    uv,\n                    view.mip_bias\n                );\n            }\n        }\n    }\n\n    let alpha_mode = flags & pbr_types::STANDARD_MATERIAL_FLAGS_ALPHA_MODE_RESERVED_BITS;\n    if alpha_mode == pbr_types::STANDARD_MATERIAL_FLAGS_ALPHA_MODE_MASK {\n        @if(BINDLESS)\n        let alpha_cutoff = pbr_bindings::material_array[material_indices[slot].material].alpha_cutoff;\n        @else\n        let alpha_cutoff = pbr_bindings::material.alpha_cutoff;\n\n        if output_color.a < alpha_cutoff {\n            discard;\n        }\n    } else if (alpha_mode == pbr_types::STANDARD_MATERIAL_FLAGS_ALPHA_MODE_BLEND ||\n            alpha_mode == pbr_types::STANDARD_MATERIAL_FLAGS_ALPHA_MODE_ADD ||\n            alpha_mode == pbr_types::STANDARD_MATERIAL_FLAGS_ALPHA_MODE_ALPHA_TO_COVERAGE) {\n        if output_color.a < PREMULTIPLIED_ALPHA_CUTOFF {\n            discard;\n        }\n    } else if alpha_mode == pbr_types::STANDARD_MATERIAL_FLAGS_ALPHA_MODE_PREMULTIPLIED {\n        if all(output_color < vec4(PREMULTIPLIED_ALPHA_CUTOFF)) {\n            discard;\n        }\n    }\n}\n\n@if(MOTION_VECTOR_PREPASS)\nfn calculate_motion_vector(world_position: vec4<f32>, previous_world_position: vec4<f32>) -> vec2<f32> {\n    let clip_position_t = view.unjittered_clip_from_world * world_position;\n    let clip_position = clip_position_t.xy / clip_position_t.w;\n    let previous_clip_position_t = previous_view_uniforms.clip_from_world * previous_world_position;\n    let previous_clip_position = previous_clip_position_t.xy / previous_clip_position_t.w;\n    // These motion vectors are used as offsets to UV positions and are stored\n    // in the range -1,1 to allow offsetting from the one corner to the\n    // diagonally-opposite corner in UV coordinates, in either direction.\n    // A difference between diagonally-opposite corners of clip space is in the\n    // range -2,2, so this needs to be scaled by 0.5. And the V direction goes\n    // down where clip space y goes up, so y needs to be flipped.\n    return (clip_position - previous_clip_position) * vec2(0.5, -0.5);\n}\n","bevy::pbr::pbr_types":"// Since this is a hot path, try to keep the alignment and size of the struct members in mind.\n// You can find the alignment and sizes at <https://www.w3.org/TR/WGSL/#alignment-and-size>.\nstruct StandardMaterial {\n    base_color: vec4<f32>,\n    emissive: vec4<f32>,\n    attenuation_color: vec4<f32>,\n    uv_transform: mat3x3<f32>,\n    reflectance: vec3<f32>,\n    perceptual_roughness: f32,\n    metallic: f32,\n    diffuse_transmission: f32,\n    specular_transmission: f32,\n    thickness: f32,\n    ior: f32,\n    attenuation_distance: f32,\n    clearcoat: f32,\n    clearcoat_perceptual_roughness: f32,\n    anisotropy_strength: f32,\n    anisotropy_rotation: vec2<f32>,\n    // 'flags' is a bit field indicating various options. u32 is 32 bits so we have up to 32 options.\n    flags: u32,\n    alpha_cutoff: f32,\n    parallax_depth_scale: f32,\n    max_parallax_layer_count: f32,\n    lightmap_exposure: f32,\n    max_relief_mapping_search_steps: u32,\n    /// ID for specifying which deferred lighting pass should be used for rendering this material, if any.\n    deferred_lighting_pass_id: u32,\n};\n\n// !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n// NOTE: if these flags are updated or changed. Be sure to also update\n// deferred_flags_from_mesh_material_flags and mesh_material_flags_from_deferred_flags\n// !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nconst STANDARD_MATERIAL_FLAGS_BASE_COLOR_TEXTURE_BIT: u32         = 1u;\nconst STANDARD_MATERIAL_FLAGS_EMISSIVE_TEXTURE_BIT: u32           = 2u;\nconst STANDARD_MATERIAL_FLAGS_METALLIC_ROUGHNESS_TEXTURE_BIT: u32 = 4u;\nconst STANDARD_MATERIAL_FLAGS_OCCLUSION_TEXTURE_BIT: u32          = 8u;\nconst STANDARD_MATERIAL_FLAGS_DOUBLE_SIDED_BIT: u32               = 16u;\nconst STANDARD_MATERIAL_FLAGS_UNLIT_BIT: u32                      = 32u;\nconst STANDARD_MATERIAL_FLAGS_TWO_COMPONENT_NORMAL_MAP: u32       = 64u;\nconst STANDARD_MATERIAL_FLAGS_FLIP_NORMAL_MAP_Y: u32              = 128u;\nconst STANDARD_MATERIAL_FLAGS_FOG_ENABLED_BIT: u32                = 256u;\nconst STANDARD_MATERIAL_FLAGS_DEPTH_MAP_BIT: u32                  = 512u;\nconst STANDARD_MATERIAL_FLAGS_SPECULAR_TRANSMISSION_TEXTURE_BIT: u32 = 1024u;\nconst STANDARD_MATERIAL_FLAGS_THICKNESS_TEXTURE_BIT: u32          = 2048u;\nconst STANDARD_MATERIAL_FLAGS_DIFFUSE_TRANSMISSION_TEXTURE_BIT: u32 = 4096u;\nconst STANDARD_MATERIAL_FLAGS_ATTENUATION_ENABLED_BIT: u32        = 8192u;\nconst STANDARD_MATERIAL_FLAGS_CLEARCOAT_TEXTURE_BIT: u32          = 16384u;\nconst STANDARD_MATERIAL_FLAGS_CLEARCOAT_ROUGHNESS_TEXTURE_BIT: u32 = 32768u;\nconst STANDARD_MATERIAL_FLAGS_CLEARCOAT_NORMAL_TEXTURE_BIT: u32   = 65536u;\nconst STANDARD_MATERIAL_FLAGS_ANISOTROPY_TEXTURE_BIT: u32         = 131072u;\nconst STANDARD_MATERIAL_FLAGS_SPECULAR_TEXTURE_BIT: u32           = 262144u;\nconst STANDARD_MATERIAL_FLAGS_SPECULAR_TINT_TEXTURE_BIT: u32      = 524288u;\nconst STANDARD_MATERIAL_FLAGS_ALPHA_MODE_RESERVED_BITS: u32       = 3758096384u; // (0b111u32 << 29)\nconst STANDARD_MATERIAL_FLAGS_ALPHA_MODE_OPAQUE: u32              = 0u;          // (0u32 << 29)\nconst STANDARD_MATERIAL_FLAGS_ALPHA_MODE_MASK: u32                = 536870912u;  // (1u32 << 29)\nconst STANDARD_MATERIAL_FLAGS_ALPHA_MODE_BLEND: u32               = 1073741824u; // (2u32 << 29)\nconst STANDARD_MATERIAL_FLAGS_ALPHA_MODE_PREMULTIPLIED: u32       = 1610612736u; // (3u32 << 29)\nconst STANDARD_MATERIAL_FLAGS_ALPHA_MODE_ADD: u32                 = 2147483648u; // (4u32 << 29)\nconst STANDARD_MATERIAL_FLAGS_ALPHA_MODE_MULTIPLY: u32            = 2684354560u; // (5u32 << 29)\nconst STANDARD_MATERIAL_FLAGS_ALPHA_MODE_ALPHA_TO_COVERAGE: u32   = 3221225472u; // (6u32 << 29)\n// ↑ To calculate/verify the values above, use the following playground:\n// https://play.rust-lang.org/?version=stable&mode=debug&edition=2021&gist=7792f8dd6fc6a8d4d0b6b1776898a7f4\n\n\n// Creates a StandardMaterial with default values\nfn standard_material_new() -> StandardMaterial {\n    var material: StandardMaterial;\n\n    // NOTE: Keep in-sync with src/pbr_material.rs!\n    material.base_color = vec4<f32>(1.0, 1.0, 1.0, 1.0);\n    material.emissive = vec4<f32>(0.0, 0.0, 0.0, 1.0);\n    material.perceptual_roughness = 0.5;\n    material.metallic = 0.00;\n    material.reflectance = vec3<f32>(0.5);\n    material.diffuse_transmission = 0.0;\n    material.specular_transmission = 0.0;\n    material.thickness = 0.0;\n    material.ior = 1.5;\n    material.attenuation_distance = 1.0;\n    material.attenuation_color = vec4<f32>(1.0, 1.0, 1.0, 1.0);\n    material.clearcoat = 0.0;\n    material.clearcoat_perceptual_roughness = 0.0;\n    material.flags = STANDARD_MATERIAL_FLAGS_ALPHA_MODE_OPAQUE;\n    material.alpha_cutoff = 0.5;\n    material.parallax_depth_scale = 0.1;\n    material.max_parallax_layer_count = 16.0;\n    material.max_relief_mapping_search_steps = 5u;\n    material.deferred_lighting_pass_id = 1u;\n    // scale 1, translation 0, rotation 0\n    material.uv_transform = mat3x3<f32>(1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0);\n\n    return material;\n}\n\nstruct PbrInput {\n    material: StandardMaterial,\n    // Note: this gets monochromized upon deferred PbrInput reconstruction.\n    diffuse_occlusion: vec3<f32>,\n    // Note: this is 1.0 (entirely unoccluded) when SSAO and SSR are off.\n    specular_occlusion: f32,\n    frag_coord: vec4<f32>,\n    world_position: vec4<f32>,\n    // Normalized world normal used for shadow mapping as normal-mapping is not used for shadow\n    // mapping\n    world_normal: vec3<f32>,\n    // Normalized normal-mapped world normal used for lighting\n    N: vec3<f32>,\n    // Normalized view vector in world space, pointing from the fragment world position toward the\n    // view world position\n    V: vec3<f32>,\n    lightmap_light: vec3<f32>,\n    clearcoat_N: vec3<f32>,\n    anisotropy_strength: f32,\n    // These two aren't specific to anisotropy, but we only fill them in if\n    // we're doing anisotropy, so they're prefixed with `anisotropy_`.\n    anisotropy_T: vec3<f32>,\n    anisotropy_B: vec3<f32>,\n    is_orthographic: bool,\n    flags: u32,\n};\n\n// Creates a PbrInput with default values\nfn pbr_input_new() -> PbrInput {\n    var pbr_input: PbrInput;\n\n    pbr_input.material = standard_material_new();\n    pbr_input.diffuse_occlusion = vec3<f32>(1.0);\n    // If SSAO is enabled, then this gets overwritten with proper specular occlusion. If its not, then we get specular environment map unoccluded (we have no data with which to occlude it with).\n    pbr_input.specular_occlusion = 1.0;\n\n    pbr_input.frag_coord = vec4<f32>(0.0, 0.0, 0.0, 1.0);\n    pbr_input.world_position = vec4<f32>(0.0, 0.0, 0.0, 1.0);\n    pbr_input.world_normal = vec3<f32>(0.0, 0.0, 1.0);\n\n    pbr_input.is_orthographic = false;\n\n    pbr_input.N = vec3<f32>(0.0, 0.0, 1.0);\n    pbr_input.V = vec3<f32>(1.0, 0.0, 0.0);\n\n    pbr_input.clearcoat_N = vec3<f32>(0.0);\n    pbr_input.anisotropy_T = vec3<f32>(0.0);\n    pbr_input.anisotropy_B = vec3<f32>(0.0);\n\n    pbr_input.lightmap_light = vec3<f32>(0.0);\n\n    pbr_input.flags = 0u;\n\n    return pbr_input;\n}\n","bevy::pbr::prepass_bindings":"struct PreviousViewUniforms {\n    view_from_world: mat4x4<f32>,\n    clip_from_world: mat4x4<f32>,\n    clip_from_view: mat4x4<f32>,\n}\n\n@group(0) @binding(2) var<uniform> previous_view_uniforms: PreviousViewUniforms;\n\n// Material bindings will be in @group(2)\n","bevy::pbr::prepass_io":"// Most of these attributes are not used in the default prepass fragment shader, but they are still needed so we can\n// pass them to custom prepass shaders like pbr_prepass.wgsl.\nstruct Vertex {\n    @builtin(instance_index) instance_index: u32,\n    @location(0) position: vec3<f32>,\n\n    @if(VERTEX_UVS_A)\n    @location(1) uv: vec2<f32>,\n\n    @if(VERTEX_UVS_B)\n    @location(2) uv_b: vec2<f32>,\n\n    @if(NORMAL_PREPASS_OR_DEFERRED_PREPASS && VERTEX_NORMALS)\n    @location(3) normal: vec3<f32>,\n\n    @if(NORMAL_PREPASS_OR_DEFERRED_PREPASS && VERTEX_TANGENTS)\n    @location(4) tangent: vec4<f32>,\n\n    @if(SKINNED)\n    @location(5) joint_indices: vec4<u32>,\n    @if(SKINNED)\n    @location(6) joint_weights: vec4<f32>,\n\n    @if(VERTEX_COLORS)\n    @location(7) color: vec4<f32>,\n\n    @if(MORPH_TARGETS)\n    @builtin(vertex_index) index: u32,\n}\n\nstruct VertexOutput {\n    // This is `clip position` when the struct is used as a vertex stage output\n    // and `frag coord` when used as a fragment stage input\n    @builtin(position) position: vec4<f32>,\n\n    @if(VERTEX_UVS_A)\n    @location(0) uv: vec2<f32>,\n\n    @if(VERTEX_UVS_B)\n    @location(1) uv_b: vec2<f32>,\n\n    @if(NORMAL_PREPASS_OR_DEFERRED_PREPASS)\n    @location(2) world_normal: vec3<f32>,\n    @if(NORMAL_PREPASS_OR_DEFERRED_PREPASS && VERTEX_TANGENTS)\n    @location(3) world_tangent: vec4<f32>,\n\n    @location(4) world_position: vec4<f32>,\n    @if(MOTION_VECTOR_PREPASS)\n    @location(5) previous_world_position: vec4<f32>,\n\n    @if(UNCLIPPED_DEPTH_ORTHO_EMULATION)\n    @location(6) unclipped_depth: f32,\n\n    @if(VERTEX_OUTPUT_INSTANCE_INDEX)\n    @location(7) instance_index: u32,\n\n    @if(VERTEX_COLORS)\n    @location(8) color: vec4<f32>,\n\n    @if(VISIBILITY_RANGE_DITHER)\n    @location(9) @interpolate(flat) visibility_range_dither: i32,\n}\n\n@if(PREPASS_FRAGMENT)\nstruct FragmentOutput {\n    @if(NORMAL_PREPASS)\n    @location(0) normal: vec4<f32>,\n\n    @if(MOTION_VECTOR_PREPASS)\n    @location(1) motion_vector: vec2<f32>,\n\n    @if(DEFERRED_PREPASS)\n    @location(2) deferred: vec4<u32>,\n    @if(DEFERRED_PREPASS)\n    @location(3) deferred_lighting_pass_id: u32,\n\n    @if(UNCLIPPED_DEPTH_ORTHO_EMULATION)\n    @builtin(frag_depth) frag_depth: f32,\n}\n","bevy::pbr::prepass_utils":"import package::pbr::mesh_view_bindings as view_bindings;\n\n@if(DEPTH_PREPASS)\nfn prepass_depth(frag_coord: vec4<f32>, sample_index: u32) -> f32 {\n    @if(MULTISAMPLED)\n    return textureLoad(view_bindings::depth_prepass_texture, vec2<i32>(frag_coord.xy), i32(sample_index));\n    @else\n    return textureLoad(view_bindings::depth_prepass_texture, vec2<i32>(frag_coord.xy), 0);\n}\n\n@if(NORMAL_PREPASS)\nfn prepass_normal(frag_coord: vec4<f32>, sample_index: u32) -> vec3<f32> {\n    @if(MULTISAMPLED)\n    let normal_sample = textureLoad(view_bindings::normal_prepass_texture, vec2<i32>(frag_coord.xy), i32(sample_index));\n    @else\n    let normal_sample = textureLoad(view_bindings::normal_prepass_texture, vec2<i32>(frag_coord.xy), 0);\n    return normalize(normal_sample.xyz * 2.0 - vec3(1.0));\n}\n\n@if(MOTION_VECTOR_PREPASS)\nfn prepass_motion_vector(frag_coord: vec4<f32>, sample_index: u32) -> vec2<f32> {\n    @if(MULTISAMPLED)\n    let motion_vector_sample = textureLoad(view_bindings::motion_vector_prepass_texture, vec2<i32>(frag_coord.xy), i32(sample_index));\n    @else\n    let motion_vector_sample = textureLoad(view_bindings::motion_vector_prepass_texture, vec2<i32>(frag_coord.xy), 0);\n    return motion_vector_sample.rg;\n}\n","bevy::pbr::raymarch":"// Copyright (c) 2023 Tomasz Stachowiak\n//\n// This contribution is dual licensed under EITHER OF\n//\n//     Apache License, Version 2.0, (http://www.apache.org/licenses/LICENSE-2.0)\n//     MIT license (http://opensource.org/licenses/MIT)\n//\n// at your option.\n//\n// This is a port of the original [`raymarch.hlsl`] to WGSL. It's deliberately\n// kept as close as possible so that patches to the original `raymarch.hlsl`\n// have the greatest chances of applying to this version.\n//\n// [`raymarch.hlsl`]:\n// https://gist.github.com/h3r2tic/9c8356bdaefbe80b1a22ae0aaee192db\n\n\nimport package::pbr::mesh_view_bindings::depth_prepass_texture;\nimport package::pbr::view_transformations::{\n    direction_world_to_clip,\n    ndc_to_uv,\n    perspective_camera_near,\n    position_world_to_ndc,\n};\n\n// Allows us to sample from the depth buffer with bilinear filtering.\n@group(1) @binding(2) var depth_linear_sampler: sampler;\n\n// Allows us to sample from the depth buffer with nearest-neighbor filtering.\n@group(1) @binding(3) var depth_nearest_sampler: sampler;\n\n// Main code\n\nstruct HybridRootFinder {\n    linear_steps: u32,\n    bisection_steps: u32,\n    use_secant: bool,\n    linear_march_exponent: f32,\n\n    jitter: f32,\n    min_t: f32,\n    max_t: f32,\n}\n\nfn hybrid_root_finder_new_with_linear_steps(v: u32) -> HybridRootFinder {\n    var res: HybridRootFinder;\n    res.linear_steps = v;\n    res.bisection_steps = 0u;\n    res.use_secant = false;\n    res.linear_march_exponent = 1.0;\n    res.jitter = 1.0;\n    res.min_t = 0.0;\n    res.max_t = 1.0;\n    return res;\n}\n\nfn hybrid_root_finder_find_root(\n    root_finder: ptr<function, HybridRootFinder>,\n    start: vec3<f32>,\n    end: vec3<f32>,\n    distance_fn: ptr<function, DepthRaymarchDistanceFn>,\n    hit_t: ptr<function, f32>,\n    miss_t: ptr<function, f32>,\n    hit_d: ptr<function, DistanceWithPenetration>,\n) -> bool {\n    let dir = end - start;\n\n    var min_t = (*root_finder).min_t;\n    var max_t = (*root_finder).max_t;\n\n    var min_d = DistanceWithPenetration(0.0, false, 0.0);\n    var max_d = DistanceWithPenetration(0.0, false, 0.0);\n\n    let step_size = (max_t - min_t) / f32((*root_finder).linear_steps);\n\n    var intersected = false;\n\n    //\n    // Ray march using linear steps\n\n    if ((*root_finder).linear_steps > 0u) {\n        let candidate_t = mix(\n            min_t,\n            max_t,\n            pow(\n                (*root_finder).jitter / f32((*root_finder).linear_steps),\n                (*root_finder).linear_march_exponent\n            )\n        );\n\n        let candidate = start + dir * candidate_t;\n        let candidate_d = depth_raymarch_distance_fn_evaluate(distance_fn, candidate);\n        intersected = candidate_d.distance < 0.0 && candidate_d.valid;\n\n        if (intersected) {\n            max_t = candidate_t;\n            max_d = candidate_d;\n            // The `[min_t .. max_t]` interval contains an intersection. End the linear search.\n        } else {\n            // No intersection yet. Carry on.\n            min_t = candidate_t;\n            min_d = candidate_d;\n\n            for (var step = 1u; step < (*root_finder).linear_steps; step += 1u) {\n                let candidate_t = mix(\n                    (*root_finder).min_t,\n                    (*root_finder).max_t,\n                    pow(\n                        (f32(step) + (*root_finder).jitter) / f32((*root_finder).linear_steps),\n                        (*root_finder).linear_march_exponent\n                    )\n                );\n\n                let candidate = start + dir * candidate_t;\n                let candidate_d = depth_raymarch_distance_fn_evaluate(distance_fn, candidate);\n                intersected = candidate_d.distance < 0.0 && candidate_d.valid;\n\n                if (intersected) {\n                    max_t = candidate_t;\n                    max_d = candidate_d;\n                    // The `[min_t .. max_t]` interval contains an intersection.\n                    // End the linear search.\n                    break;\n                } else {\n                    // No intersection yet. Carry on.\n                    min_t = candidate_t;\n                    min_d = candidate_d;\n                }\n            }\n        }\n    }\n\n    *miss_t = min_t;\n    *hit_t = min_t;\n\n    //\n    // Refine the hit using bisection\n\n    if (intersected) {\n        for (var step = 0u; step < (*root_finder).bisection_steps; step += 1u) {\n            let mid_t = (min_t + max_t) * 0.5;\n            let candidate = start + dir * mid_t;\n            let candidate_d = depth_raymarch_distance_fn_evaluate(distance_fn, candidate);\n\n            if (candidate_d.distance < 0.0 && candidate_d.valid) {\n                // Intersection at the mid point. Refine the first half.\n                max_t = mid_t;\n                max_d = candidate_d;\n            } else {\n                // No intersection yet at the mid point. Refine the second half.\n                min_t = mid_t;\n                min_d = candidate_d;\n            }\n        }\n\n        if ((*root_finder).use_secant) {\n            // Finish with one application of the secant method\n            let total_d = min_d.distance + -max_d.distance;\n\n            let mid_t = mix(min_t, max_t, min_d.distance / total_d);\n            let candidate = start + dir * mid_t;\n            let candidate_d = depth_raymarch_distance_fn_evaluate(distance_fn, candidate);\n\n            // Only accept the result of the secant method if it improves upon\n            // the previous result.\n            //\n            // Technically root_finder should be `abs(candidate_d.distance) <\n            // min(min_d.distance, -max_d.distance) * frac`, but root_finder seems\n            // sufficient.\n            if (abs(candidate_d.distance) < min_d.distance * 0.9 && candidate_d.valid) {\n                *hit_t = mid_t;\n                *hit_d = candidate_d;\n            } else {\n                *hit_t = max_t;\n                *hit_d = max_d;\n            }\n\n            return true;\n        } else {\n            *hit_t = max_t;\n            *hit_d = max_d;\n            return true;\n        }\n    } else {\n        // Mark the conservative miss distance.\n        *hit_t = min_t;\n        return false;\n    }\n}\n\nstruct DistanceWithPenetration {\n    /// Distance to the surface of which a root we're trying to find\n    distance: f32,\n\n    /// Whether to consider this sample valid for intersection.\n    /// Mostly relevant for allowing the ray marcher to travel behind surfaces,\n    /// as it will mark surfaces it travels under as invalid.\n    valid: bool,\n\n    /// Conservative estimate of depth to which the ray penetrates the marched surface.\n    penetration: f32,\n}\n\nstruct DepthRaymarchDistanceFn {\n    depth_tex_size: vec2<f32>,\n\n    march_behind_surfaces: bool,\n    depth_thickness: f32,\n\n    use_sloppy_march: bool,\n}\n\nfn depth_raymarch_distance_fn_evaluate(\n    distance_fn: ptr<function, DepthRaymarchDistanceFn>,\n    ray_point_cs: vec3<f32>,\n) -> DistanceWithPenetration {\n    let interp_uv = ndc_to_uv(ray_point_cs.xy);\n\n    let ray_depth = 1.0 / ray_point_cs.z;\n\n    // We're using both point-sampled and bilinear-filtered values from the depth buffer.\n    //\n    // That's really stupid but works like magic. For samples taken near the ray origin,\n    // the discrete nature of the depth buffer becomes a problem. It's not a land of continuous surfaces,\n    // but a bunch of stacked duplo bricks.\n    //\n    // Technically we should be taking discrete steps in distance_fn duplo land, but then we're at the mercy\n    // of arbitrary quantization of our directions -- and sometimes we'll take a step which would\n    // claim that the ray is occluded -- even though the underlying smooth surface wouldn't occlude it.\n    //\n    // If we instead take linear taps from the depth buffer, we reconstruct the linear surface.\n    // That fixes acne, but introduces false shadowing near object boundaries, as we now pretend\n    // that everything is shrink-wrapped by distance_fn continuous 2.5D surface, and our depth thickness\n    // heuristic ends up falling apart.\n    //\n    // The fix is to consider both the smooth and the discrete surfaces, and only claim occlusion\n    // when the ray descends below both.\n    //\n    // The two approaches end up fixing each other's artifacts:\n    // * The false occlusions due to duplo land are rejected because the ray stays above the smooth surface.\n    // * The shrink-wrap surface is no longer continuous, so it's possible for rays to miss it.\n\n    let linear_depth =\n        1.0 / textureSampleLevel(depth_prepass_texture, depth_linear_sampler, interp_uv, 0u);\n    let unfiltered_depth =\n        1.0 / textureSampleLevel(depth_prepass_texture, depth_nearest_sampler, interp_uv, 0u);\n\n    var max_depth: f32;\n    var min_depth: f32;\n\n    if ((*distance_fn).use_sloppy_march) {\n        max_depth = unfiltered_depth;\n        min_depth = unfiltered_depth;\n    } else {\n        max_depth = max(linear_depth, unfiltered_depth);\n        min_depth = min(linear_depth, unfiltered_depth);\n    }\n\n    let bias = 0.000002;\n\n    var res: DistanceWithPenetration;\n    res.distance = max_depth * (1.0 + bias) - ray_depth;\n\n    // distance_fn will be used at the end of the ray march to potentially discard the hit.\n    res.penetration = ray_depth - min_depth;\n\n    if ((*distance_fn).march_behind_surfaces) {\n        res.valid = res.penetration < (*distance_fn).depth_thickness;\n    } else {\n        res.valid = true;\n    }\n\n    return res;\n}\n\nstruct DepthRayMarchResult {\n    /// True if the raymarch hit something.\n    hit: bool,\n\n    /// In case of a hit, the normalized distance to it.\n    ///\n    /// In case of a miss, the furthest the ray managed to travel, which could either be\n    /// exceeding the max range, or getting behind a surface further than the depth thickness.\n    ///\n    /// Range: `0..=1` as a lerp factor over `ray_start_cs..=ray_end_cs`.\n    hit_t: f32,\n\n    /// UV corresponding to `hit_t`.\n    hit_uv: vec2<f32>,\n\n    /// The distance that the hit point penetrates into the hit surface.\n    /// Will normally be non-zero due to limited precision of the ray march.\n    ///\n    /// In case of a miss: undefined.\n    hit_penetration: f32,\n\n    /// Ditto, within the range `0..DepthRayMarch::depth_thickness_linear_z`\n    ///\n    /// In case of a miss: undefined.\n    hit_penetration_frac: f32,\n}\n\nstruct DepthRayMarch {\n    /// Number of steps to be taken at regular intervals to find an initial intersection.\n    /// Must not be zero.\n    linear_steps: u32,\n\n    /// Exponent to be applied in the linear part of the march.\n    ///\n    /// A value of 1.0 will result in equidistant steps, and higher values will compress\n    /// the earlier steps, and expand the later ones. This might be desirable in order\n    /// to get more detail close to objects in SSR or SSGI.\n    ///\n    /// For optimal performance, this should be a small compile-time unsigned integer,\n    /// such as 1 or 2.\n    linear_march_exponent: f32,\n\n    /// Number of steps in a bisection (binary search) to perform once the linear search\n    /// has found an intersection. Helps narrow down the hit, increasing the chance of\n    /// the secant method finding an accurate hit point.\n    ///\n    /// Useful when sampling color, e.g. SSR or SSGI, but pointless for contact shadows.\n    bisection_steps: u32,\n\n    /// Approximate the root position using the secant method -- by solving for line-line\n    /// intersection between the ray approach rate and the surface gradient.\n    ///\n    /// Useful when sampling color, e.g. SSR or SSGI, but pointless for contact shadows.\n    use_secant: bool,\n\n    /// Jitter to apply to the first step of the linear search; 0..=1 range, mapping\n    /// to the extent of a single linear step in the first phase of the search.\n    /// Use 1.0 if you don't want jitter.\n    jitter: f32,\n\n    /// Clip space coordinates (w=1) of the ray.\n    ray_start_cs: vec3<f32>,\n    ray_end_cs: vec3<f32>,\n\n    /// Should be used for contact shadows, but not for any color bounce, e.g. SSR.\n    ///\n    /// For SSR etc. this can easily create leaks, but with contact shadows it allows the rays\n    /// to pass over invalid occlusions (due to thickness), and find potentially valid ones ahead.\n    ///\n    /// Note that this will cause the linear search to potentially miss surfaces,\n    /// because when the ray overshoots and ends up penetrating a surface further than\n    /// `depth_thickness_linear_z`, the ray marcher will just carry on.\n    ///\n    /// For this reason, this may require a lot of samples, or high depth thickness,\n    /// so that `depth_thickness_linear_z >= world space ray length / linear_steps`.\n    march_behind_surfaces: bool,\n\n    /// If `true`, the ray marcher only performs nearest lookups of the depth buffer,\n    /// resulting in aliasing and false occlusion when marching tiny detail.\n    /// It should work fine for longer traces with fewer rays though.\n    use_sloppy_march: bool,\n\n    /// When marching the depth buffer, we only have 2.5D information, and don't know how\n    /// thick surfaces are. We shall assume that the depth buffer fragments are little squares\n    /// with a constant thickness defined by this parameter.\n    depth_thickness_linear_z: f32,\n\n    /// Size of the depth buffer we're marching in, in pixels.\n    depth_tex_size: vec2<f32>,\n}\n\nfn depth_ray_march_new_from_depth(depth_tex_size: vec2<f32>) -> DepthRayMarch {\n    var res: DepthRayMarch;\n    res.jitter = 1.0;\n    res.linear_steps = 4u;\n    res.bisection_steps = 0u;\n    res.linear_march_exponent = 1.0;\n    res.depth_tex_size = depth_tex_size;\n    res.depth_thickness_linear_z = 1.0;\n    res.march_behind_surfaces = false;\n    res.use_sloppy_march = false;\n    return res;\n}\n\nfn depth_ray_march_to_cs_dir_impl(\n    raymarch: ptr<function, DepthRayMarch>,\n    dir_cs: vec4<f32>,\n    infinite: bool,\n) {\n    var end_cs = vec4((*raymarch).ray_start_cs, 1.0) + dir_cs;\n\n    // Perform perspective division, but avoid dividing by zero for rays\n    // heading directly towards the eye.\n    end_cs /= select(-1.0, 1.0, end_cs.w >= 0.0) * max(1e-10, abs(end_cs.w));\n\n    // Clip ray start to the view frustum\n    var delta_cs = end_cs.xyz - (*raymarch).ray_start_cs;\n    let near_edge = select(vec3(-1.0, -1.0, 0.0), vec3(1.0, 1.0, 1.0), delta_cs < vec3(0.0));\n    let dist_to_near_edge = (near_edge - (*raymarch).ray_start_cs) / delta_cs;\n    let max_dist_to_near_edge = max(dist_to_near_edge.x, dist_to_near_edge.y);\n    (*raymarch).ray_start_cs += delta_cs * max(0.0, max_dist_to_near_edge);\n\n    // Clip ray end to the view frustum\n\n    delta_cs = end_cs.xyz - (*raymarch).ray_start_cs;\n    let far_edge = select(vec3(-1.0, -1.0, 0.0), vec3(1.0, 1.0, 1.0), delta_cs >= vec3(0.0));\n    let dist_to_far_edge = (far_edge - (*raymarch).ray_start_cs) / delta_cs;\n    let min_dist_to_far_edge = min(\n        min(dist_to_far_edge.x, dist_to_far_edge.y),\n        dist_to_far_edge.z\n    );\n\n    if (infinite) {\n        delta_cs *= min_dist_to_far_edge;\n    } else {\n        // If unbounded, would make the ray reach the end of the frustum\n        delta_cs *= min(1.0, min_dist_to_far_edge);\n    }\n\n    (*raymarch).ray_end_cs = (*raymarch).ray_start_cs + delta_cs;\n}\n\n/// March from a clip-space position (w = 1)\nfn depth_ray_march_from_cs(raymarch: ptr<function, DepthRayMarch>, v: vec3<f32>) {\n    (*raymarch).ray_start_cs = v;\n}\n\n/// March to a clip-space position (w = 1)\n///\n/// Must be called after `from_cs`, as it will clip the world-space ray to the view frustum.\nfn depth_ray_march_to_cs(raymarch: ptr<function, DepthRayMarch>, end_cs: vec3<f32>) {\n    let dir = vec4(end_cs - (*raymarch).ray_start_cs, 0.0) * sign(end_cs.z);\n    depth_ray_march_to_cs_dir_impl(raymarch, dir, false);\n}\n\n/// March towards a clip-space direction. Infinite (ray is extended to cover the whole view frustum).\n///\n/// Must be called after `from_cs`, as it will clip the world-space ray to the view frustum.\nfn depth_ray_march_to_cs_dir(raymarch: ptr<function, DepthRayMarch>, dir: vec4<f32>) {\n    depth_ray_march_to_cs_dir_impl(raymarch, dir, true);\n}\n\n/// March to a world-space position.\n///\n/// Must be called after `from_cs`, as it will clip the world-space ray to the view frustum.\nfn depth_ray_march_to_ws(raymarch: ptr<function, DepthRayMarch>, end: vec3<f32>) {\n    depth_ray_march_to_cs(raymarch, position_world_to_ndc(end));\n}\n\n/// March towards a world-space direction. Infinite (ray is extended to cover the whole view frustum).\n///\n/// Must be called after `from_cs`, as it will clip the world-space ray to the view frustum.\nfn depth_ray_march_to_ws_dir(raymarch: ptr<function, DepthRayMarch>, dir: vec3<f32>) {\n    depth_ray_march_to_cs_dir_impl(raymarch, direction_world_to_clip(dir), true);\n}\n\n/// Perform the ray march.\nfn depth_ray_march_march(raymarch: ptr<function, DepthRayMarch>) -> DepthRayMarchResult {\n    var res = DepthRayMarchResult(false, 0.0, vec2(0.0), 0.0, 0.0);\n\n    let ray_start_uv = ndc_to_uv((*raymarch).ray_start_cs.xy);\n    let ray_end_uv = ndc_to_uv((*raymarch).ray_end_cs.xy);\n\n    let ray_uv_delta = ray_end_uv - ray_start_uv;\n    let ray_len_px = ray_uv_delta * (*raymarch).depth_tex_size;\n\n    let min_px_per_step = 1u;\n    let step_count = max(\n        2,\n        min(i32((*raymarch).linear_steps), i32(floor(length(ray_len_px) / f32(min_px_per_step))))\n    );\n\n    let linear_z_to_scaled_linear_z = 1.0 / perspective_camera_near();\n    let depth_thickness = (*raymarch).depth_thickness_linear_z * linear_z_to_scaled_linear_z;\n\n    var distance_fn: DepthRaymarchDistanceFn;\n    distance_fn.depth_tex_size = (*raymarch).depth_tex_size;\n    distance_fn.march_behind_surfaces = (*raymarch).march_behind_surfaces;\n    distance_fn.depth_thickness = depth_thickness;\n    distance_fn.use_sloppy_march = (*raymarch).use_sloppy_march;\n\n    var hit: DistanceWithPenetration;\n\n    var hit_t = 0.0;\n    var miss_t = 0.0;\n    var root_finder = hybrid_root_finder_new_with_linear_steps(u32(step_count));\n    root_finder.bisection_steps = (*raymarch).bisection_steps;\n    root_finder.use_secant = (*raymarch).use_secant;\n    root_finder.linear_march_exponent = (*raymarch).linear_march_exponent;\n    root_finder.jitter = (*raymarch).jitter;\n    let intersected = hybrid_root_finder_find_root(\n        &root_finder,\n        (*raymarch).ray_start_cs,\n        (*raymarch).ray_end_cs,\n        &distance_fn,\n        &hit_t,\n        &miss_t,\n        &hit\n    );\n\n    res.hit_t = hit_t;\n\n    if (intersected && hit.penetration < depth_thickness && hit.distance < depth_thickness) {\n        res.hit = true;\n        res.hit_uv = mix(ray_start_uv, ray_end_uv, res.hit_t);\n        res.hit_penetration = hit.penetration / linear_z_to_scaled_linear_z;\n        res.hit_penetration_frac = hit.penetration / depth_thickness;\n        return res;\n    }\n\n    res.hit_t = miss_t;\n    res.hit_uv = mix(ray_start_uv, ray_end_uv, res.hit_t);\n\n    return res;\n}\n","bevy::pbr::rgb9e5":"const RGB9E5_EXPONENT_BITS        = 5u;\nconst RGB9E5_MANTISSA_BITS        = 9;\nconst RGB9E5_MANTISSA_BITSU       = 9u;\nconst RGB9E5_EXP_BIAS             = 15;\nconst RGB9E5_MAX_VALID_BIASED_EXP = 31u;\n\n//#define MAX_RGB9E5_EXP               (RGB9E5_MAX_VALID_BIASED_EXP - RGB9E5_EXP_BIAS)\n//#define RGB9E5_MANTISSA_VALUES       (1<<RGB9E5_MANTISSA_BITS)\n//#define MAX_RGB9E5_MANTISSA          (RGB9E5_MANTISSA_VALUES-1)\n//#define MAX_RGB9E5                   ((f32(MAX_RGB9E5_MANTISSA))/RGB9E5_MANTISSA_VALUES * (1<<MAX_RGB9E5_EXP))\n//#define EPSILON_RGB9E5_              ((1.0/RGB9E5_MANTISSA_VALUES) / (1<<RGB9E5_EXP_BIAS))\n\nconst MAX_RGB9E5_EXP              = 16u;\nconst RGB9E5_MANTISSA_VALUES      = 512;\nconst MAX_RGB9E5_MANTISSA         = 511;\nconst MAX_RGB9E5_MANTISSAU        = 511u;\nconst MAX_RGB9E5_                 = 65408.0;\nconst EPSILON_RGB9E5_             = 0.000000059604645;\n\nfn floor_log2_(x: f32) -> i32 {\n    let f = bitcast<u32>(x);\n    let biasedexponent = (f & 0x7F800000u) >> 23u;\n    return i32(biasedexponent) - 127;\n}\n\n// https://www.khronos.org/registry/OpenGL/extensions/EXT/EXT_texture_shared_exponent.txt\nfn vec3_to_rgb9e5_(rgb_in: vec3<f32>) -> u32 {\n    let rgb = clamp(rgb_in, vec3(0.0), vec3(MAX_RGB9E5_));\n\n    let maxrgb = max(rgb.r, max(rgb.g, rgb.b));\n    var exp_shared = max(-RGB9E5_EXP_BIAS - 1, floor_log2_(maxrgb)) + 1 + RGB9E5_EXP_BIAS;\n    var denom = exp2(f32(exp_shared - RGB9E5_EXP_BIAS - RGB9E5_MANTISSA_BITS));\n\n    let maxm = i32(floor(maxrgb / denom + 0.5));\n    if (maxm == RGB9E5_MANTISSA_VALUES) {\n        denom *= 2.0;\n        exp_shared += 1;\n    }\n\n    let n = vec3<u32>(floor(rgb / denom + 0.5));\n    \n    return (u32(exp_shared) << 27u) | (n.b << 18u) | (n.g << 9u) | (n.r << 0u);\n}\n\n// Builtin extractBits() is not working on WEBGL or DX12\n// DX12: HLSL: Unimplemented(\"write_expr_math ExtractBits\")\nfn extract_bits(value: u32, offset: u32, bits: u32) -> u32 {\n    let mask = (1u << bits) - 1u;\n    return (value >> offset) & mask;\n}\n\nfn rgb9e5_to_vec3_(v: u32) -> vec3<f32> {\n    let exponent = i32(extract_bits(v, 27u, RGB9E5_EXPONENT_BITS)) - RGB9E5_EXP_BIAS - RGB9E5_MANTISSA_BITS;\n    let scale = exp2(f32(exponent));\n\n    return vec3(\n        f32(extract_bits(v, 0u, RGB9E5_MANTISSA_BITSU)),\n        f32(extract_bits(v, 9u, RGB9E5_MANTISSA_BITSU)),\n        f32(extract_bits(v, 18u, RGB9E5_MANTISSA_BITSU))\n    ) * scale;\n}\n","bevy::pbr::shadow_sampling":"import package::pbr::{\n    mesh_view_bindings as view_bindings,\n    utils::interleaved_gradient_noise,\n    utils,\n};\nimport package::render::maths::{orthonormalize, PI};\n\n// Do the lookup, using HW 2x2 PCF and comparison\nfn sample_shadow_map_hardware(light_local: vec2<f32>, depth: f32, array_index: i32) -> f32 {\n    @if(NO_ARRAY_TEXTURES_SUPPORT) {\n        return textureSampleCompare(\n            view_bindings::directional_shadow_textures,\n            view_bindings::directional_shadow_textures_comparison_sampler,\n            light_local,\n            depth,\n        );\n    }\n    @else {\n        return textureSampleCompareLevel(\n            view_bindings::directional_shadow_textures,\n            view_bindings::directional_shadow_textures_comparison_sampler,\n            light_local,\n            array_index,\n            depth,\n        );\n    }\n}\n\n// Does a single sample of the blocker search, a part of the PCSS algorithm.\n// This is the variant used for directional lights.\nfn search_for_blockers_in_shadow_map_hardware(\n    light_local: vec2<f32>,\n    depth: f32,\n    array_index: i32,\n) -> vec2<f32> {\n    @if(WEBGL2) {\n        // Make sure that the WebGL 2 compiler doesn't see `sampled_depth` sampled\n        // with different samplers, or it'll blow up.\n        return vec2(0.0);\n    }\n    @else {\n        @if(PCSS_SAMPLERS_AVAILABLE) {\n            @if(NO_ARRAY_TEXTURES_SUPPORT)\n            let sampled_depth = textureSampleLevel(\n                view_bindings::directional_shadow_textures,\n                view_bindings::directional_shadow_textures_linear_sampler,\n                light_local,\n                0u,\n            );\n            @else\n            let sampled_depth = textureSampleLevel(\n                view_bindings::directional_shadow_textures,\n                view_bindings::directional_shadow_textures_linear_sampler,\n                light_local,\n                array_index,\n                0u,\n            );\n\n            return select(vec2(0.0), vec2(sampled_depth, 1.0), sampled_depth >= depth);\n        }\n        @else {\n            return vec2(0.0);\n        }\n    }\n}\n\n// Numbers determined by trial and error that gave nice results.\nconst SPOT_SHADOW_TEXEL_SIZE: f32 = 0.0134277345;\nconst POINT_SHADOW_SCALE: f32 = 0.003;\nconst POINT_SHADOW_TEMPORAL_OFFSET_SCALE: f32 = 0.5;\n\n// These are the standard MSAA sample point positions from D3D. They were chosen\n// to get a reasonable distribution that's not too regular.\n//\n// https://learn.microsoft.com/en-us/windows/win32/api/d3d11/ne-d3d11-d3d11_standard_multisample_quality_levels?redirectedfrom=MSDN\nconst D3D_SAMPLE_POINT_POSITIONS: array<vec2<f32>, 8> = array(\n    vec2( 0.125, -0.375),\n    vec2(-0.125,  0.375),\n    vec2( 0.625,  0.125),\n    vec2(-0.375, -0.625),\n    vec2(-0.625,  0.625),\n    vec2(-0.875, -0.125),\n    vec2( 0.375,  0.875),\n    vec2( 0.875, -0.875),\n);\n\n// And these are the coefficients corresponding to the probability distribution\n// function of a 2D Gaussian lobe with zero mean and the identity covariance\n// matrix at those points.\nconst D3D_SAMPLE_POINT_COEFFS: array<f32, 8> = array(\n    0.157112,\n    0.157112,\n    0.138651,\n    0.130251,\n    0.114946,\n    0.114946,\n    0.107982,\n    0.079001,\n);\n\n// https://web.archive.org/web/20230210095515/http://the-witness.net/news/2013/09/shadow-mapping-summary-part-1\nfn sample_shadow_map_castano_thirteen(light_local: vec2<f32>, depth: f32, array_index: i32) -> f32 {\n    let shadow_map_size = vec2<f32>(textureDimensions(view_bindings::directional_shadow_textures));\n    let inv_shadow_map_size = 1.0 / shadow_map_size;\n\n    let uv = light_local * shadow_map_size;\n    var base_uv = floor(uv + 0.5);\n    let s = (uv.x + 0.5 - base_uv.x);\n    let t = (uv.y + 0.5 - base_uv.y);\n    base_uv -= 0.5;\n    base_uv *= inv_shadow_map_size;\n\n    let uw0 = (4.0 - 3.0 * s);\n    let uw1 = 7.0;\n    let uw2 = (1.0 + 3.0 * s);\n\n    let u0 = (3.0 - 2.0 * s) / uw0 - 2.0;\n    let u1 = (3.0 + s) / uw1;\n    let u2 = s / uw2 + 2.0;\n\n    let vw0 = (4.0 - 3.0 * t);\n    let vw1 = 7.0;\n    let vw2 = (1.0 + 3.0 * t);\n\n    let v0 = (3.0 - 2.0 * t) / vw0 - 2.0;\n    let v1 = (3.0 + t) / vw1;\n    let v2 = t / vw2 + 2.0;\n\n    var sum = 0.0;\n\n    sum += uw0 * vw0 * sample_shadow_map_hardware(base_uv + (vec2(u0, v0) * inv_shadow_map_size), depth, array_index);\n    sum += uw1 * vw0 * sample_shadow_map_hardware(base_uv + (vec2(u1, v0) * inv_shadow_map_size), depth, array_index);\n    sum += uw2 * vw0 * sample_shadow_map_hardware(base_uv + (vec2(u2, v0) * inv_shadow_map_size), depth, array_index);\n\n    sum += uw0 * vw1 * sample_shadow_map_hardware(base_uv + (vec2(u0, v1) * inv_shadow_map_size), depth, array_index);\n    sum += uw1 * vw1 * sample_shadow_map_hardware(base_uv + (vec2(u1, v1) * inv_shadow_map_size), depth, array_index);\n    sum += uw2 * vw1 * sample_shadow_map_hardware(base_uv + (vec2(u2, v1) * inv_shadow_map_size), depth, array_index);\n\n    sum += uw0 * vw2 * sample_shadow_map_hardware(base_uv + (vec2(u0, v2) * inv_shadow_map_size), depth, array_index);\n    sum += uw1 * vw2 * sample_shadow_map_hardware(base_uv + (vec2(u1, v2) * inv_shadow_map_size), depth, array_index);\n    sum += uw2 * vw2 * sample_shadow_map_hardware(base_uv + (vec2(u2, v2) * inv_shadow_map_size), depth, array_index);\n\n    return sum * (1.0 / 144.0);\n}\n\nfn map(min1: f32, max1: f32, min2: f32, max2: f32, value: f32) -> f32 {\n    return min2 + (value - min1) * (max2 - min2) / (max1 - min1);\n}\n\n// Creates a random rotation matrix using interleaved gradient noise.\n//\n// See: https://www.iryoku.com/next-generation-post-processing-in-call-of-duty-advanced-warfare/\nfn random_rotation_matrix(scale: vec2<f32>, temporal: bool) -> mat2x2<f32> {\n    let random_angle = 2.0 * PI * interleaved_gradient_noise(\n        scale, select(1u, view_bindings::globals.frame_count, temporal));\n    let m = vec2(sin(random_angle), cos(random_angle));\n    return mat2x2(\n        m.y, -m.x,\n        m.x, m.y\n    );\n}\n\n// Calculates the distance between spiral samples for the given texel size and\n// penumbra size. This is used for the Jimenez '14 (i.e. temporal) variant of\n// shadow sampling.\nfn calculate_uv_offset_scale_jimenez_fourteen(texel_size: f32, blur_size: f32) -> vec2<f32> {\n    let shadow_map_size = vec2<f32>(textureDimensions(view_bindings::directional_shadow_textures));\n\n    // Empirically chosen fudge factor to make PCF look better across different CSM cascades\n    let f = map(0.00390625, 0.022949219, 0.015, 0.035, texel_size);\n    return f * blur_size / (texel_size * shadow_map_size);\n}\n\nfn sample_shadow_map_jimenez_fourteen(\n    light_local: vec2<f32>,\n    depth: f32,\n    array_index: i32,\n    texel_size: f32,\n    blur_size: f32,\n    temporal: bool,\n) -> f32 {\n    let shadow_map_size = vec2<f32>(textureDimensions(view_bindings::directional_shadow_textures));\n    let rotation_matrix = random_rotation_matrix(light_local * shadow_map_size, temporal);\n    let uv_offset_scale = calculate_uv_offset_scale_jimenez_fourteen(texel_size, blur_size);\n\n    // https://www.iryoku.com/next-generation-post-processing-in-call-of-duty-advanced-warfare (slides 120-135)\n    let sample_offset0 = (rotation_matrix * utils::SPIRAL_OFFSET_0_) * uv_offset_scale;\n    let sample_offset1 = (rotation_matrix * utils::SPIRAL_OFFSET_1_) * uv_offset_scale;\n    let sample_offset2 = (rotation_matrix * utils::SPIRAL_OFFSET_2_) * uv_offset_scale;\n    let sample_offset3 = (rotation_matrix * utils::SPIRAL_OFFSET_3_) * uv_offset_scale;\n    let sample_offset4 = (rotation_matrix * utils::SPIRAL_OFFSET_4_) * uv_offset_scale;\n    let sample_offset5 = (rotation_matrix * utils::SPIRAL_OFFSET_5_) * uv_offset_scale;\n    let sample_offset6 = (rotation_matrix * utils::SPIRAL_OFFSET_6_) * uv_offset_scale;\n    let sample_offset7 = (rotation_matrix * utils::SPIRAL_OFFSET_7_) * uv_offset_scale;\n\n    var sum = 0.0;\n    sum += sample_shadow_map_hardware(light_local + sample_offset0, depth, array_index);\n    sum += sample_shadow_map_hardware(light_local + sample_offset1, depth, array_index);\n    sum += sample_shadow_map_hardware(light_local + sample_offset2, depth, array_index);\n    sum += sample_shadow_map_hardware(light_local + sample_offset3, depth, array_index);\n    sum += sample_shadow_map_hardware(light_local + sample_offset4, depth, array_index);\n    sum += sample_shadow_map_hardware(light_local + sample_offset5, depth, array_index);\n    sum += sample_shadow_map_hardware(light_local + sample_offset6, depth, array_index);\n    sum += sample_shadow_map_hardware(light_local + sample_offset7, depth, array_index);\n    return sum / 8.0;\n}\n\n// Performs the blocker search portion of percentage-closer soft shadows (PCSS).\n// This is the variation used for directional lights.\n//\n// We can't use Castano '13 here because that has a hard-wired fixed size, while\n// the PCSS algorithm requires a search size that varies based on the size of\n// the light. So we instead use the D3D sample point positions, spaced according\n// to the search size, to provide a sample pattern in a similar manner to the\n// cubemap sampling approach we use for PCF.\n//\n// `search_size` is the size of the search region in texels.\nfn search_for_blockers_in_shadow_map(\n    light_local: vec2<f32>,\n    depth: f32,\n    array_index: i32,\n    texel_size: f32,\n    search_size: f32,\n) -> f32 {\n    let shadow_map_size = vec2<f32>(textureDimensions(view_bindings::directional_shadow_textures));\n    let uv_offset_scale = search_size / (texel_size * shadow_map_size);\n\n    let offset0 = D3D_SAMPLE_POINT_POSITIONS[0] * uv_offset_scale;\n    let offset1 = D3D_SAMPLE_POINT_POSITIONS[1] * uv_offset_scale;\n    let offset2 = D3D_SAMPLE_POINT_POSITIONS[2] * uv_offset_scale;\n    let offset3 = D3D_SAMPLE_POINT_POSITIONS[3] * uv_offset_scale;\n    let offset4 = D3D_SAMPLE_POINT_POSITIONS[4] * uv_offset_scale;\n    let offset5 = D3D_SAMPLE_POINT_POSITIONS[5] * uv_offset_scale;\n    let offset6 = D3D_SAMPLE_POINT_POSITIONS[6] * uv_offset_scale;\n    let offset7 = D3D_SAMPLE_POINT_POSITIONS[7] * uv_offset_scale;\n\n    var sum = vec2(0.0);\n    sum += search_for_blockers_in_shadow_map_hardware(light_local + offset0, depth, array_index);\n    sum += search_for_blockers_in_shadow_map_hardware(light_local + offset1, depth, array_index);\n    sum += search_for_blockers_in_shadow_map_hardware(light_local + offset2, depth, array_index);\n    sum += search_for_blockers_in_shadow_map_hardware(light_local + offset3, depth, array_index);\n    sum += search_for_blockers_in_shadow_map_hardware(light_local + offset4, depth, array_index);\n    sum += search_for_blockers_in_shadow_map_hardware(light_local + offset5, depth, array_index);\n    sum += search_for_blockers_in_shadow_map_hardware(light_local + offset6, depth, array_index);\n    sum += search_for_blockers_in_shadow_map_hardware(light_local + offset7, depth, array_index);\n\n    if (sum.y == 0.0) {\n        return 0.0;\n    }\n    return sum.x / sum.y;\n}\n\nfn sample_shadow_map(light_local: vec2<f32>, depth: f32, array_index: i32, texel_size: f32) -> f32 {\n    @if(SHADOW_FILTER_METHOD_GAUSSIAN)\n    return sample_shadow_map_castano_thirteen(light_local, depth, array_index);\n    @elif(SHADOW_FILTER_METHOD_TEMPORAL)\n    return sample_shadow_map_jimenez_fourteen(\n        light_local, depth, array_index, texel_size, 1.0, true);\n    @elif(SHADOW_FILTER_METHOD_HARDWARE_2X2)\n    return sample_shadow_map_hardware(light_local, depth, array_index);\n    // This needs a default return value to avoid shader compilation errors if it's compiled with no SHADOW_FILTER_METHOD_* defined.\n    // (eg. if the normal prepass is enabled it ends up compiling this due to the normal prepass depending on pbr_functions, which depends on shadows)\n    // This should never actually get used, as anyone using bevy's lighting/shadows should always have a SHADOW_FILTER_METHOD defined.\n    // Set to 0 to make it obvious that something is wrong.\n    @else\n    return 0.0;\n}\n\n// Samples the shadow map for a directional light when percentage-closer soft\n// shadows are being used.\n//\n// We first search for a *blocker*, which is the average depth value of any\n// shadow map samples that are adjacent to the sample we're considering. That\n// allows us to determine the penumbra size; a larger gap between the blocker\n// and the depth of this sample results in a wider penumbra. Finally, we sample\n// the shadow map the same way we do in PCF, using that penumbra width.\n//\n// A good overview of the technique:\n// <https://medium.com/@varunm100/soft-shadows-for-mobile-ar-9e8da2e6f4ba>\nfn sample_shadow_map_pcss(\n    light_local: vec2<f32>,\n    depth: f32,\n    array_index: i32,\n    texel_size: f32,\n    light_size: f32,\n) -> f32 {\n    // Determine the average Z value of the closest blocker.\n    let z_blocker = search_for_blockers_in_shadow_map(\n        light_local, depth, array_index, texel_size, light_size);\n\n    // Don't let the blur size go below 0.5, or shadows will look unacceptably aliased.\n    let blur_size = max((z_blocker - depth) * light_size / depth, 0.5);\n\n    // FIXME: We can't use Castano '13 here because that has a hard-wired fixed\n    // size. So we instead use Jimenez '14 unconditionally. In the non-temporal\n    // variant this is unfortunately rather noisy. This may be improvable in the\n    // future by generating a mip chain of the shadow map and using that to\n    // provide better blurs.\n    @if(SHADOW_FILTER_METHOD_TEMPORAL)\n    return sample_shadow_map_jimenez_fourteen(\n        light_local, depth, array_index, texel_size, blur_size, true);\n    @else\n    return sample_shadow_map_jimenez_fourteen(\n        light_local, depth, array_index, texel_size, blur_size, false);\n}\n\n// NOTE: Due to the non-uniform control flow in `shadows::fetch_point_shadow`,\n// we must use the Level variant of textureSampleCompare to avoid undefined\n// behavior due to some of the fragments in a quad (2x2 fragments) being\n// processed not being sampled, and this messing with mip-mapping functionality.\n// The shadow maps have no mipmaps so Level just samples from LOD 0.\nfn sample_shadow_cubemap_hardware(light_local: vec3<f32>, depth: f32, light_id: u32) -> f32 {\n    @if(NO_CUBE_ARRAY_TEXTURES_SUPPORT)\n    return textureSampleCompare(\n        view_bindings::point_shadow_textures,\n        view_bindings::point_shadow_textures_comparison_sampler,\n        light_local,\n        depth\n    );\n    @else\n    return textureSampleCompareLevel(\n        view_bindings::point_shadow_textures,\n        view_bindings::point_shadow_textures_comparison_sampler,\n        light_local,\n        i32(light_id),\n        depth\n    );\n}\n\n// Performs one sample of the blocker search. This variation of the blocker\n// search function is for point and spot lights.\nfn search_for_blockers_in_shadow_cubemap_hardware(\n    light_local: vec3<f32>,\n    depth: f32,\n    light_id: u32,\n) -> vec2<f32> {\n    @if(WEBGL2) {\n        // Make sure that the WebGL 2 compiler doesn't see `sampled_depth` sampled\n        // with different samplers, or it'll blow up.\n        return vec2(0.0);\n    }\n    @else {\n        @if(PCSS_SAMPLERS_AVAILABLE) {\n            @if(NO_CUBE_ARRAY_TEXTURES_SUPPORT)\n            let sampled_depth = textureSample(\n                view_bindings::point_shadow_textures,\n                view_bindings::point_shadow_textures_linear_sampler,\n                light_local,\n            );\n            @else\n            let sampled_depth = textureSample(\n                view_bindings::point_shadow_textures,\n                view_bindings::point_shadow_textures_linear_sampler,\n                light_local,\n                i32(light_id),\n            );\n            return select(vec2(0.0), vec2(sampled_depth, 1.0), sampled_depth >= depth);\n        }\n        @else {\n            return vec2(0.0);\n        }\n    }\n}\n\nfn sample_shadow_cubemap_at_offset(\n    position: vec2<f32>,\n    coeff: f32,\n    x_basis: vec3<f32>,\n    y_basis: vec3<f32>,\n    light_local: vec3<f32>,\n    depth: f32,\n    light_id: u32,\n) -> f32 {\n    return sample_shadow_cubemap_hardware(\n        light_local + position.x * x_basis + position.y * y_basis,\n        depth,\n        light_id\n    ) * coeff;\n}\n\n// Computes the search position and performs one sample of the blocker search.\n// This variation of the blocker search function is for point and spot lights.\n//\n// `x_basis`, `y_basis`, and `light_local` form an orthonormal basis over which\n// the blocker search happens.\nfn search_for_blockers_in_shadow_cubemap_at_offset(\n    position: vec2<f32>,\n    x_basis: vec3<f32>,\n    y_basis: vec3<f32>,\n    light_local: vec3<f32>,\n    depth: f32,\n    light_id: u32,\n) -> vec2<f32> {\n    return search_for_blockers_in_shadow_cubemap_hardware(\n        light_local + position.x * x_basis + position.y * y_basis,\n        depth,\n        light_id\n    );\n}\n\n// This more or less does what Castano13 does, but in 3D space. Castano13 is\n// essentially an optimized 2D Gaussian filter that takes advantage of the\n// bilinear filtering hardware to reduce the number of samples needed. This\n// trick doesn't apply to cubemaps, so we manually apply a Gaussian filter over\n// the standard 8xMSAA pattern instead.\nfn sample_shadow_cubemap_gaussian(\n    light_local: vec3<f32>,\n    depth: f32,\n    scale: f32,\n    distance_to_light: f32,\n    light_id: u32,\n) -> f32 {\n    // Create an orthonormal basis so we can apply a 2D sampling pattern to a\n    // cubemap.\n    var up = vec3(0.0, 1.0, 0.0);\n    if (dot(up, normalize(light_local)) > 0.99) {\n        up = vec3(1.0, 0.0, 0.0);   // Avoid creating a degenerate basis.\n    }\n    let basis = orthonormalize(light_local, up) * scale * distance_to_light;\n\n    var sum: f32 = 0.0;\n    sum += sample_shadow_cubemap_at_offset(\n        D3D_SAMPLE_POINT_POSITIONS[0], D3D_SAMPLE_POINT_COEFFS[0],\n        basis[0], basis[1], light_local, depth, light_id);\n    sum += sample_shadow_cubemap_at_offset(\n        D3D_SAMPLE_POINT_POSITIONS[1], D3D_SAMPLE_POINT_COEFFS[1],\n        basis[0], basis[1], light_local, depth, light_id);\n    sum += sample_shadow_cubemap_at_offset(\n        D3D_SAMPLE_POINT_POSITIONS[2], D3D_SAMPLE_POINT_COEFFS[2],\n        basis[0], basis[1], light_local, depth, light_id);\n    sum += sample_shadow_cubemap_at_offset(\n        D3D_SAMPLE_POINT_POSITIONS[3], D3D_SAMPLE_POINT_COEFFS[3],\n        basis[0], basis[1], light_local, depth, light_id);\n    sum += sample_shadow_cubemap_at_offset(\n        D3D_SAMPLE_POINT_POSITIONS[4], D3D_SAMPLE_POINT_COEFFS[4],\n        basis[0], basis[1], light_local, depth, light_id);\n    sum += sample_shadow_cubemap_at_offset(\n        D3D_SAMPLE_POINT_POSITIONS[5], D3D_SAMPLE_POINT_COEFFS[5],\n        basis[0], basis[1], light_local, depth, light_id);\n    sum += sample_shadow_cubemap_at_offset(\n        D3D_SAMPLE_POINT_POSITIONS[6], D3D_SAMPLE_POINT_COEFFS[6],\n        basis[0], basis[1], light_local, depth, light_id);\n    sum += sample_shadow_cubemap_at_offset(\n        D3D_SAMPLE_POINT_POSITIONS[7], D3D_SAMPLE_POINT_COEFFS[7],\n        basis[0], basis[1], light_local, depth, light_id);\n    return sum;\n}\n\n// This is a port of the Jimenez14 filter above to the 3D space. It jitters the\n// points in the spiral pattern after first creating a 2D orthonormal basis\n// along the principal light direction.\nfn sample_shadow_cubemap_jittered(\n    light_local: vec3<f32>,\n    depth: f32,\n    scale: f32,\n    distance_to_light: f32,\n    light_id: u32,\n    temporal: bool,\n) -> f32 {\n    // Create an orthonormal basis so we can apply a 2D sampling pattern to a\n    // cubemap.\n    var up = vec3(0.0, 1.0, 0.0);\n    if (dot(up, normalize(light_local)) > 0.99) {\n        up = vec3(1.0, 0.0, 0.0);   // Avoid creating a degenerate basis.\n    }\n    let basis = orthonormalize(light_local, up) * scale * distance_to_light;\n\n    let rotation_matrix = random_rotation_matrix(vec2(1.0), temporal);\n\n    let sample_offset0 = rotation_matrix * utils::SPIRAL_OFFSET_0_ *\n        POINT_SHADOW_TEMPORAL_OFFSET_SCALE;\n    let sample_offset1 = rotation_matrix * utils::SPIRAL_OFFSET_1_ *\n        POINT_SHADOW_TEMPORAL_OFFSET_SCALE;\n    let sample_offset2 = rotation_matrix * utils::SPIRAL_OFFSET_2_ *\n        POINT_SHADOW_TEMPORAL_OFFSET_SCALE;\n    let sample_offset3 = rotation_matrix * utils::SPIRAL_OFFSET_3_ *\n        POINT_SHADOW_TEMPORAL_OFFSET_SCALE;\n    let sample_offset4 = rotation_matrix * utils::SPIRAL_OFFSET_4_ *\n        POINT_SHADOW_TEMPORAL_OFFSET_SCALE;\n    let sample_offset5 = rotation_matrix * utils::SPIRAL_OFFSET_5_ *\n        POINT_SHADOW_TEMPORAL_OFFSET_SCALE;\n    let sample_offset6 = rotation_matrix * utils::SPIRAL_OFFSET_6_ *\n        POINT_SHADOW_TEMPORAL_OFFSET_SCALE;\n    let sample_offset7 = rotation_matrix * utils::SPIRAL_OFFSET_7_ *\n        POINT_SHADOW_TEMPORAL_OFFSET_SCALE;\n\n    var sum: f32 = 0.0;\n    sum += sample_shadow_cubemap_at_offset(\n        sample_offset0, 0.125, basis[0], basis[1], light_local, depth, light_id);\n    sum += sample_shadow_cubemap_at_offset(\n        sample_offset1, 0.125, basis[0], basis[1], light_local, depth, light_id);\n    sum += sample_shadow_cubemap_at_offset(\n        sample_offset2, 0.125, basis[0], basis[1], light_local, depth, light_id);\n    sum += sample_shadow_cubemap_at_offset(\n        sample_offset3, 0.125, basis[0], basis[1], light_local, depth, light_id);\n    sum += sample_shadow_cubemap_at_offset(\n        sample_offset4, 0.125, basis[0], basis[1], light_local, depth, light_id);\n    sum += sample_shadow_cubemap_at_offset(\n        sample_offset5, 0.125, basis[0], basis[1], light_local, depth, light_id);\n    sum += sample_shadow_cubemap_at_offset(\n        sample_offset6, 0.125, basis[0], basis[1], light_local, depth, light_id);\n    sum += sample_shadow_cubemap_at_offset(\n        sample_offset7, 0.125, basis[0], basis[1], light_local, depth, light_id);\n    return sum;\n}\n\nfn sample_shadow_cubemap(\n    light_local: vec3<f32>,\n    distance_to_light: f32,\n    depth: f32,\n    light_id: u32,\n) -> f32 {\n    @if(SHADOW_FILTER_METHOD_GAUSSIAN)\n    return sample_shadow_cubemap_gaussian(\n        light_local, depth, POINT_SHADOW_SCALE, distance_to_light, light_id);\n    @elif(SHADOW_FILTER_METHOD_TEMPORAL)\n    return sample_shadow_cubemap_jittered(\n        light_local, depth, POINT_SHADOW_SCALE, distance_to_light, light_id, true);\n    @elif(SHADOW_FILTER_METHOD_HARDWARE_2X2)\n    return sample_shadow_cubemap_hardware(light_local, depth, light_id);\n    // This needs a default return value to avoid shader compilation errors if it's compiled with no SHADOW_FILTER_METHOD_* defined.\n    // (eg. if the normal prepass is enabled it ends up compiling this due to the normal prepass depending on pbr_functions, which depends on shadows)\n    // This should never actually get used, as anyone using bevy's lighting/shadows should always have a SHADOW_FILTER_METHOD defined.\n    // Set to 0 to make it obvious that something is wrong.\n    @else\n    return 0.0;\n}\n\n// Searches for PCSS blockers in a cubemap. This is the variant of the blocker\n// search used for point and spot lights.\n//\n// This follows the logic in `sample_shadow_cubemap_gaussian`, but uses linear\n// sampling instead of percentage-closer filtering.\n//\n// The `scale` parameter represents the size of the light.\nfn search_for_blockers_in_shadow_cubemap(\n    light_local: vec3<f32>,\n    depth: f32,\n    scale: f32,\n    distance_to_light: f32,\n    light_id: u32,\n) -> f32 {\n    // Create an orthonormal basis so we can apply a 2D sampling pattern to a\n    // cubemap.\n    var up = vec3(0.0, 1.0, 0.0);\n    if (dot(up, normalize(light_local)) > 0.99) {\n        up = vec3(1.0, 0.0, 0.0);   // Avoid creating a degenerate basis.\n    }\n    let basis = orthonormalize(light_local, up) * scale * distance_to_light;\n\n    var sum: vec2<f32> = vec2(0.0);\n    sum += search_for_blockers_in_shadow_cubemap_at_offset(\n        D3D_SAMPLE_POINT_POSITIONS[0], basis[0], basis[1], light_local, depth, light_id);\n    sum += search_for_blockers_in_shadow_cubemap_at_offset(\n        D3D_SAMPLE_POINT_POSITIONS[1], basis[0], basis[1], light_local, depth, light_id);\n    sum += search_for_blockers_in_shadow_cubemap_at_offset(\n        D3D_SAMPLE_POINT_POSITIONS[2], basis[0], basis[1], light_local, depth, light_id);\n    sum += search_for_blockers_in_shadow_cubemap_at_offset(\n        D3D_SAMPLE_POINT_POSITIONS[3], basis[0], basis[1], light_local, depth, light_id);\n    sum += search_for_blockers_in_shadow_cubemap_at_offset(\n        D3D_SAMPLE_POINT_POSITIONS[4], basis[0], basis[1], light_local, depth, light_id);\n    sum += search_for_blockers_in_shadow_cubemap_at_offset(\n        D3D_SAMPLE_POINT_POSITIONS[5], basis[0], basis[1], light_local, depth, light_id);\n    sum += search_for_blockers_in_shadow_cubemap_at_offset(\n        D3D_SAMPLE_POINT_POSITIONS[6], basis[0], basis[1], light_local, depth, light_id);\n    sum += search_for_blockers_in_shadow_cubemap_at_offset(\n        D3D_SAMPLE_POINT_POSITIONS[7], basis[0], basis[1], light_local, depth, light_id);\n\n    if (sum.y == 0.0) {\n        return 0.0;\n    }\n    return sum.x / sum.y;\n}\n\n// Samples the shadow map for a point or spot light when percentage-closer soft\n// shadows are being used.\n//\n// A good overview of the technique:\n// <https://medium.com/@varunm100/soft-shadows-for-mobile-ar-9e8da2e6f4ba>\nfn sample_shadow_cubemap_pcss(\n    light_local: vec3<f32>,\n    distance_to_light: f32,\n    depth: f32,\n    light_id: u32,\n    light_size: f32,\n) -> f32 {\n    let z_blocker = search_for_blockers_in_shadow_cubemap(\n        light_local, depth, light_size, distance_to_light, light_id);\n\n    // Don't let the blur size go below 0.5, or shadows will look unacceptably aliased.\n    let blur_size = max((z_blocker - depth) * light_size / depth, 0.5);\n\n    @if(SHADOW_FILTER_METHOD_TEMPORAL)\n    return sample_shadow_cubemap_jittered(\n        light_local, depth, POINT_SHADOW_SCALE * blur_size, distance_to_light, light_id, true);\n    @else\n    return sample_shadow_cubemap_jittered(\n        light_local, depth, POINT_SHADOW_SCALE * blur_size, distance_to_light, light_id, false);\n}\n","bevy::pbr::shadows":"import package::pbr::{\n    mesh_view_types::POINT_LIGHT_FLAGS_SPOT_LIGHT_Y_NEGATIVE,\n    mesh_view_bindings as view_bindings,\n    shadow_sampling::{\n        SPOT_SHADOW_TEXEL_SIZE, sample_shadow_cubemap, sample_shadow_cubemap_pcss,\n        sample_shadow_map, sample_shadow_map_pcss,\n    }\n};\n\nimport package::render::{\n    color_operations::hsv_to_rgb,\n    maths::PI_2\n};\n\nconst flip_z: vec3<f32> = vec3<f32>(1.0, 1.0, -1.0);\n\nfn fetch_point_shadow(light_id: u32, frag_position: vec4<f32>, surface_normal: vec3<f32>) -> f32 {\n    let light = &view_bindings::clusterable_objects.data[light_id];\n\n    // because the shadow maps align with the axes and the frustum planes are at 45 degrees\n    // we can get the worldspace depth by taking the largest absolute axis\n    let surface_to_light = (*light).position_radius.xyz - frag_position.xyz;\n    let surface_to_light_abs = abs(surface_to_light);\n    let distance_to_light = max(surface_to_light_abs.x, max(surface_to_light_abs.y, surface_to_light_abs.z));\n\n    // The normal bias here is already scaled by the texel size at 1 world unit from the light.\n    // The texel size increases proportionally with distance from the light so multiplying by\n    // distance to light scales the normal bias to the texel size at the fragment distance.\n    let normal_offset = (*light).shadow_normal_bias * distance_to_light * surface_normal.xyz;\n    let depth_offset = (*light).shadow_depth_bias * normalize(surface_to_light.xyz);\n    let offset_position = frag_position.xyz + normal_offset + depth_offset;\n\n    // similar largest-absolute-axis trick as above, but now with the offset fragment position\n    let frag_ls = offset_position.xyz - (*light).position_radius.xyz ;\n    let abs_position_ls = abs(frag_ls);\n    let major_axis_magnitude = max(abs_position_ls.x, max(abs_position_ls.y, abs_position_ls.z));\n\n    // NOTE: These simplifications come from multiplying:\n    // projection * vec4(0, 0, -major_axis_magnitude, 1.0)\n    // and keeping only the terms that have any impact on the depth.\n    // Projection-agnostic approach:\n    let zw = -major_axis_magnitude * (*light).light_custom_data.xy + (*light).light_custom_data.zw;\n    let depth = zw.x / zw.y;\n\n    // If soft shadows are enabled, use the PCSS path. Cubemaps assume a\n    // left-handed coordinate space, so we have to flip the z-axis when\n    // sampling.\n    if ((*light).soft_shadow_size > 0.0) {\n        return sample_shadow_cubemap_pcss(\n            frag_ls * flip_z,\n            distance_to_light,\n            depth,\n            light_id,\n            (*light).soft_shadow_size,\n        );\n    }\n\n    // Do the lookup, using HW PCF and comparison. Cubemaps assume a left-handed\n    // coordinate space, so we have to flip the z-axis when sampling.\n    return sample_shadow_cubemap(frag_ls * flip_z, distance_to_light, depth, light_id);\n}\n\nfn fetch_spot_shadow(\n    light_id: u32,\n    frag_position: vec4<f32>,\n    surface_normal: vec3<f32>,\n    near_z: f32,\n) -> f32 {\n    let light = &view_bindings::clusterable_objects.data[light_id];\n\n    let surface_to_light = (*light).position_radius.xyz - frag_position.xyz;\n\n    // construct the light view matrix\n    var spot_dir = vec3<f32>((*light).light_custom_data.x, 0.0, (*light).light_custom_data.y);\n    // reconstruct spot dir from x/z and y-direction flag\n    spot_dir.y = sqrt(max(0.0, 1.0 - spot_dir.x * spot_dir.x - spot_dir.z * spot_dir.z));\n    if (((*light).flags & POINT_LIGHT_FLAGS_SPOT_LIGHT_Y_NEGATIVE) != 0u) {\n        spot_dir.y = -spot_dir.y;\n    }\n\n    // view matrix z_axis is the reverse of transform.forward()\n    let fwd = -spot_dir;\n    let distance_to_light = dot(fwd, surface_to_light);\n    let offset_position =\n        -surface_to_light\n        + ((*light).shadow_depth_bias * normalize(surface_to_light))\n        + (surface_normal.xyz * (*light).shadow_normal_bias) * distance_to_light;\n\n    // the construction of the up and right vectors needs to precisely mirror the code\n    // in render/light.rs:spot_light_view_matrix\n    var sign = -1.0;\n    if (fwd.z >= 0.0) {\n        sign = 1.0;\n    }\n    let a = -1.0 / (fwd.z + sign);\n    let b = fwd.x * fwd.y * a;\n    let up_dir = vec3<f32>(1.0 + sign * fwd.x * fwd.x * a, sign * b, -sign * fwd.x);\n    let right_dir = vec3<f32>(-b, -sign - fwd.y * fwd.y * a, fwd.y);\n    let light_inv_rot = mat3x3<f32>(right_dir, up_dir, fwd);\n\n    // because the matrix is a pure rotation matrix, the inverse is just the transpose, and to calculate\n    // the product of the transpose with a vector we can just post-multiply instead of pre-multiplying.\n    // this allows us to keep the matrix construction code identical between CPU and GPU.\n    let projected_position = offset_position * light_inv_rot;\n\n    // divide xy by perspective matrix \"f\" and by -projected.z (projected.z is -projection matrix's w)\n    // to get ndc coordinates\n    let f_div_minus_z = 1.0 / ((*light).spot_light_tan_angle * -projected_position.z);\n    let shadow_xy_ndc = projected_position.xy * f_div_minus_z;\n    // convert to uv coordinates\n    let shadow_uv = shadow_xy_ndc * vec2<f32>(0.5, -0.5) + vec2<f32>(0.5, 0.5);\n\n    let depth = near_z / -projected_position.z;\n\n    // If soft shadows are enabled, use the PCSS path.\n    let array_index = i32(light_id) + view_bindings::lights.spot_light_shadowmap_offset;\n    if ((*light).soft_shadow_size > 0.0) {\n        return sample_shadow_map_pcss(\n            shadow_uv, depth, array_index, SPOT_SHADOW_TEXEL_SIZE, (*light).soft_shadow_size);\n    }\n\n    return sample_shadow_map(shadow_uv, depth, array_index, SPOT_SHADOW_TEXEL_SIZE);\n}\n\nfn get_cascade_index(light_id: u32, view_z: f32) -> u32 {\n    let light = &view_bindings::lights.directional_lights[light_id];\n\n    for (var i: u32 = 0u; i < (*light).num_cascades; i = i + 1u) {\n        if (-view_z < (*light).cascades[i].far_bound) {\n            return i;\n        }\n    }\n    return (*light).num_cascades;\n}\n\n// Converts from world space to the uv position in the light's shadow map.\n//\n// The depth is stored in the return value's z coordinate. If the return value's\n// w coordinate is 0.0, then we landed outside the shadow map entirely.\nfn world_to_directional_light_local(\n    light_id: u32,\n    cascade_index: u32,\n    offset_position: vec4<f32>\n) -> vec4<f32> {\n    let light = &view_bindings::lights.directional_lights[light_id];\n    let cascade = &(*light).cascades[cascade_index];\n\n    let offset_position_clip = (*cascade).clip_from_world * offset_position;\n    if (offset_position_clip.w <= 0.0) {\n        return vec4(0.0);\n    }\n    let offset_position_ndc = offset_position_clip.xyz / offset_position_clip.w;\n    // No shadow outside the orthographic projection volume\n    if (any(offset_position_ndc.xy < vec2<f32>(-1.0)) || offset_position_ndc.z < 0.0\n            || any(offset_position_ndc > vec3<f32>(1.0))) {\n        return vec4(0.0);\n    }\n\n    // compute texture coordinates for shadow lookup, compensating for the Y-flip difference\n    // between the NDC and texture coordinates\n    let flip_correction = vec2<f32>(0.5, -0.5);\n    let light_local = offset_position_ndc.xy * flip_correction + vec2<f32>(0.5, 0.5);\n\n    let depth = offset_position_ndc.z;\n\n    return vec4(light_local, depth, 1.0);\n}\n\nfn sample_directional_cascade(\n    light_id: u32,\n    cascade_index: u32,\n    frag_position: vec4<f32>,\n    surface_normal: vec3<f32>,\n) -> f32 {\n    let light = &view_bindings::lights.directional_lights[light_id];\n    let cascade = &(*light).cascades[cascade_index];\n\n    // The normal bias is scaled to the texel size.\n    let normal_offset = (*light).shadow_normal_bias * (*cascade).texel_size * surface_normal.xyz;\n    let depth_offset = (*light).shadow_depth_bias * (*light).direction_to_light.xyz;\n    let offset_position = vec4<f32>(frag_position.xyz + normal_offset + depth_offset, frag_position.w);\n\n    let light_local = world_to_directional_light_local(light_id, cascade_index, offset_position);\n    if (light_local.w == 0.0) {\n        return 1.0;\n    }\n\n    let array_index = i32((*light).depth_texture_base_index + cascade_index);\n    let texel_size = (*cascade).texel_size;\n\n    // If soft shadows are enabled, use the PCSS path.\n    if ((*light).soft_shadow_size > 0.0) {\n        return sample_shadow_map_pcss(\n            light_local.xy, light_local.z, array_index, texel_size, (*light).soft_shadow_size);\n    }\n\n    return sample_shadow_map(light_local.xy, light_local.z, array_index, texel_size);\n}\n\nfn fetch_directional_shadow(light_id: u32, frag_position: vec4<f32>, surface_normal: vec3<f32>, view_z: f32) -> f32 {\n    let light = &view_bindings::lights.directional_lights[light_id];\n    let cascade_index = get_cascade_index(light_id, view_z);\n\n    if (cascade_index >= (*light).num_cascades) {\n        return 1.0;\n    }\n\n    var shadow = sample_directional_cascade(light_id, cascade_index, frag_position, surface_normal);\n\n    // Blend with the next cascade, if there is one.\n    let next_cascade_index = cascade_index + 1u;\n    if (next_cascade_index < (*light).num_cascades) {\n        let this_far_bound = (*light).cascades[cascade_index].far_bound;\n        let next_near_bound = (1.0 - (*light).cascades_overlap_proportion) * this_far_bound;\n        if (-view_z >= next_near_bound) {\n            let next_shadow = sample_directional_cascade(light_id, next_cascade_index, frag_position, surface_normal);\n            shadow = mix(shadow, next_shadow, (-view_z - next_near_bound) / (this_far_bound - next_near_bound));\n        }\n    }\n    return shadow;\n}\n\nfn cascade_debug_visualization(\n    output_color: vec3<f32>,\n    light_id: u32,\n    view_z: f32,\n) -> vec3<f32> {\n    let overlay_alpha = 0.95;\n    let cascade_index = get_cascade_index(light_id, view_z);\n    let cascade_color_hsv = vec3(\n        f32(cascade_index) / f32(u32(constants::MAX_CASCADES_PER_LIGHT) + 1u) * PI_2,\n        1.0,\n        0.5\n    );\n    let cascade_color = hsv_to_rgb(cascade_color_hsv);\n    return vec3<f32>(\n        (1.0 - overlay_alpha) * output_color.rgb + overlay_alpha * cascade_color\n    );\n}\n","bevy::pbr::skinning":"import package::pbr::mesh_types::SkinnedMesh;\nimport package::pbr::mesh_bindings::mesh;\n\n@if(!SKINNED)\nconst module_requires_flag_SKINNED = false;\n@if(!SKINNED)\nconst_assert module_requires_flag_SKINNED; // module requires feature flag SKINNED\n\n@if(SKINS_USE_UNIFORM_BUFFERS)\n@group(1) @binding(1) var<uniform> joint_matrices: SkinnedMesh;\n@else\n@group(1) @binding(1) var<storage> joint_matrices: array<mat4x4<f32>>;\n\n// An array of matrices specifying the joint positions from the previous frame.\n//\n// This is used for motion vector computation.\n//\n// If this is the first frame, or we're otherwise prevented from using data from\n// the previous frame, this is simply the same as `joint_matrices` above.\n@if(SKINS_USE_UNIFORM_BUFFERS)\n@group(1) @binding(6) var<uniform> prev_joint_matrices: SkinnedMesh;\n@else\n@group(1) @binding(6) var<storage> prev_joint_matrices: array<mat4x4<f32>>;\n\nfn skin_model(\n    indexes: vec4<u32>,\n    weights: vec4<f32>,\n    instance_index: u32,\n) -> mat4x4<f32> {\n    @if(SKINS_USE_UNIFORM_BUFFERS) {\n        return weights.x * joint_matrices.data[indexes.x]\n            + weights.y * joint_matrices.data[indexes.y]\n            + weights.z * joint_matrices.data[indexes.z]\n            + weights.w * joint_matrices.data[indexes.w];\n    }\n    @else {\n        var skin_index = mesh[instance_index].current_skin_index;\n        return weights.x * joint_matrices[skin_index + indexes.x]\n            + weights.y * joint_matrices[skin_index + indexes.y]\n            + weights.z * joint_matrices[skin_index + indexes.z]\n            + weights.w * joint_matrices[skin_index + indexes.w];\n    }\n}\n\n// Returns the skinned position of a vertex with the given weights from the\n// previous frame.\n//\n// This is used for motion vector computation.\nfn skin_prev_model(\n    indexes: vec4<u32>,\n    weights: vec4<f32>,\n    instance_index: u32,\n) -> mat4x4<f32> {\n    @if(SKINS_USE_UNIFORM_BUFFERS) {\n        return weights.x * prev_joint_matrices.data[indexes.x]\n            + weights.y * prev_joint_matrices.data[indexes.y]\n            + weights.z * prev_joint_matrices.data[indexes.z]\n            + weights.w * prev_joint_matrices.data[indexes.w];\n    }\n    @else {\n        let skin_index = mesh[instance_index].current_skin_index;\n        return weights.x * prev_joint_matrices[skin_index + indexes.x]\n            + weights.y * prev_joint_matrices[skin_index + indexes.y]\n            + weights.z * prev_joint_matrices[skin_index + indexes.z]\n            + weights.w * prev_joint_matrices[skin_index + indexes.w];\n        \n    }\n}\n\nfn inverse_transpose_3x3m(in: mat3x3<f32>) -> mat3x3<f32> {\n    let x = cross(in[1], in[2]);\n    let y = cross(in[2], in[0]);\n    let z = cross(in[0], in[1]);\n    let det = dot(in[2], z);\n    return mat3x3<f32>(\n        x / det,\n        y / det,\n        z / det\n    );\n}\n\nfn skin_normals(\n    world_from_local: mat4x4<f32>,\n    normal: vec3<f32>,\n) -> vec3<f32> {\n    return normalize(\n        inverse_transpose_3x3m(\n            mat3x3<f32>(\n                world_from_local[0].xyz,\n                world_from_local[1].xyz,\n                world_from_local[2].xyz\n            )\n        ) * normal\n    );\n}\n","bevy::pbr::ssao_utils":"import package::render::maths::{PI, HALF_PI};\n\n// Approximates single-bounce ambient occlusion to multi-bounce ambient occlusion\n// https://blog.selfshadow.com/publications/s2016-shading-course/activision/s2016_pbs_activision_occlusion.pdf#page=78\nfn ssao_multibounce(visibility: f32, base_color: vec3<f32>) -> vec3<f32> {\n    let a = 2.0404 * base_color - 0.3324;\n    let b = -4.7951 * base_color + 0.6417;\n    let c = 2.7552 * base_color + 0.6903;\n    let x = vec3<f32>(visibility);\n    return max(x, ((x * a + b) * x + c) * x);\n}\n","bevy::pbr::ssr":"// A postprocessing pass that performs screen-space reflections.\n\nimport package::core_pipeline::fullscreen_vertex_shader::FullscreenVertexOutput;\nimport package::pbr::{\n    clustered_forward,\n    lighting,\n    lighting::{LAYER_BASE, LAYER_CLEARCOAT},\n    mesh_view_bindings::{view, depth_prepass_texture, deferred_prepass_texture, ssr_settings},\n    pbr_deferred_functions::pbr_input_from_deferred_gbuffer,\n    pbr_deferred_types,\n    pbr_functions,\n    prepass_utils,\n    raymarch::{\n        depth_ray_march_from_cs,\n        depth_ray_march_march,\n        depth_ray_march_new_from_depth,\n        depth_ray_march_to_ws_dir,\n    },\n    utils,\n    view_transformations::{\n        depth_ndc_to_view_z,\n        frag_coord_to_ndc,\n        ndc_to_frag_coord,\n        ndc_to_uv,\n        position_view_to_ndc,\n        position_world_to_ndc,\n        position_world_to_view,\n    },\n};\nimport package::render::view::View;\n\n@if(ENVIRONMENT_MAP)\nimport package::pbr::environment_map;\n\n// The texture representing the color framebuffer.\n@group(1) @binding(0) var color_texture: texture_2d<f32>;\n\n// The sampler that lets us sample from the color framebuffer.\n@group(1) @binding(1) var color_sampler: sampler;\n\n// Group 1, bindings 2 and 3 are in `raymarch.wgsl`.\n\n// Returns the reflected color in the RGB channel and the specular occlusion in\n// the alpha channel.\n//\n// The general approach here is similar to [1]. We first project the reflection\n// ray into screen space. Then we perform uniform steps along that screen-space\n// reflected ray, converting each step to view space.\n//\n// The arguments are:\n//\n// * `R_world`: The reflection vector in world space.\n//\n// * `P_world`: The current position in world space.\n//\n// [1]: https://lettier.github.io/3d-game-shaders-for-beginners/screen-space-reflection.html\nfn evaluate_ssr(R_world: vec3<f32>, P_world: vec3<f32>) -> vec4<f32> {\n    let depth_size = vec2<f32>(textureDimensions(depth_prepass_texture));\n\n    var raymarch = depth_ray_march_new_from_depth(depth_size);\n    depth_ray_march_from_cs(&raymarch, position_world_to_ndc(P_world));\n    depth_ray_march_to_ws_dir(&raymarch, normalize(R_world));\n    raymarch.linear_steps = ssr_settings.linear_steps;\n    raymarch.bisection_steps = ssr_settings.bisection_steps;\n    raymarch.use_secant = ssr_settings.use_secant != 0u;\n    raymarch.depth_thickness_linear_z = ssr_settings.thickness;\n    raymarch.jitter = 1.0;  // Disable jitter for now.\n    raymarch.march_behind_surfaces = false;\n\n    let raymarch_result = depth_ray_march_march(&raymarch);\n    if (raymarch_result.hit) {\n        return vec4(\n            textureSampleLevel(color_texture, color_sampler, raymarch_result.hit_uv, 0.0).rgb,\n            0.0\n        );\n    }\n\n    return vec4(0.0, 0.0, 0.0, 1.0);\n}\n\n@fragment\nfn fragment(in: FullscreenVertexOutput) -> @location(0) vec4<f32> {\n    // Sample the depth.\n    var frag_coord = in.position;\n    frag_coord.z = prepass_utils::prepass_depth(in.position, 0u);\n\n    // Load the G-buffer data.\n    let fragment = textureLoad(color_texture, vec2<i32>(frag_coord.xy), 0);\n    let gbuffer = textureLoad(deferred_prepass_texture, vec2<i32>(frag_coord.xy), 0);\n    let pbr_input = pbr_input_from_deferred_gbuffer(frag_coord, gbuffer);\n\n    // Don't do anything if the surface is too rough, since we can't blur or do\n    // temporal accumulation yet.\n    let perceptual_roughness = pbr_input.material.perceptual_roughness;\n    if (perceptual_roughness > ssr_settings.perceptual_roughness_threshold) {\n        return fragment;\n    }\n\n    // Unpack the PBR input.\n    var specular_occlusion = pbr_input.specular_occlusion;\n    let world_position = pbr_input.world_position.xyz;\n    let N = pbr_input.N;\n    let V = pbr_input.V;\n\n    // Calculate the reflection vector.\n    let R = reflect(-V, N);\n\n    // Do the raymarching.\n    let ssr_specular = evaluate_ssr(R, world_position);\n    var indirect_light = ssr_specular.rgb;\n    specular_occlusion *= ssr_specular.a;\n\n    // Sample the environment map if necessary.\n    //\n    // This will take the specular part of the environment map into account if\n    // the ray missed. Otherwise, it only takes the diffuse part.\n    //\n    // TODO: Merge this with the duplicated code in `apply_pbr_lighting`.\n    @if(ENVIRONMENT_MAP) {\n        // Unpack values required for environment mapping.\n        let base_color = pbr_input.material.base_color.rgb;\n        let metallic = pbr_input.material.metallic;\n        let reflectance = pbr_input.material.reflectance;\n        let specular_transmission = pbr_input.material.specular_transmission;\n        let diffuse_transmission = pbr_input.material.diffuse_transmission;\n        let diffuse_occlusion = pbr_input.diffuse_occlusion;\n\n        // Do the above calculations again for the clearcoat layer. Remember that\n        // the clearcoat can have its own roughness and its own normal.\n        // TODO(mbr): this is not ideal\n        @if(STANDARD_MATERIAL_CLEARCOAT)\n        let clearcoat = pbr_input.material.clearcoat;\n        @if(STANDARD_MATERIAL_CLEARCOAT)\n        let clearcoat_perceptual_roughness = pbr_input.material.clearcoat_perceptual_roughness;\n        @if(STANDARD_MATERIAL_CLEARCOAT)\n        let clearcoat_roughness = lighting::perceptualRoughnessToRoughness(clearcoat_perceptual_roughness);\n        @if(STANDARD_MATERIAL_CLEARCOAT)\n        let clearcoat_N = pbr_input.clearcoat_N;\n        @if(STANDARD_MATERIAL_CLEARCOAT)\n        let clearcoat_NdotV = max(dot(clearcoat_N, pbr_input.V), 0.0001);\n        @if(STANDARD_MATERIAL_CLEARCOAT)\n        let clearcoat_R = reflect(-pbr_input.V, clearcoat_N);\n\n        // Calculate various other values needed for environment mapping.\n        let roughness = lighting::perceptualRoughnessToRoughness(perceptual_roughness);\n        let diffuse_color = pbr_functions::calculate_diffuse_color(\n            base_color,\n            metallic,\n            specular_transmission,\n            diffuse_transmission\n        );\n        let NdotV = max(dot(N, V), 0.0001);\n        let F_ab = lighting::F_AB(perceptual_roughness, NdotV);\n        let F0 = pbr_functions::calculate_F0(base_color, metallic, reflectance);\n\n        // Pack all the values into a structure.\n        var lighting_input: lighting::LightingInput;\n        lighting_input.layers[LAYER_BASE].NdotV = NdotV;\n        lighting_input.layers[LAYER_BASE].N = N;\n        lighting_input.layers[LAYER_BASE].R = R;\n        lighting_input.layers[LAYER_BASE].perceptual_roughness = perceptual_roughness;\n        lighting_input.layers[LAYER_BASE].roughness = roughness;\n        lighting_input.P = world_position.xyz;\n        lighting_input.V = V;\n        lighting_input.diffuse_color = diffuse_color;\n        lighting_input.F0_ = F0;\n        lighting_input.F_ab = F_ab;\n        @if(STANDARD_MATERIAL_CLEARCOAT) {\n            lighting_input.layers[LAYER_CLEARCOAT].NdotV = clearcoat_NdotV;\n            lighting_input.layers[LAYER_CLEARCOAT].N = clearcoat_N;\n            lighting_input.layers[LAYER_CLEARCOAT].R = clearcoat_R;\n            lighting_input.layers[LAYER_CLEARCOAT].perceptual_roughness = clearcoat_perceptual_roughness;\n            lighting_input.layers[LAYER_CLEARCOAT].roughness = clearcoat_roughness;\n            lighting_input.clearcoat_strength = clearcoat;\n        }\n\n        // Determine which cluster we're in. We'll need this to find the right\n        // reflection probe.\n        let cluster_index = clustered_forward::fragment_cluster_index(\n            frag_coord.xy, frag_coord.z, false);\n        var clusterable_object_index_ranges =\n            clustered_forward::unpack_clusterable_object_index_ranges(cluster_index);\n\n        // Sample the environment map.\n        let environment_light = environment_map::environment_map_light(\n            &lighting_input, &clusterable_object_index_ranges, false);\n\n        // Accumulate the environment map light.\n        indirect_light += view.exposure *\n            (environment_light.diffuse * diffuse_occlusion +\n            environment_light.specular * specular_occlusion);\n    }\n\n    // Write the results.\n    return vec4(fragment.rgb + indirect_light, 1.0);\n}\n","bevy::pbr::transmission":"import package::pbr::{\n    lighting,\n    prepass_utils,\n    utils::interleaved_gradient_noise,\n    utils,\n    mesh_view_bindings as view_bindings,\n};\n\nimport package::render::maths::PI;\n\n@if(TONEMAP_IN_SHADER)\nimport package::core_pipeline::tonemapping::approximate_inverse_tone_mapping;\n\nfn specular_transmissive_light(world_position: vec4<f32>, frag_coord: vec3<f32>, view_z: f32, N: vec3<f32>, V: vec3<f32>, F0: vec3<f32>, ior: f32, thickness: f32, perceptual_roughness: f32, specular_transmissive_color: vec3<f32>, transmitted_environment_light_specular: vec3<f32>) -> vec3<f32> {\n    // Calculate the ratio between refraction indexes. Assume air/vacuum for the space outside the mesh\n    let eta = 1.0 / ior;\n\n    // Calculate incidence vector (opposite to view vector) and its dot product with the mesh normal\n    let I = -V;\n    let NdotI = dot(N, I);\n\n    // Calculate refracted direction using Snell's law\n    let k = 1.0 - eta * eta * (1.0 - NdotI * NdotI);\n    let T = eta * I - (eta * NdotI + sqrt(k)) * N;\n\n    // Calculate the exit position of the refracted ray, by propagating refracted direction through thickness\n    let exit_position = world_position.xyz + T * thickness;\n\n    // Transform exit_position into clip space\n    let clip_exit_position = view_bindings::view.clip_from_world * vec4<f32>(exit_position, 1.0);\n\n    // Scale / offset position so that coordinate is in right space for sampling transmissive background texture\n    let offset_position = (clip_exit_position.xy / clip_exit_position.w) * vec2<f32>(0.5, -0.5) + 0.5;\n\n    // Fetch background color\n    var background_color: vec4<f32>;\n    if perceptual_roughness == 0.0 {\n        // If the material has zero roughness, we can use a faster approach without the blur\n        background_color = fetch_transmissive_background_non_rough(offset_position, frag_coord);\n    } else {\n        background_color = fetch_transmissive_background(offset_position, frag_coord, view_z, perceptual_roughness);\n    }\n\n    // Compensate for exposure, since the background color is coming from an already exposure-adjusted texture\n    background_color = vec4(background_color.rgb / view_bindings::view.exposure, background_color.a);\n\n    // Dot product of the refracted direction with the exit normal (Note: We assume the exit normal is the entry normal but inverted)\n    let MinusNdotT = dot(-N, T);\n\n    // Calculate 1.0 - fresnel factor (how much light is _NOT_ reflected, i.e. how much is transmitted)\n    let F = vec3(1.0) - lighting::fresnel(F0, MinusNdotT);\n\n    // Calculate final color by applying fresnel multiplied specular transmissive color to a mix of background color and transmitted specular environment light\n    return F * specular_transmissive_color * mix(transmitted_environment_light_specular, background_color.rgb, background_color.a);\n};\n\nfn fetch_transmissive_background_non_rough(offset_position: vec2<f32>, frag_coord: vec3<f32>) -> vec4<f32> {\n    var background_color = textureSampleLevel(\n        view_bindings::view_transmission_texture,\n        view_bindings::view_transmission_sampler,\n        offset_position,\n        0.0\n    );\n\n    // Use depth prepass data to reject values that are in front of the current fragment\n    @if(DEPTH_PREPASS && !WEBGL2)\n    if prepass_utils::prepass_depth(vec4<f32>(offset_position * view_bindings::view.viewport.zw, 0.0, 0.0), 0u) > frag_coord.z {\n        background_color.a = 0.0;\n    }\n\n    @if(TONEMAP_IN_SHADER) {\n        background_color = approximate_inverse_tone_mapping(background_color, view_bindings::view.color_grading);\n    }\n\n    return background_color;\n}\n\nfn fetch_transmissive_background(offset_position: vec2<f32>, frag_coord: vec3<f32>, view_z: f32, perceptual_roughness: f32) -> vec4<f32> {\n    // Calculate view aspect ratio, used to scale offset so that it's proportionate\n    let aspect = view_bindings::view.viewport.z / view_bindings::view.viewport.w;\n\n    // Calculate how “blurry” the transmission should be.\n    // Blur is more or less eyeballed to look approximately “right”, since the “correct”\n    // approach would involve projecting many scattered rays and figuring out their individual\n    // exit positions. IRL, light rays can be scattered when entering/exiting a material (due to\n    // roughness) or inside the material (due to subsurface scattering). Here, we only consider\n    // the first scenario.\n    //\n    // Blur intensity is:\n    // - proportional to the square of `perceptual_roughness`\n    // - proportional to the inverse of view z\n    let blur_intensity = (perceptual_roughness * perceptual_roughness) / view_z;\n\n    @if(SCREEN_SPACE_SPECULAR_TRANSMISSION_BLUR_TAPS)\n    let num_taps = constants::SCREEN_SPACE_SPECULAR_TRANSMISSION_BLUR_TAPS; // Controlled by the `Camera3d::screen_space_specular_transmission_quality` property\n    @else\n    let num_taps = 8; // Fallback to 8 taps, if not specified\n\n    let num_spirals = i32(ceil(f32(num_taps) / 8.0));\n\n    @if(TEMPORAL_JITTER)\n    let random_angle = interleaved_gradient_noise(frag_coord.xy, view_bindings::globals.frame_count);\n    @else\n    let random_angle = interleaved_gradient_noise(frag_coord.xy, 0u);\n\n    // Pixel checkerboard pattern (helps make the interleaved gradient noise pattern less visible)\n    @if(TEMPORAL_JITTER)\n    let pixel_checkboard = (\n        // 0 or 1 on even/odd pixels, alternates every frame\n        (i32(frag_coord.x) + i32(frag_coord.y) + i32(view_bindings::globals.frame_count)) % 2\n    );\n    @else\n    let pixel_checkboard = (\n        // 0 or 1 on even/odd pixels\n        (i32(frag_coord.x) + i32(frag_coord.y)) % 2\n    );\n\n    var result = vec4<f32>(0.0);\n    for (var i: i32 = 0; i < num_taps; i = i + 1) {\n        let current_spiral = (i >> 3u);\n        let angle = (random_angle + f32(current_spiral) / f32(num_spirals)) * 2.0 * PI;\n        let m = vec2(sin(angle), cos(angle));\n        let rotation_matrix = mat2x2(\n            m.y, -m.x,\n            m.x, m.y\n        );\n\n        // Get spiral offset\n        var spiral_offset: vec2<f32>;\n        switch i & 7 {\n            // https://www.iryoku.com/next-generation-post-processing-in-call-of-duty-advanced-warfare (slides 120-135)\n            // TODO: Figure out a more reasonable way of doing this, as WGSL\n            // seems to only allow constant indexes into constant arrays at the moment.\n            // The downstream shader compiler should be able to optimize this into a single\n            // constant when unrolling the for loop, but it's still not ideal.\n            case 0: { spiral_offset = utils::SPIRAL_OFFSET_0_; } // Note: We go even first and then odd, so that the lowest\n            case 1: { spiral_offset = utils::SPIRAL_OFFSET_2_; } // quality possible (which does 4 taps) still does a full spiral\n            case 2: { spiral_offset = utils::SPIRAL_OFFSET_4_; } // instead of just the first half of it\n            case 3: { spiral_offset = utils::SPIRAL_OFFSET_6_; }\n            case 4: { spiral_offset = utils::SPIRAL_OFFSET_1_; }\n            case 5: { spiral_offset = utils::SPIRAL_OFFSET_3_; }\n            case 6: { spiral_offset = utils::SPIRAL_OFFSET_5_; }\n            case 7: { spiral_offset = utils::SPIRAL_OFFSET_7_; }\n            default: {}\n        }\n\n        // Make each consecutive spiral slightly smaller than the previous one\n        spiral_offset *= 1.0 - (0.5 * f32(current_spiral + 1) / f32(num_spirals));\n\n        // Rotate and correct for aspect ratio\n        let rotated_spiral_offset = (rotation_matrix * spiral_offset) * vec2(1.0, aspect);\n\n        // Calculate final offset position, with blur and spiral offset\n        let modified_offset_position = offset_position + rotated_spiral_offset * blur_intensity * (1.0 - f32(pixel_checkboard) * 0.1);\n\n        // Sample the view transmission texture at the offset position + noise offset, to get the background color\n        var sample = textureSampleLevel(\n            view_bindings::view_transmission_texture,\n            view_bindings::view_transmission_sampler,\n            modified_offset_position,\n            0.0\n        );\n\n        // Use depth prepass data to reject values that are in front of the current fragment\n        @if(DEPTH_PREPASS && !WEBGL2)\n        if prepass_utils::prepass_depth(vec4<f32>(modified_offset_position * view_bindings::view.viewport.zw, 0.0, 0.0), 0u) > frag_coord.z {\n            sample = vec4<f32>(0.0);\n        }\n\n        // As blur intensity grows higher, gradually limit *very bright* color RGB values towards a\n        // maximum length of 1.0 to prevent stray “firefly” pixel artifacts. This can potentially make\n        // very strong emissive meshes appear much dimmer, but the artifacts are noticeable enough to\n        // warrant this treatment.\n        let normalized_rgb = normalize(sample.rgb);\n        result += vec4(min(sample.rgb, normalized_rgb / saturate(blur_intensity / 2.0)), sample.a);\n    }\n\n    result /= f32(num_taps);\n\n    @if(TONEMAP_IN_SHADER) {\n        result = approximate_inverse_tone_mapping(result, view_bindings::view.color_grading);\n    }\n\n    return result;\n}\n","bevy::pbr::utils":"import package::pbr::rgb9e5;\n\n// Generates a random u32 in range [0, u32::MAX].\n//\n// `state` is a mutable reference to a u32 used as the seed.\n//\n// Values are generated via \"white noise\", with no correlation between values.\n// In shaders, you often want spatial and/or temporal correlation. Use a different RNG method for these use cases.\n//\n// https://www.pcg-random.org\n// https://www.reedbeta.com/blog/hash-functions-for-gpu-rendering\nfn rand_u(state: ptr<function, u32>) -> u32 {\n    *state = *state * 747796405u + 2891336453u;\n    let word = ((*state >> ((*state >> 28u) + 4u)) ^ *state) * 277803737u;\n    return (word >> 22u) ^ word;\n}\n\n// Generates a random f32 in range [0, 1.0].\nfn rand_f(state: ptr<function, u32>) -> f32 {\n    *state = *state * 747796405u + 2891336453u;\n    let word = ((*state >> ((*state >> 28u) + 4u)) ^ *state) * 277803737u;\n    return f32((word >> 22u) ^ word) * bitcast<f32>(0x2f800004u);\n}\n\n// Generates a random vec2<f32> where each value is in range [0, 1.0].\nfn rand_vec2f(state: ptr<function, u32>) -> vec2<f32> {\n    return vec2(rand_f(state), rand_f(state));\n}\n\n// Generates a random u32 in range [0, n).\nfn rand_range_u(n: u32, state: ptr<function, u32>) -> u32 {\n    return rand_u(state) % n;\n}\n\n// returns the (0-1, 0-1) position within the given viewport for the current buffer coords .\n// buffer coords can be obtained from `@builtin(position).xy`.\n// the view uniform struct contains the current camera viewport in `view.viewport`.\n// topleft = 0,0\nfn coords_to_viewport_uv(position: vec2<f32>, viewport: vec4<f32>) -> vec2<f32> {\n    return (position - viewport.xy) / viewport.zw;\n}\n\n// https://jcgt.org/published/0003/02/01/paper.pdf\n\n// For encoding normals or unit direction vectors as octahedral coordinates.\nfn octahedral_encode(v: vec3<f32>) -> vec2<f32> {\n    var n = v / (abs(v.x) + abs(v.y) + abs(v.z));\n    let octahedral_wrap = (1.0 - abs(n.yx)) * select(vec2(-1.0), vec2(1.0), n.xy > vec2f(0.0));\n    let n_xy = select(octahedral_wrap, n.xy, n.z >= 0.0);\n    return n_xy * 0.5 + 0.5;\n}\n\n// For decoding normals or unit direction vectors from octahedral coordinates.\nfn octahedral_decode(v: vec2<f32>) -> vec3<f32> {\n    let f = v * 2.0 - 1.0;\n    var n = octahedral_decode_signed(f);\n    return normalize(n);\n}\n\n// Like octahedral_decode, but for input in [-1, 1] instead of [0, 1].\nfn octahedral_decode_signed(v: vec2<f32>) -> vec3<f32> {\n    var n = vec3(v.xy, 1.0 - abs(v.x) - abs(v.y));\n    let t = saturate(-n.z);\n    let w = select(vec2(t), vec2(-t), n.xy >= vec2(0.0));\n    n = vec3(n.xy + w, n.z);\n    return normalize(n);\n}\n\n// https://blog.demofox.org/2022/01/01/interleaved-gradient-noise-a-different-kind-of-low-discrepancy-sequence\nfn interleaved_gradient_noise(pixel_coordinates: vec2<f32>, frame: u32) -> f32 {\n    let xy = pixel_coordinates + 5.588238 * f32(frame % 64u);\n    return fract(52.9829189 * fract(0.06711056 * xy.x + 0.00583715 * xy.y));\n}\n\n// https://www.iryoku.com/next-generation-post-processing-in-call-of-duty-advanced-warfare (slides 120-135)\n// TODO: Use an array here instead of a bunch of constants, once arrays work properly under DX12.\n// NOTE: The names have a final underscore to avoid the following error:\n// `Composable module identifiers must not require substitution according to naga writeback rules`\nconst SPIRAL_OFFSET_0_ = vec2<f32>(-0.7071,  0.7071);\nconst SPIRAL_OFFSET_1_ = vec2<f32>(-0.0000, -0.8750);\nconst SPIRAL_OFFSET_2_ = vec2<f32>( 0.5303,  0.5303);\nconst SPIRAL_OFFSET_3_ = vec2<f32>(-0.6250, -0.0000);\nconst SPIRAL_OFFSET_4_ = vec2<f32>( 0.3536, -0.3536);\nconst SPIRAL_OFFSET_5_ = vec2<f32>(-0.0000,  0.3750);\nconst SPIRAL_OFFSET_6_ = vec2<f32>(-0.1768, -0.1768);\nconst SPIRAL_OFFSET_7_ = vec2<f32>( 0.1250,  0.0000);\n","bevy::pbr::view_transformations":"import package::pbr::mesh_view_bindings as view_bindings;\nimport package::pbr::prepass_bindings;\n\n/// World space:\n/// +y is up\n\n/// View space:\n/// -z is forward, +x is right, +y is up\n/// Forward is from the camera position into the scene.\n/// (0.0, 0.0, -1.0) is linear distance of 1.0 in front of the camera's view relative to the camera's rotation\n/// (0.0, 1.0, 0.0) is linear distance of 1.0 above the camera's view relative to the camera's rotation\n\n/// NDC (normalized device coordinate):\n/// https://www.w3.org/TR/webgpu/#coordinate-systems\n/// (-1.0, -1.0) in NDC is located at the bottom-left corner of NDC\n/// (1.0, 1.0) in NDC is located at the top-right corner of NDC\n/// Z is depth where: \n///    1.0 is near clipping plane\n///    Perspective projection: 0.0 is inf far away\n///    Orthographic projection: 0.0 is far clipping plane\n\n/// UV space:\n/// 0.0, 0.0 is the top left\n/// 1.0, 1.0 is the bottom right\n\n\n// -----------------\n// TO WORLD --------\n// -----------------\n\n/// Convert a view space position to world space\nfn position_view_to_world(view_pos: vec3<f32>) -> vec3<f32> {\n    let world_pos = view_bindings::view.world_from_view * vec4(view_pos, 1.0);\n    return world_pos.xyz;\n}\n\n/// Convert a clip space position to world space\nfn position_clip_to_world(clip_pos: vec4<f32>) -> vec3<f32> {\n    let world_pos = view_bindings::view.world_from_clip * clip_pos;\n    return world_pos.xyz;\n}\n\n/// Convert a ndc space position to world space\nfn position_ndc_to_world(ndc_pos: vec3<f32>) -> vec3<f32> {\n    let world_pos = view_bindings::view.world_from_clip * vec4(ndc_pos, 1.0);\n    return world_pos.xyz / world_pos.w;\n}\n\n/// Convert a view space direction to world space\nfn direction_view_to_world(view_dir: vec3<f32>) -> vec3<f32> {\n    let world_dir = view_bindings::view.world_from_view * vec4(view_dir, 0.0);\n    return world_dir.xyz;\n}\n\n/// Convert a clip space direction to world space\nfn direction_clip_to_world(clip_dir: vec4<f32>) -> vec3<f32> {\n    let world_dir = view_bindings::view.world_from_clip * clip_dir;\n    return world_dir.xyz;\n}\n\n// -----------------\n// TO VIEW ---------\n// -----------------\n\n/// Convert a world space position to view space\nfn position_world_to_view(world_pos: vec3<f32>) -> vec3<f32> {\n    let view_pos = view_bindings::view.view_from_world * vec4(world_pos, 1.0);\n    return view_pos.xyz;\n}\n\n/// Convert a clip space position to view space\nfn position_clip_to_view(clip_pos: vec4<f32>) -> vec3<f32> {\n    let view_pos = view_bindings::view.view_from_clip * clip_pos;\n    return view_pos.xyz;\n}\n\n/// Convert a ndc space position to view space\nfn position_ndc_to_view(ndc_pos: vec3<f32>) -> vec3<f32> {\n    let view_pos = view_bindings::view.view_from_clip * vec4(ndc_pos, 1.0);\n    return view_pos.xyz / view_pos.w;\n}\n\n/// Convert a world space direction to view space\nfn direction_world_to_view(world_dir: vec3<f32>) -> vec3<f32> {\n    let view_dir = view_bindings::view.view_from_world * vec4(world_dir, 0.0);\n    return view_dir.xyz;\n}\n\n/// Convert a clip space direction to view space\nfn direction_clip_to_view(clip_dir: vec4<f32>) -> vec3<f32> {\n    let view_dir = view_bindings::view.view_from_clip * clip_dir;\n    return view_dir.xyz;\n}\n\n// -----------------\n// TO PREV. VIEW ---\n// -----------------\n\nfn position_world_to_prev_view(world_pos: vec3<f32>) -> vec3<f32> {\n    let view_pos = prepass_bindings::previous_view_uniforms.view_from_world *\n        vec4(world_pos, 1.0);\n    return view_pos.xyz;\n}\n\nfn position_world_to_prev_ndc(world_pos: vec3<f32>) -> vec3<f32> {\n    let ndc_pos = prepass_bindings::previous_view_uniforms.clip_from_world *\n        vec4(world_pos, 1.0);\n    return ndc_pos.xyz / ndc_pos.w;\n}\n\n// -----------------\n// TO CLIP ---------\n// -----------------\n\n/// Convert a world space position to clip space\nfn position_world_to_clip(world_pos: vec3<f32>) -> vec4<f32> {\n    let clip_pos = view_bindings::view.clip_from_world * vec4(world_pos, 1.0);\n    return clip_pos;\n}\n\n/// Convert a view space position to clip space\nfn position_view_to_clip(view_pos: vec3<f32>) -> vec4<f32> {\n    let clip_pos = view_bindings::view.clip_from_view * vec4(view_pos, 1.0);\n    return clip_pos;\n}\n\n/// Convert a world space direction to clip space\nfn direction_world_to_clip(world_dir: vec3<f32>) -> vec4<f32> {\n    let clip_dir = view_bindings::view.clip_from_world * vec4(world_dir, 0.0);\n    return clip_dir;\n}\n\n/// Convert a view space direction to clip space\nfn direction_view_to_clip(view_dir: vec3<f32>) -> vec4<f32> {\n    let clip_dir = view_bindings::view.clip_from_view * vec4(view_dir, 0.0);\n    return clip_dir;\n}\n\n// -----------------\n// TO NDC ----------\n// -----------------\n\n/// Convert a world space position to ndc space\nfn position_world_to_ndc(world_pos: vec3<f32>) -> vec3<f32> {\n    let ndc_pos = view_bindings::view.clip_from_world * vec4(world_pos, 1.0);\n    return ndc_pos.xyz / ndc_pos.w;\n}\n\n/// Convert a view space position to ndc space\nfn position_view_to_ndc(view_pos: vec3<f32>) -> vec3<f32> {\n    let ndc_pos = view_bindings::view.clip_from_view * vec4(view_pos, 1.0);\n    return ndc_pos.xyz / ndc_pos.w;\n}\n\n// -----------------\n// DEPTH -----------\n// -----------------\n\n/// Retrieve the perspective camera near clipping plane\nfn perspective_camera_near() -> f32 {\n    return view_bindings::view.clip_from_view[3][2];\n}\n\n/// Convert ndc depth to linear view z. \n/// Note: Depth values in front of the camera will be negative as -z is forward\nfn depth_ndc_to_view_z(ndc_depth: f32) -> f32 {\n    @if(VIEW_PROJECTION_PERSPECTIVE) {\n        return -perspective_camera_near() / ndc_depth;\n    }\n    @elif(VIEW_PROJECTION_ORTHOGRAPHIC) {\n        return -(view_bindings::view.clip_from_view[3][2] - ndc_depth) / view_bindings::view.clip_from_view[2][2];\n    }\n    @else {\n        let view_pos = view_bindings::view.view_from_clip * vec4(0.0, 0.0, ndc_depth, 1.0);\n        return view_pos.z / view_pos.w;\n    }\n}\n\n/// Convert linear view z to ndc depth. \n/// Note: View z input should be negative for values in front of the camera as -z is forward\nfn view_z_to_depth_ndc(view_z: f32) -> f32 {\n    @if(VIEW_PROJECTION_PERSPECTIVE) {\n        return -perspective_camera_near() / view_z;\n    }\n    @elif(VIEW_PROJECTION_ORTHOGRAPHIC) {\n        return view_bindings::view.clip_from_view[3][2] + view_z * view_bindings::view.clip_from_view[2][2];\n    }\n    @else {\n        let ndc_pos = view_bindings::view.clip_from_view * vec4(0.0, 0.0, view_z, 1.0);\n        return ndc_pos.z / ndc_pos.w;\n    }\n}\n\nfn prev_view_z_to_depth_ndc(view_z: f32) -> f32 {\n    @if(VIEW_PROJECTION_PERSPECTIVE) {\n        return -perspective_camera_near() / view_z;\n    }\n    @elif(VIEW_PROJECTION_ORTHOGRAPHIC) {\n        return prepass_bindings::previous_view_uniforms.clip_from_view[3][2] +\n            view_z * prepass_bindings::previous_view_uniforms.clip_from_view[2][2];\n    }\n    @else {\n        let ndc_pos = prepass_bindings::previous_view_uniforms.clip_from_view *\n            vec4(0.0, 0.0, view_z, 1.0);\n        return ndc_pos.z / ndc_pos.w;\n    }\n}\n\n// -----------------\n// UV --------------\n// -----------------\n\n/// Convert ndc space xy coordinate [-1.0 .. 1.0] to uv [0.0 .. 1.0]\nfn ndc_to_uv(ndc: vec2<f32>) -> vec2<f32> {\n    return ndc * vec2(0.5, -0.5) + vec2(0.5);\n}\n\n/// Convert uv [0.0 .. 1.0] coordinate to ndc space xy [-1.0 .. 1.0]\nfn uv_to_ndc(uv: vec2<f32>) -> vec2<f32> {\n    return uv * vec2(2.0, -2.0) + vec2(-1.0, 1.0);\n}\n\n/// returns the (0.0, 0.0) .. (1.0, 1.0) position within the viewport for the current render target\n/// [0 .. render target viewport size] eg. [(0.0, 0.0) .. (1280.0, 720.0)] to [(0.0, 0.0) .. (1.0, 1.0)]\nfn frag_coord_to_uv(frag_coord: vec2<f32>) -> vec2<f32> {\n    return (frag_coord - view_bindings::view.viewport.xy) / view_bindings::view.viewport.zw;\n}\n\n/// Convert frag coord to ndc\nfn frag_coord_to_ndc(frag_coord: vec4<f32>) -> vec3<f32> {\n    return vec3(uv_to_ndc(frag_coord_to_uv(frag_coord.xy)), frag_coord.z);\n}\n\n/// Convert ndc space xy coordinate [-1.0 .. 1.0] to [0 .. render target\n/// viewport size]\nfn ndc_to_frag_coord(ndc: vec2<f32>) -> vec2<f32> {\n    return ndc_to_uv(ndc) * view_bindings::view.viewport.zw;\n}\n","bevy::render::bindless":"@if(!BINDLESS)\nconst module_requires_flag_BINDLESS = false;\n@if(!BINDLESS)\nconst_assert module_requires_flag_BINDLESS; // module requires feature flag BINDLESS\n\n// Defines the common arrays used to access bindless resources.\n//\n// This need to be kept up to date with the `BINDING_NUMBERS` table in\n// `bindless.rs`.\n//\n// You access these by indexing into the bindless index table, and from there\n// indexing into the appropriate binding array. For example, to access the base\n// color texture of a `StandardMaterial` in bindless mode, write\n// `bindless_textures_2d[materials[slot].base_color_texture]`, where\n// `materials` is the bindless index table and `slot` is the index into that\n// table (which can be found in the `Mesh`).\n\n\n// Binding 0 is the bindless index table.\n// Filtering samplers.\n@group(2) @binding(1) var bindless_samplers_filtering: binding_array<sampler>;\n// Non-filtering samplers (nearest neighbor).\n@group(2) @binding(2) var bindless_samplers_non_filtering: binding_array<sampler>;\n// Comparison samplers (typically for shadow mapping).\n@group(2) @binding(3) var bindless_samplers_comparison: binding_array<sampler>;\n// 1D textures.\n@group(2) @binding(4) var bindless_textures_1d: binding_array<texture_1d<f32>>;\n// 2D textures.\n@group(2) @binding(5) var bindless_textures_2d: binding_array<texture_2d<f32>>;\n// 2D array textures.\n@group(2) @binding(6) var bindless_textures_2d_array: binding_array<texture_2d_array<f32>>;\n// 3D textures.\n@group(2) @binding(7) var bindless_textures_3d: binding_array<texture_3d<f32>>;\n// Cubemap textures.\n@group(2) @binding(8) var bindless_textures_cube: binding_array<texture_cube<f32>>;\n// Cubemap array textures.\n@group(2) @binding(9) var bindless_textures_cube_array: binding_array<texture_cube_array<f32>>;\n","bevy::render::color_operations":"import package::render::maths::FRAC_PI_3;\n\n// Converts HSV to RGB.\n//\n// Input: H ∈ [0, 2π), S ∈ [0, 1], V ∈ [0, 1].\n// Output: R ∈ [0, 1], G ∈ [0, 1], B ∈ [0, 1].\n//\n// <https://en.wikipedia.org/wiki/HSL_and_HSV#HSV_to_RGB_alternative>\nfn hsv_to_rgb(hsv: vec3<f32>) -> vec3<f32> {\n    let n = vec3(5.0, 3.0, 1.0);\n    let k = (n + hsv.x / FRAC_PI_3) % 6.0;\n    return hsv.z - hsv.z * hsv.y * max(vec3(0.0), min(k, min(4.0 - k, vec3(1.0))));\n}\n\n// Converts RGB to HSV.\n//\n// Input: R ∈ [0, 1], G ∈ [0, 1], B ∈ [0, 1].\n// Output: H ∈ [0, 2π), S ∈ [0, 1], V ∈ [0, 1].\n//\n// <https://en.wikipedia.org/wiki/HSL_and_HSV#From_RGB>\nfn rgb_to_hsv(rgb: vec3<f32>) -> vec3<f32> {\n    let x_max = max(rgb.r, max(rgb.g, rgb.b));  // i.e. V\n    let x_min = min(rgb.r, min(rgb.g, rgb.b));\n    let c = x_max - x_min;  // chroma\n\n    var swizzle = vec3<f32>(0.0);\n    if (x_max == rgb.r) {\n        swizzle = vec3(rgb.gb, 0.0);\n    } else if (x_max == rgb.g) {\n        swizzle = vec3(rgb.br, 2.0);\n    } else {\n        swizzle = vec3(rgb.rg, 4.0);\n    }\n\n    let h = FRAC_PI_3 * (((swizzle.x - swizzle.y) / c + swizzle.z) % 6.0);\n\n    // Avoid division by zero.\n    var s = 0.0;\n    if (x_max > 0.0) {\n        s = c / x_max;\n    }\n\n    return vec3(h, s, x_max);\n}\n\n","bevy::render::globals":"struct Globals {\n    // The time since startup in seconds\n    // Wraps to 0 after 1 hour.\n    time: f32,\n    // The delta time since the previous frame in seconds\n    delta_time: f32,\n    // Frame count since the start of the app.\n    // It wraps to zero when it reaches the maximum value of a u32.\n    frame_count: u32,\n    // WebGL2 structs must be 16 byte aligned.\n    @if(SIXTEEN_BYTE_ALIGNMENT)\n    _webgl2_padding: f32\n};\n","bevy::render::maths":"const PI: f32 = 3.141592653589793;      // π\nconst PI_2: f32 = 6.283185307179586;    // 2π\nconst HALF_PI: f32 = 1.57079632679;     // π/2\nconst FRAC_PI_3: f32 = 1.0471975512;    // π/3\nconst E: f32 = 2.718281828459045;       // exp(1)\n\nfn affine2_to_square(affine: mat3x2<f32>) -> mat3x3<f32> {\n    return mat3x3<f32>(\n        vec3<f32>(affine[0].xy, 0.0),\n        vec3<f32>(affine[1].xy, 0.0),\n        vec3<f32>(affine[2].xy, 1.0),\n    );\n}\n\nfn affine3_to_square(affine: mat3x4<f32>) -> mat4x4<f32> {\n    return transpose(mat4x4<f32>(\n        affine[0],\n        affine[1],\n        affine[2],\n        vec4<f32>(0.0, 0.0, 0.0, 1.0),\n    ));\n}\n\nfn mat2x4_f32_to_mat3x3_unpack(\n    a: mat2x4<f32>,\n    b: f32,\n) -> mat3x3<f32> {\n    return mat3x3<f32>(\n        a[0].xyz,\n        vec3<f32>(a[0].w, a[1].xy),\n        vec3<f32>(a[1].zw, b),\n    );\n}\n\n// Extracts the square portion of an affine matrix: i.e. discards the\n// translation.\nfn affine3_to_mat3x3(affine: mat4x3<f32>) -> mat3x3<f32> {\n    return mat3x3<f32>(affine[0].xyz, affine[1].xyz, affine[2].xyz);\n}\n\n// Returns the inverse of a 3x3 matrix.\nfn inverse_mat3x3(matrix: mat3x3<f32>) -> mat3x3<f32> {\n    let tmp0 = cross(matrix[1], matrix[2]);\n    let tmp1 = cross(matrix[2], matrix[0]);\n    let tmp2 = cross(matrix[0], matrix[1]);\n    let inv_det = 1.0 / dot(matrix[2], tmp2);\n    return transpose(mat3x3<f32>(tmp0 * inv_det, tmp1 * inv_det, tmp2 * inv_det));\n}\n\n// Returns the inverse of an affine matrix.\n//\n// https://en.wikipedia.org/wiki/Affine_transformation#Groups\nfn inverse_affine3(affine: mat4x3<f32>) -> mat4x3<f32> {\n    let matrix3 = affine3_to_mat3x3(affine);\n    let inv_matrix3 = inverse_mat3x3(matrix3);\n    return mat4x3<f32>(inv_matrix3[0], inv_matrix3[1], inv_matrix3[2], -(inv_matrix3 * affine[3]));\n}\n\n// Extracts the upper 3x3 portion of a 4x4 matrix.\nfn mat4x4_to_mat3x3(m: mat4x4<f32>) -> mat3x3<f32> {\n    return mat3x3<f32>(m[0].xyz, m[1].xyz, m[2].xyz);\n}\n\n// Creates an orthonormal basis given a Z vector and an up vector (which becomes\n// Y after orthonormalization).\n//\n// The results are equivalent to the Gram-Schmidt process [1].\n//\n// [1]: https://math.stackexchange.com/a/1849294\nfn orthonormalize(z_unnormalized: vec3<f32>, up: vec3<f32>) -> mat3x3<f32> {\n    let z_basis = normalize(z_unnormalized);\n    let x_basis = normalize(cross(z_basis, up));\n    let y_basis = cross(z_basis, x_basis);\n    return mat3x3(x_basis, y_basis, z_basis);\n}\n\n// Returns true if any part of a sphere is on the positive side of a plane.\n//\n// `sphere_center.w` should be 1.0.\n//\n// This is used for frustum culling.\nfn sphere_intersects_plane_half_space(\n    plane: vec4<f32>,\n    sphere_center: vec4<f32>,\n    sphere_radius: f32\n) -> bool {\n    return dot(plane, sphere_center) + sphere_radius > 0.0;\n}\n\n// pow() but safe for NaNs/negatives\nfn powsafe(color: vec3<f32>, power: f32) -> vec3<f32> {\n    return pow(abs(color), vec3(power)) * sign(color);\n}\n\n// https://en.wikipedia.org/wiki/Vector_projection#Vector_projection_2\nfn project_onto(lhs: vec3<f32>, rhs: vec3<f32>) -> vec3<f32> {\n    let other_len_sq_rcp = 1.0 / dot(rhs, rhs);\n    return rhs * dot(lhs, rhs) * other_len_sq_rcp;\n}\n\n// Below are fast approximations of common irrational and trig functions. These\n// are likely most useful when raymarching, for example, where complete numeric\n// accuracy can be sacrificed for greater sample count.\n\nfn fast_sqrt(x: f32) -> f32 {\n    let n = bitcast<f32>(0x1fbd1df5 + (bitcast<i32>(x) >> 1u));\n    // One Newton's method iteration for better precision\n    return 0.5 * (n + x / n);\n}\n\n// Slightly less accurate than fast_acos_4, but much simpler.\nfn fast_acos(in_x: f32) -> f32 {\n    let x = abs(in_x);\n    var res = -0.156583 * x + HALF_PI;\n    res *= fast_sqrt(1.0 - x);\n    return select(PI - res, res, in_x >= 0.0);\n}\n\n// 4th order polynomial approximation\n// 4 VGRP, 16 ALU Full Rate\n// 7 * 10^-5 radians precision\n// Reference : Handbook of Mathematical Functions (chapter : Elementary Transcendental Functions), M. Abramowitz and I.A. Stegun, Ed.\nfn fast_acos_4(x: f32) -> f32 {\n    let x1 = abs(x);\n    let x2 = x1 * x1;\n    let x3 = x2 * x1;\n    var s: f32;\n\n    s = -0.2121144 * x1 + 1.5707288;\n    s = 0.0742610 * x2 + s;\n    s = -0.0187293 * x3 + s;\n    s = fast_sqrt(1.0 - x1) * s;\n\n\t// acos function mirroring\n    return select(PI - s, s, x >= 0.0);\n}\n\nfn fast_atan2(y: f32, x: f32) -> f32 {\n    var t0 = max(abs(x), abs(y));\n    var t1 = min(abs(x), abs(y));\n    var t3 = t1 / t0;\n    var t4 = t3 * t3;\n\n    t0 = 0.0872929;\n    t0 = t0 * t4 - 0.301895;\n    t0 = t0 * t4 + 1.0;\n    t3 = t0 * t3;\n\n    t3 = select(t3, (0.5 * PI) - t3, abs(y) > abs(x));\n    t3 = select(t3, PI - t3, x < 0);\n    t3 = select(-t3, t3, y > 0);\n\n    return t3;\n}\n","bevy::render::view":"struct ColorGrading {\n    balance: mat3x3<f32>,\n    saturation: vec3<f32>,\n    contrast: vec3<f32>,\n    gamma: vec3<f32>,\n    gain: vec3<f32>,\n    lift: vec3<f32>,\n    midtone_range: vec2<f32>,\n    exposure: f32,\n    hue: f32,\n    post_saturation: f32,\n}\n\nstruct View {\n    clip_from_world: mat4x4<f32>,\n    unjittered_clip_from_world: mat4x4<f32>,\n    world_from_clip: mat4x4<f32>,\n    world_from_view: mat4x4<f32>,\n    view_from_world: mat4x4<f32>,\n    // Typically a right-handed projection matrix, one of either:\n    //\n    // Perspective (infinite reverse z)\n    // ```\n    // f = 1 / tan(fov_y_radians / 2)\n    //\n    // ⎡ f / aspect  0     0   0 ⎤\n    // ⎢          0  f     0   0 ⎥\n    // ⎢          0  0     0  -1 ⎥\n    // ⎣          0  0  near   0 ⎦\n    // ```\n    //\n    // Orthographic\n    // ```\n    // w = right - left\n    // h = top - bottom\n    // d = near - far\n    // cw = -right - left\n    // ch = -top - bottom\n    //\n    // ⎡  2 / w       0         0  0 ⎤\n    // ⎢      0   2 / h         0  0 ⎥\n    // ⎢      0       0     1 / d  0 ⎥\n    // ⎣ cw / w  ch / h  near / d  1 ⎦\n    // ```\n    //\n    // `clip_from_view[3][3] == 1.0` is the standard way to check if a projection is orthographic\n    // \n    // Custom projections are also possible however.\n    clip_from_view: mat4x4<f32>,\n    view_from_clip: mat4x4<f32>,\n    world_position: vec3<f32>,\n    exposure: f32,\n    // viewport(x_origin, y_origin, width, height)\n    viewport: vec4<f32>,\n    // 6 world-space half spaces (normal: vec3, distance: f32) ordered left, right, top, bottom, near, far.\n    // The normal vectors point towards the interior of the frustum.\n    // A half space contains `p` if `normal.dot(p) + distance > 0.`\n    frustum: array<vec4<f32>, 6>,\n    color_grading: ColorGrading,\n    mip_bias: f32,\n    frame_count: u32,\n};\n","bevy::sprite::mesh2d_bindings":"import package::sprite::mesh2d_types::Mesh2d;\n\n@if(PER_OBJECT_BUFFER_BATCH_SIZE)\n@group(1) @binding(0) var<uniform> mesh: array<Mesh2d, u32(constants::PER_OBJECT_BUFFER_BATCH_SIZE)>;\n@else\n@group(1) @binding(0) var<storage> mesh: array<Mesh2d>;\n","bevy::sprite::mesh2d_functions":"import package::sprite::{\n    mesh2d_view_bindings::view,\n    mesh2d_bindings::mesh,\n};\nimport package::render::maths::{affine3_to_square, mat2x4_f32_to_mat3x3_unpack};\n\nfn get_world_from_local(instance_index: u32) -> mat4x4<f32> {\n    return affine3_to_square(mesh[instance_index].world_from_local);\n}\n\nfn mesh2d_position_local_to_world(world_from_local: mat4x4<f32>, vertex_position: vec4<f32>) -> vec4<f32> {\n    return world_from_local * vertex_position;\n}\n\nfn mesh2d_position_world_to_clip(world_position: vec4<f32>) -> vec4<f32> {\n    return view.clip_from_world * world_position;\n}\n\n// NOTE: The intermediate world_position assignment is important\n// for precision purposes when using the 'equals' depth comparison\n// function.\nfn mesh2d_position_local_to_clip(world_from_local: mat4x4<f32>, vertex_position: vec4<f32>) -> vec4<f32> {\n    let world_position = mesh2d_position_local_to_world(world_from_local, vertex_position);\n    return mesh2d_position_world_to_clip(world_position);\n}\n\nfn mesh2d_normal_local_to_world(vertex_normal: vec3<f32>, instance_index: u32) -> vec3<f32> {\n    return mat2x4_f32_to_mat3x3_unpack(\n        mesh[instance_index].local_from_world_transpose_a,\n        mesh[instance_index].local_from_world_transpose_b,\n    ) * vertex_normal;\n}\n\nfn mesh2d_tangent_local_to_world(world_from_local: mat4x4<f32>, vertex_tangent: vec4<f32>) -> vec4<f32> {\n    return vec4<f32>(\n        mat3x3<f32>(\n            world_from_local[0].xyz,\n            world_from_local[1].xyz,\n            world_from_local[2].xyz\n        ) * vertex_tangent.xyz,\n        vertex_tangent.w\n    );\n}\n\nfn get_tag(instance_index: u32) -> u32 {\n    return mesh[instance_index].tag;\n}\n","bevy::sprite::mesh2d_types":"struct Mesh2d {\n    // Affine 4x3 matrix transposed to 3x4\n    // Use package::render::maths::affine3_to_square to unpack\n    world_from_local: mat3x4<f32>,\n    // 3x3 matrix packed in mat2x4 and f32 as:\n    // [0].xyz, [1].x,\n    // [1].yz, [2].xy\n    // [2].z\n    // Use package::render::maths::mat2x4_f32_to_mat3x3_unpack to unpack\n    local_from_world_transpose_a: mat2x4<f32>,\n    local_from_world_transpose_b: f32,\n    // 'flags' is a bit field indicating various options. u32 is 32 bits so we have up to 32 options.\n    flags: u32,\n    tag: u32,\n};\n","bevy::sprite::mesh2d_vertex_output":"struct VertexOutput {\n    // this is `clip position` when the struct is used as a vertex stage output \n    // and `frag coord` when used as a fragment stage input\n    @builtin(position) position: vec4<f32>,\n    @location(0) world_position: vec4<f32>,\n    @location(1) world_normal: vec3<f32>,\n    @location(2) uv: vec2<f32>,\n    @if(VERTEX_TANGENTS)\n    @location(3) world_tangent: vec4<f32>,\n    @if(VERTEX_COLORS)\n    @location(4) color: vec4<f32>,\n}\n","bevy::sprite::mesh2d_view_bindings":"import package::render::view::View;\nimport package::render::globals::Globals;\n\n@group(0) @binding(0) var<uniform> view: View;\n\n@group(0) @binding(1) var<uniform> globals: Globals;\n\n@group(0) @binding(2) var dt_lut_texture: texture_3d<f32>;\n@group(0) @binding(3) var dt_lut_sampler: sampler;\n","bevy::sprite::mesh2d_view_types":"import package::render::view;\nimport package::render::globals;\n","bevy::sprite::sprite_view_bindings":"import package::render::view::View;\n\n@group(0) @binding(0) var<uniform> view: View;\n\n@group(0) @binding(1) var dt_lut_texture: texture_3d<f32>;\n@group(0) @binding(2) var dt_lut_sampler: sampler;\n\n","bevy::ui::ui_vertex_output":"// The Vertex output of the default vertex shader for the Ui Material pipeline.\nstruct UiVertexOutput {\n    @location(0) uv: vec2<f32>,\n    // The size of the borders in UV space. Order is Left, Right, Top, Bottom.\n    @location(1) border_widths: vec4<f32>,\n    // The size of the borders in pixels. Order is top left, top right, bottom right, bottom left.\n    @location(2) border_radius: vec4<f32>,\n    // The size of the node in pixels. Order is width, height.\n    @location(3) @interpolate(flat) size: vec2<f32>,\n    @builtin(position) position: vec4<f32>,\n};\n"}